{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicals I: Fitting DGMs and introducing structure into latent representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first practical session, we will \n",
    " * learn how to build a neural network in Julia\n",
    " * compare different dimension reduction methods on a single-cell RNA-seq dataset\n",
    " * build an autoencoder model that learns a low-dimensional representation of a single-cell RNA-seq dataset and compare its representation PCA and UMAP \n",
    " * train the autoencoder to find a latent representation that resembles the ones found by UMAP or PCA\n",
    " * make it generative: turn the autoencoder into a VAE and sample synthetic data  \n",
    " * explore different loss functions and how different models are affected by hyperparameter choices\n",
    "\n",
    "Bonus material: \n",
    " * build a neural network from scratch \n",
    " * train it to classify cells into GABA-ergic and Glutamatergic neurons based on their single-cell gene expression profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started: \n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "Huge thanks to our amazing Master's student **Niklas Brunn** for the great work on revising, extending and polishing this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "In the `Project.toml` and `Manifest.toml` file in the folder in the repository, all the necessary packages and their versions are specified. We don't have to manually install them, but can simply tell Julia to configure the environment as specified in these files, including installing all necessary dependencies. This can be done with the Julia package manager with the following commands: \n",
    "\n",
    "For more info on the Julia package manager, see [here](https://docs.julialang.org/en/v1/stdlib/Pkg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; # loading the package manager\n",
    "cd(@__DIR__) # change into the directory where the current file (this notebook) is located\n",
    "Pkg.activate(\".\") # activate the environment specified in the .toml files\n",
    "Pkg.instantiate() # install the necessary dependencies\n",
    "Pkg.status() # show which packages are currently installed in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data \n",
    "\n",
    "We will use a dataset from Tasic et al. (2016), where gene expression is profiled in 1525 single neurons, extracted from the cortices of mice. The authors reported a set of 104 genes (Figure 3 of Tasic et al.), based on which different subtypes of neurons can be discriminated and a set of 80 neurotransmitter receptor genes (Figure S15 of Tasic et al.), totalling to 184 genes. \n",
    "\n",
    "The data from Tasic et al. (2016) has been stored in the [Gene expression Omnibus (GEO)](https://www.ncbi.nlm.nih.gov/geo/) under accession number GSE71585. \n",
    "\n",
    "We have downloaded the expression data and the correspoding sample description sheet from the GEO and removed the samples from the expression data which have been flagged by Tasic et al. (2016) as being of bad quality. Since the samples of bad quality are not contained within the sample description sheet, we can filter the samples of bad quality by comparing the sample IDs in the expression data with the IDs in the sample description sheet. \n",
    "\n",
    "Further, we have removed genes which are expressed with less than 10 aligned reads in less than 20 samples. This is necessary since many genes are usually expressed only in a single cell and consequently these genes do not substantially represent the structure in the data.\n",
    "\n",
    "Next, the data has been normalized for sequencing depth using an algorithm from DESeq (Anders and Huber, 2010).\n",
    "Normalization for sequencing depth is important since the number of aligned reads per sample fluctuate which affects the estimated expression level for each gene. The algorithm of Anders and Huber (2010) calculates correction factors, one for each sequencing library, i.e. sample, by which the sequencing data are divided. \n",
    "\n",
    "The data contains expression levels of over 15.119 genes in 1679 samples. To lower computational demands of the following analysis while still maintaining most of the biological signal int he dataset, we have subsetted the data using a set of genes identified as most relevant by Tasic et al. (2016): Specifically, we use the marker genes which are known to indicate cell type membership ([Figure 3](https://www.nature.com/articles/nn.4216/figures/3) in Tasic et al. (2016)) and the neurotransmitter receptorgenes which are differentially expressed in different neuron cell types but have a less pronounced expression pattern (supplementary material, [Figure S 15](https://static-content.springer.com/esm/art%3A10.1038%2Fnn.4216/MediaObjects/41593_2016_BFnn4216_MOESM67_ESM.pdf))\n",
    " \n",
    "Finally, we selected only the neural cells using the cell type annotation provided by Tasic et al. (2016), and annotated the cell types as GABAergic vs. Glutamatergic. \n",
    "\n",
    "The complete preprocessing can be reproduced by running the script `preprocess_Tasic.jl`, where at the end the following four files are saved to the `data` subfolder: \n",
    "\n",
    " * `genes.txt` - list of selected genes \n",
    " * `single_cell_mat` - the normalized count matrix, subsetted to the relevant genes and neural cells \n",
    " * `celltype.txt` - the cell type annotation of the cells in `single_cell_mat.txt`, as provided by Tasic et al. (2016)\n",
    " * `gabagluta.txt` - the annotation of cells as GABAergic vs. Glutamatergic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "genenames = readdlm(\"data/genes.txt\")\n",
    "mat_norm = readdlm(\"data/single_cell_mat.txt\")\n",
    "celltype = readdlm(\"data/celltype.txt\")\n",
    "gabagluta = readdlm(\"data/gabagluta.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many genes and cells we have, and some more information about the number of different cell types and Gaba- vs Glutamatergic cells: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngenes, ncells = size(mat_norm)\n",
    "ncelltypes = length(unique(celltype))\n",
    "ngaba = length(gabagluta[gabagluta.==\"GABA\"])\n",
    "ngluta = length(gabagluta[gabagluta.==\"Glutamate\"])\n",
    "\n",
    "println(\"Data matrix is of size \", size(mat_norm))\n",
    "println(\"Number of genes is \", length(genenames))\n",
    "println(\"Number of cells is \", length(celltype))\n",
    "\n",
    "println(\"There are $ncelltypes different cell types\")\n",
    "println(\"There are $ngaba GABA-ergic cells and $ngluta Glutamatergic cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some exploratory visualization \n",
    "\n",
    "Now that we have loaded the data and checked some basic dimensions, let's take a look at it! \n",
    "\n",
    "We start by looking at the distribution of overall counts per gene.\n",
    "\n",
    "- exemplary distribution of some genes across GABA vs Gluta cells\n",
    "- some summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countspergene = mapslices(x -> sum(x), mat_norm, dims=2)\n",
    "using VegaLite\n",
    "@vlplot(:bar, \n",
    "    x={log.(vec(countspergene)), bin={step=1, extent=[0,18]}, title=\"log(sum of counts across all cells)\"}, \n",
    "    y={\"count()\", title=\"number of genes\"},\n",
    "    width=500,\n",
    "    height=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also split this into GABA-ergic and Glutamatergic cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabainds = findall(x -> x == \"GABA\", vec(gabagluta))\n",
    "glutainds = findall(x -> x == \"Glutamate\", vec(gabagluta))\n",
    "data_gaba = mat_norm[:,gabainds]\n",
    "data_gluta = mat_norm[:,glutainds]\n",
    "\n",
    "countspergene_gaba = mapslices(x -> sum(x), data_gaba, dims=2)\n",
    "countspergene_gluta = mapslices(x -> sum(x), data_gluta, dims=2)\n",
    "\n",
    "countspergene_gabagluta = vcat(countspergene_gaba, countspergene_gluta)\n",
    "gabagluta_info = vec(vcat(fill(\"GABA\", ngenes), fill(\"Glutamate\", ngenes)))\n",
    "\n",
    "@vlplot(:bar, \n",
    "    x={log.(vec(countspergene_gabagluta)), bin={step=1, extent=[0,18]}, title=\"log(sum of counts across all cells)\"}, \n",
    "    y={\"count()\", title=\"number of genes\"},\n",
    "    column = gabagluta_info,\n",
    "    color = gabagluta_info,\n",
    "    width=230,\n",
    "    height=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to look at the structure in the data by applying some dimension reduction. We will calculate PCA and UMAP coordinates of the data and plot. \n",
    "\n",
    "First, we start off by applying PCA. Since PCA expects symmetric data, we need to standardize it, after log-transforming and adding a pseudo-count of 1 to deal with the over-dispersion and skewness. \n",
    "\n",
    "Let's first define the required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize: subtract mean and divide by standard deviation \n",
    "function standardize(x)\n",
    "    (x .- mean(x, dims = 1)) ./ std(x, dims = 1)\n",
    "end\n",
    "\n",
    "# function to compute principal components based on singular value decomposition\n",
    "using LinearAlgebra\n",
    "using StatsBase\n",
    "function prcomps(mat)\n",
    "    u,s,v = svd(mat)\n",
    "    prcomps = u * Diagonal(s)\n",
    "    return prcomps\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the functions, we can apply PCA to the data matrix. Note that we have defined the `prcomps` function to calculate the principal components such that it expects the features (=genes) to be the *columsn* of the data matrix, while currently the genes are the rows. Thus, we transpose the standardized and log-transformed data before passing it to the `prcomps` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mat = standardize(log.(mat_norm .+ 1)') # ' realises matrix transpose\n",
    "pcs = prcomps(input_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 2 principal components and visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "dimred_df = DataFrame(PC1 = pcs[:,1], PC2 = pcs[:,2])\n",
    "dimred_df |> \n",
    "@vlplot(:point, \n",
    "    x = \"PC1\", y=\"PC2\",\n",
    "    width=400, height=250,\n",
    "    title=\"First 2 principal components\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great! There seem to be some groups in the data. Let's add cell type and GABA-ergic/Glutamatergic info to the plot so we can see whether the clusters correspond to biologically meaningful groups of cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimred_df[!,:celltype] = vec(celltype)\n",
    "dimred_df |> \n",
    "@vlplot(:point, \n",
    "    x = :PC1, y=:PC2,\n",
    "    color={:celltype, title=\"Cell type\", scale={scheme=\"category20\"}},\n",
    "    title=\"PCs color-coded by cell type\",\n",
    "    width=400, height=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimred_df[!,:gabagluta] = vec(gabagluta)\n",
    "dimred_df |> \n",
    "@vlplot(:point, \n",
    "    x = :PC1, y=:PC2,\n",
    "    color={:gabagluta, title=\"GABAergic vs Glutamatergic\"},\n",
    "    title=\"PCs color-coded by GABA-ergic vs Glutamatergic cells\",\n",
    "    width=400, height=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare this to UMAP coordinates. We use the Julia implementation in the `UMAP.jl` package.\n",
    "As for PCA, we use the log-transformed and standardized data matrix `input_mat` as input (Unlike PCA, UMAP expects the data in a (`feature` x `observations`) format, hence the double transpose): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using UMAP \n",
    "Random.seed!(42); # for reproducibility \n",
    "umap_coords = umap(input_mat', n_neighbors=70, min_dist=1.5)' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the UMAP-coordinates to our `dimred_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimred_df[!,:UMAP1] = umap_coords[:,1]\n",
    "dimred_df[!,:UMAP2] = umap_coords[:,2]\n",
    "\n",
    "dimred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting, again using color-coding for the celltype and GABA- vs. Glutamatergic cells, we re-structure our data frame a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimred_stacked = DataFrame(dim1 = vcat(pcs[:,1], umap_coords[:,1]),\n",
    "                        dim2 = vcat(pcs[:,2], umap_coords[:,2]),\n",
    "                        celltype = repeat(vec(celltype), outer=2),\n",
    "                        gabagluta = repeat(vec(gabagluta), outer=2),\n",
    "                        method = repeat([\"PCA\", \"UMAP\"], inner=size(pcs,1))\n",
    ")\n",
    "dimred_stacked |> \n",
    "@vlplot(:point,\n",
    "    x={:dim1, title=\"\"}, y={:dim2, title=\"\"},\n",
    "    color={:celltype, title=\"cell type\", scale={scheme=\"category20\"}},\n",
    "    column={:method, title=\"\"},\n",
    "    resolve={scale={x=\"independent\",y=\"independent\"}} #so axes can have different scales for each method   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:**\n",
    ">\n",
    "> Make the same plot as above but use the `:gabagluta` annotation to color the cells, to see how well the two types separate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How deep learning in Julia works\n",
    "\n",
    "Now that we've gotten an idea of what's in our data, we will learn how to build a simple neural network from scratch in Julia. \n",
    "\n",
    "There is also a package `Flux.jl` where many building blocks, training routines and data handling functions are built-in. Later on in the notebook, we will use this package, but for this first exercise we will learn how to build a network from scratch. This is also pretty much what `Flux.jl` does under the hood.\n",
    "\n",
    "We have seen so far that all dimension reduction methods do a good job at differentiating between the GABA- and the Glutamatergic cells. \n",
    "For this first exercise to serve as an instructive example of how to build and train a neural network, we will thus train a classifier to automatically assign the cells into GABA-ergic and Glutamatergic neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining neural networks\n",
    "\n",
    "Mathematically speaking, a neural network is essentially simply a function composition, consisting of both linear and non-linear functions. Each linear function is defined as multiplication by some parameter matrix called the *weights* of the network, and addition of an offset parameter vector called the *bias*. \n",
    "\n",
    "Denoting the weight matrix as $W \\in \\mathbb{R}^{n \\times p}$ and the bias vector as $b \\in \\mathbb{R}^p$, the linear transformation of an input vector $x \\in \\mathbb{R}^p$ can be written as $W\\cdot x + b$. Such a linear transformation is typically followed by a non-linear *activation function* that is applied element-wise. Commonly used activation functions are the relu, sigmoid or hyperbolic tangent function. \n",
    "\n",
    "Denoting the activation function as $\\phi: \\mathbb{R} \\to \\mathbb{R}$, one layer of a neural network can thus be defined as \n",
    "$ \\phi.(W\\cdot x + b) $, where $.$ denotes element-wise application of $\\phi$. \n",
    "\n",
    "A *deep* neural network now simply consists of several such layers stacked on top of each other. A network with $K$ layers can thus be represented as a function $f_{\\mathrm{NeuralNet}}(x) := f_K \\circ f_{K-1} \\circ ...\\circ f_2 \\circ f_1$, where each $f_k$ is of the form $f_k = \\phi_k.(W_k \\cdot x_k + b_k)$.\n",
    "\n",
    "Data is processed throught the network by subsequently applying these layer functions. A neural network architecture and forward propagation is illustrated in the following graphic: \n",
    "\n",
    "![](figures/NNForwardPropDetails.png)\n",
    "\n",
    "(Figure adapted from [Angermueller et al. (2016) Deep learning for computational biology, Molecular Systems Biology.](https://www.embopress.org/doi/full/10.15252/msb.20156651))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Flux.jl`, the `Dense` struct implements a fully-connected layer of exactly the form as described above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "?Flux.Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "help?> Dense\n",
    "search: Dense DenseArray DenseVector DenseMatrix DenseVecOrMat DenseConvDims\n",
    "\n",
    "  Dense(in => out, σ=identity; bias=true, init=glorot_uniform)\n",
    "  Dense(W::AbstractMatrix, [bias, σ])\n",
    "\n",
    "  Create a traditional fully connected layer, whose forward pass is given by:\n",
    "\n",
    "  y = σ.(W * x .+ bias)\n",
    "\n",
    "  The input x should be a vector of length in, or batch of vectors represented\n",
    "  as an in × N matrix, or any array with size(x,1) == in. The out y will be a\n",
    "  vector of length out, or a batch with size(y) == (out, size(x)[2:end]...)\n",
    "\n",
    "  Keyword bias=false will switch off trainable bias for the layer. The\n",
    "  initialisation of the weight matrix is W = init(out, in), calling the\n",
    "  function given to keyword init, with default glorot_uniform. The weight\n",
    "  matrix and/or the bias vector (of length out) may also be provided\n",
    "  explicitly.\n",
    "\n",
    "  Examples\n",
    "  ≡≡≡≡≡≡≡≡≡≡\n",
    "\n",
    "  julia> d = Dense(5 => 2)\n",
    "  Dense(5 => 2)       # 12 parameters\n",
    "\n",
    "  julia> d(rand(Float32, 5, 64)) |> size\n",
    "  (2, 64)\n",
    "\n",
    "  julia> d(rand(Float32, 5, 1, 1, 64)) |> size  # treated as three batch dimensions\n",
    "  (2, 1, 1, 64)\n",
    "\n",
    "  julia> d1 = Dense(ones(2, 5), false, tanh)  # using provided weight matrix\n",
    "  Dense(5 => 2, tanh; bias=false)  # 10 parameters\n",
    "\n",
    "  julia> d1(ones(5))\n",
    "  2-element Vector{Float64}:\n",
    "   0.9999092042625951\n",
    "   0.9999092042625951\n",
    "\n",
    "  julia> Flux.params(d1)  # no trainable bias\n",
    "  Params([[1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]])\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be constructed by passing the input and output dimensions `n` and `p`, i.e., the dimensions of the weight matrix, and the activation function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Dense(10, 20, relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple layers can be combined into a `Chain`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = Chain(Dense(10, 20, relu), Dense(20, 40, tanh), Dense(40, 2, σ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural networks\n",
    "\n",
    "All these parameters need to be optimised, in order for our neural network to learn something meaningful. \n",
    "Training a neural network means to find values for the weights and biases of all layers, such that the overall neural network approximates some function of interest.\n",
    "\n",
    "For this, in addition to the neural network model itself, we need to define a loss function that measures how good our current approximation is. We can then minimize the loss function with respect to the network parameters, the weights and biases, using a version of gradient descent. \n",
    "\n",
    "Gradient descent is an numerical optimization algorithm where the network parameters get iteratively updated by adding the weighted negativ gradient of the network, with respect to the network parameters, to each parameter. The gradient is calculated via backpropagation (reverse mode of automatic differentiation). \n",
    "\n",
    "The following images show the optimization process of a neural network via gradient descent:\n",
    "\n",
    "![](figures/ForwardAndBackwardProp.png)\n",
    "![](figures/GradientDescent.png)\n",
    "\n",
    "(Figure adapted from [Angermueller et al. (2016) Deep learning for computational biology, Molecular Systems Biology.](https://www.embopress.org/doi/full/10.15252/msb.20156651))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an autoencoder\n",
    "\n",
    "Now that we know how to build simple models, let's see if we can find some underlying structure in the data by dimension reduction, similar to PCA and UMAP, but based on neural networks. \n",
    "\n",
    "For this purpose, we build an autoencoder: A model consisting of two neural networks, the encoder and the decoder, that are chained together. The encoder maps the input data to a low-dimension space, from which the decoder maps it back to the original data space. The aim of the model optimization is to reconstrut the original input data with the decoder from the lower-dimensional representation given by the encoder. If this reconstruction is successful, the model will have learned to capture the central structure in the data (based on which it can be accurately reconstructed) in the low-dimension bottleneck layer. \n",
    "\n",
    "![](figures/AEwithCellbyGeneMat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our autoencoder model and train it with our previously defined binary train data `Xtrain`. \n",
    "\n",
    "First, we need some functions to initialise our model. We use the hyperbolic tangent as activation function and create an autoencoder with an encoder- and decoder network consisting of two chained layers each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct AE\n",
    "    encoder\n",
    "    decoder\n",
    "end\n",
    "\n",
    "# function to initialise encoder and decoder \n",
    "function AE(in::Int, zdim::Int; \n",
    "    hidden::Int=round(Int(in/10)), \n",
    "    act_enc::Function=tanh, \n",
    "    act_dec::Function=tanh, \n",
    "    seed::Int=1234)\n",
    "    \n",
    "    Random.seed!(seed)\n",
    "    encoder = Chain(Dense(in, hidden, act_enc),\n",
    "                    Dense(hidden, zdim)\n",
    "    )\n",
    "    decoder = Chain(Dense(zdim, hidden, act_dec),\n",
    "                    Dense(hidden, in)\n",
    "    )\n",
    "    return AE(encoder, decoder)\n",
    "end\n",
    "\n",
    "# define how the model should operate on an input x \n",
    "(m::AE)(x) = m.decoder(m.encoder(x))\n",
    "# automatically collect parameters from the layers included in the AE struct by Flux: \n",
    "Flux.Functors.@functor AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As our loss function, we use the logit binary cross entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: logitbinarycrossentropy\n",
    "reconstruction_loss(m::AE, x) = logitbinarycrossentropy(m(x), x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we dichotomise our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dichotomize(x)\n",
    "\tn,p=size(x)\n",
    "\tdicho = zeros(n,p)\n",
    "\tfor i=1:p\n",
    "\t\tdicho[:,i] = ifelse.(x[:,i] .>median(x[:,i]),1.0,0.0)\n",
    "\tend\n",
    "\treturn Float32.(dicho)\n",
    "end\n",
    "\n",
    "# data\n",
    "X = dichotomize(mat_norm')' \n",
    "\n",
    "# labels\n",
    "gabagluta_binary = collect(gabagluta.==\"GABA\")\n",
    "y = gabagluta_binary\n",
    "\n",
    "# random permutation of the cells in X\n",
    "p, n = size(X)\n",
    "Random.seed!(1234) # for reproducibility \n",
    "randindex = Random.randperm(n);\n",
    "\n",
    "X_perm = X[:, randindex]\n",
    "y_perm = y[randindex]\n",
    "\n",
    "# splitting in train and test data\n",
    "Xtrain = X_perm[:, 1:1000]\n",
    "Xtest = X_perm[:, 1001:end]\n",
    "\n",
    "# splitting in train and test labels\n",
    "ytrain = y_perm[1:1000]\n",
    "ytest = y_perm[1001:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define all hyperparameters needed for the training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 500 \n",
    "batchsize = 10\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define and build our model with our previously defined autoencoder-structure by choosing a representative number of latent dimensions `zdim` for our purposes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdim = 2 \n",
    "Random.seed!(1234) \n",
    "m = AE(ngenes, zdim)\n",
    "\n",
    "ps = Flux.params(m) # collecting trainable parameters of our model\n",
    "opt = ADAM(lr) # specifying the optimization algorithm\n",
    "\n",
    "trainingdata = Flux.DataLoader(Xtrain, batchsize=batchsize, shuffle=true) # generating randomly shuffeled train-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all we can start training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloss = []\n",
    "testloss = []\n",
    "\n",
    "for ep in 1:nepochs \n",
    "    for batch in trainingdata\n",
    "        gs = Flux.gradient(ps) do \n",
    "            reconstruction_loss(m, batch)\n",
    "        end\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "    push!(trainloss, reconstruction_loss(m, Xtrain))\n",
    "    push!(testloss, reconstruction_loss(m, Xtest)) \n",
    "\n",
    "    if ep % (nepochs/20) == 0 \n",
    "        @info \"epoch$(ep): current train-loss: $(reconstruction_loss(m, Xtrain)), current test-loss: $(reconstruction_loss(m, Xtest))\"\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training we want to get a feeling of how well our model generalizes to unseen data. Therefore we visualize the model loss for train- and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot(collect(1:nepochs), hcat(log.(10, trainloss .+ 1), log.(10, testloss .+1 ), fill(minimum(log.(10, testloss .+1 )), nepochs)), title = \"Loss\", label=[\"train-loss\" \"test-loss\" \"min-test-loss\"], xlabel=\"number of epoch\", ylabel=\"log10(model loss)\", legend=:topright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:**\n",
    "> \n",
    "> What is happening here? What could be the reason for that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** \n",
    ">\n",
    "> How can we prevent our model from overfitting?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Number of epoch with minimum test-loss: $(argmin(log.(10, testloss .+1 )))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next steps, you can re-run the training with a new number of epochs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to take a look at the latent space! The following two plots show the latent space for our train data, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvals_train = m.encoder(Xtrain)\n",
    "latentrepr = @vlplot(:point, x = zvals_train[1,:], y = zvals_train[2,:], color = {vec(celltype[randindex[1:1000]]), scale={scheme=\"category20\"}})\n",
    "latentrepr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:**\n",
    ">\n",
    "> Plot the `z_vals` color-coded according to GABA- vs. glutamatergic cells! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:**\n",
    ">\n",
    "> Look at the `z_vals` color-coded according to both cell type and GABA- vs. glutamatergic cells also on the test data! \n",
    "> \n",
    "> Does the model generalise well to unseen data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if its not as good as with UMAP, the autoencoder is able to learn kind of a structured representation for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we train the autoencoder to look more like UMAP or PCA? \n",
    "\n",
    "For that, we need another loss function. We can augment our loss with another term that pushes the learned latent representation more towards the UMAP or PCA representation. You might ask why this would be desirable, if one already has the UMAP solution. Unlike UMAP, the AE gives you a parameteric mapping, so an explicit function to transform general data into the embedding, even if we had new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's again have a look at the previously stored UMAP-representation at beginning of our notebook, for the standardized and log-transformed data matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapplot = @vlplot(:point, x = umap_coords[:,1], y = umap_coords[:,2], color = {vec(celltype), scale={scheme=\"category20\"}})\n",
    "umapplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the new loss function, assuming taking the UMAP/PCA labels as `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_loss(m::AE, x, y) = Flux.mse(m.encoder(x), y)\n",
    "joint_loss(m::AE, x, y) = reconstruction_loss(m, x) + embedding_loss(m, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can re-initialize the model parameters, specify our hyperparameters and create the modified training-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234); \n",
    "m = AE(ngenes, zdim)\n",
    "ps = Flux.params(m)\n",
    "\n",
    "nepochs = 500\n",
    "lr = 1e-3\n",
    "batchsize = 100\n",
    "\n",
    "trainingdata = Flux.DataLoader((Xtrain, umap_coords[randindex[1:1000], :]'), batchsize=batchsize, shuffle=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the generated UMAP data, we are ready to train our modified autoencoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losslist_joint_train = []\n",
    "losslist_rec_train = []\n",
    "losslist_emb_train = []\n",
    "losslist_joint_test = []\n",
    "losslist_rec_test = []\n",
    "losslist_emb_test = []\n",
    "\n",
    "for ep in 1:nepochs \n",
    "    for batch in trainingdata\n",
    "        gs = Flux.gradient(ps) do \n",
    "            joint_loss(m, batch...)\n",
    "        end\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "    push!(losslist_joint_train, joint_loss(m, Xtrain, umap_coords[randindex[1:1000], :]'))\n",
    "    push!(losslist_rec_train, reconstruction_loss(m, Xtrain)) \n",
    "    push!(losslist_emb_train, embedding_loss(m, Xtrain, umap_coords[randindex[1:1000], :]'))\n",
    "    push!(losslist_joint_test, joint_loss(m, Xtest, umap_coords[randindex[1001:1525], :]'))\n",
    "    push!(losslist_rec_test, reconstruction_loss(m, Xtest)) \n",
    "    push!(losslist_emb_test, embedding_loss(m, Xtest, umap_coords[randindex[1001:1525], :]'))\n",
    "\n",
    "    if ep % (nepochs/20) == 0\n",
    "        @info \"epoch$(ep): current train-loss: $(joint_loss(m, Xtrain, umap_coords[randindex[1:1000], :]')), current test-loss: $(joint_loss(m, Xtest, umap_coords[randindex[1001:1525], :]'))\"\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the different losses from the data we stored during training the autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot(collect(1:nepochs), hcat(log.(10, losslist_joint_train .+ 1), log.(10, losslist_joint_test .+1 ), fill(minimum(log.(10, losslist_joint_test .+1 )), nepochs)), title = \"joint loss\", label=[\"train-joint loss\" \"test- joint loss\" \"min test- joint loss\"], xlabel=\"number of epoch\", ylabel=\"log10(model joint loss)\", legend=:topright)\n",
    "p2 = plot(collect(1:nepochs), hcat(log.(10, losslist_rec_train .+ 1), log.(10, losslist_rec_test .+1 ), fill(minimum(log.(10, losslist_rec_test .+1 )), nepochs)), title = \"reconstruction loss\", label=[\"train- rec. loss\" \"test- rec. loss\" \"min test- rec. loss\"], xlabel=\"number of epoch\", ylabel=\"log10(model rec. loss)\", legend=:topright)\n",
    "p3 = plot(collect(1:nepochs), hcat(log.(10, losslist_emb_train .+ 1), log.(10, losslist_emb_test .+1 ), fill(minimum(log.(10, losslist_emb_test .+1 )), nepochs)), title = \"embedding loss\", label=[\"train- emb. loss\" \"test- emb. loss\" \"min test- emb. loss\"], xlabel=\"number of epoch\", ylabel=\"log10(model emb. loss)\", legend=:topright)\n",
    "\n",
    "plot(p1,p2, p3, layout=(1,3), size=(1400,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, training results in overfitting after a few number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:**\n",
    "> \n",
    "> As above, identify the number of epochs for which the different test-losses reach their minimum value. \n",
    ">\n",
    "> Identify a suitable number of epochs and re-train the model for that number of epochs. \n",
    ">\n",
    "> Compare the plots of the latent representation for the test data with a model trained on more vs. less epochs. \n",
    "> How do they change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Number of epoch with minimum joint test-loss: $(argmin(log.(10, losslist_joint_test .+1 )))\"\n",
    "@info \"Number of epoch with minimum reconstruction test-loss: $(argmin(log.(10, losslist_rec_test .+1 )))\"\n",
    "@info \"Number of epoch with minimum embedding test-loss: $(argmin(log.(10, losslist_emb_test .+1 )))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have a look at the latent representation and are curious whether this now looks more like the UMAP representation for train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvals_train = m.encoder(Xtrain)\n",
    "@vlplot(:point, x = zvals_train[1,:], y = zvals_train[2,:], color = vec(gabagluta[randindex[1:1000]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(:point, x = zvals_train[1,:], y = zvals_train[2,:], color = {vec(celltype[randindex[1:1000]]), scale={scheme=\"category20\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly compare this to the original UMAP plot and the standard autoencoder-representation from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umapplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latentrepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvals_test = m.encoder(Xtest)\n",
    "@vlplot(:point, x = zvals_test[1,:], y = zvals_test[2,:], color = vec(gabagluta[randindex[1001:1525]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(:point, x = zvals_test[1,:], y = zvals_test[2,:], color ={vec(celltype[randindex[1001:1525]]), scale={scheme=\"category20\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> Choosing a samller number of epochs leads to a better generalization to unseen data. This is one big advantage of the autoencoder model over classical UMAP. \n",
    "\n",
    "But if we were just interested in a good visualization of the latent representation on the train set, we can get even closer to the UMAP-representation by choosing a larger number of epochs, for which the joint loss is significantly lower.\n",
    "\n",
    "If you want to check this out re-run the training with $>=500$ epochs and afterwards re-run the cells with the plots ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** \n",
    ">\n",
    "> Do the same with PCA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mat = standardize(log.(mat_norm .+ 1)')\n",
    "pcaout = prcomps(input_mat)\n",
    "#...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, let's make it generative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we first show how to make the VAE work with binary data. \n",
    "\n",
    "For the binary version of the VAE, we do again use our dichotomized data `X` from before, meaning that we use `Xtrain` for the training process and later validate the performance of the VAE with `Xtest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Dimensions of Xtrain: $(size(Xtrain))\"\n",
    "@info \"Dimensions of Xtest: $(size(Xtest))\"\n",
    "Xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VAE struct and initialisation, and some functions for the VAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct VAE\n",
    "    encoder\n",
    "    μ_encoder\n",
    "    logσ_encoder\n",
    "    decoder \n",
    "end\n",
    "Flux.Functors.@functor VAE\n",
    "\n",
    "# init\n",
    "function VAE(in::Int, zdim::Int; \n",
    "    hidden::Int=round(Int(in/10)), \n",
    "    act_enc::Function=tanh, \n",
    "    act_dec::Function=tanh, \n",
    "    seed::Int=1234)\n",
    "    \n",
    "    Random.seed!(seed)\n",
    "    encoder = Dense(in, hidden, act_enc)\n",
    "    μ_encoder = Dense(hidden, zdim)\n",
    "    logσ_encoder = Dense(hidden, zdim)\n",
    "    decoder = Chain(Dense(zdim, hidden, act_dec),\n",
    "                    Dense(hidden, in, σ)\n",
    "    )\n",
    "    return VAE(encoder, μ_encoder, logσ_encoder, decoder)\n",
    "end\n",
    "\n",
    "(m::VAE)(x) = m.decoder(m.μ_encoder(m.encoder(x)))\n",
    "\n",
    "# loss - components \n",
    "\n",
    "sqnorm(x) = sum(abs2, x)\n",
    "# parameters \n",
    "function Flux.params(m::VAE)\n",
    "    Flux.params(m.encoder, m.μ_encoder, m.logσ_encoder, m.decoder)\n",
    "end\n",
    "\n",
    "function latentz(μ::AbstractArray{Float32}, logσ::AbstractArray{Float32})\n",
    "    return μ .+ exp.(logσ) .* randn(Float32, size(μ))\n",
    "end\n",
    "\n",
    "function latentz(μ, logσ)\n",
    "    return μ + exp(logσ) * randn(Float32)\n",
    "end\n",
    "\n",
    "function kl_q_p(μ::AbstractArray{Float32}, logσ::AbstractArray{Float32})\n",
    "    return 0.5f0 * sum(exp.(2f0 .* logσ) + μ.^2 .- 1f0 .- (2.0f0 .* logσ), dims=1)\n",
    "end\n",
    "\n",
    "function logp_x_z(x::AbstractArray{Float32}, dec_z::AbstractArray{Float32})\n",
    "    return sum(x .* log.(dec_z .+ eps(Float32)) .+ (1.0f0 .- x) .* log.(1.0f0 .- dec_z .+ eps(Float32)), dims=1)\n",
    "end\n",
    "\n",
    "# loss - complete functions \n",
    "function vae_loss(m::VAE, X::AbstractArray{Float32}) \n",
    "    post_μ, post_logσ = m.μ_encoder(m.encoder(X)), m.logσ_encoder(m.encoder(X))\n",
    "    sample_z = latentz.(post_μ, post_logσ)\n",
    "    ELBO = logp_x_z(X, m.decoder(sample_z)) .- kl_q_p(post_μ, post_logσ)\n",
    "    lossval = sum(-ELBO) #+ 0.01f0 * sum(sqnorm, Flux.params(m.decoder))\n",
    "    return lossval\n",
    "end\n",
    "\n",
    "# for callback during training: loss without sampling step \n",
    "function determ_vae_loss(m::VAE, X::AbstractArray{Float32}) \n",
    "    post_μ, post_logσ = m.μ_encoder(m.encoder(X)), m.logσ_encoder(m.encoder(X))\n",
    "    ELBO = logp_x_z(X, m.decoder(post_μ)) .- kl_q_p(post_μ, post_logσ) \n",
    "    lossval = sum(-ELBO) #+ 0.01f0 * sum(sqnorm, Flux.params(m.decoder))\n",
    "    return lossval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdim = 2\n",
    "nepochs=400\n",
    "lr=1e-3\n",
    "batchsize=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the model and train! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngenes, nobs = size(Xtrain)\n",
    "\n",
    "Random.seed!(1234); \n",
    "m = VAE(ngenes, zdim)\n",
    "ps = Flux.params(m)\n",
    "\n",
    "opt = ADAM(lr)\n",
    "trainingdata = Flux.DataLoader(Xtrain, batchsize=batchsize, shuffle=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losslist_train = [] \n",
    "losslist_test = []\n",
    "\n",
    "for ep in 1:nepochs\n",
    "    for batch in trainingdata\n",
    "        gs = Flux.gradient(ps) do \n",
    "            vae_loss(m, batch)\n",
    "        end\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "    push!(losslist_train, determ_vae_loss(m, Xtrain))\n",
    "    push!(losslist_test, determ_vae_loss(m, Xtest))\n",
    "\n",
    "    if ep % (nepochs/20) == 0 # show loss only at 10 timepoints\n",
    "        @info \"epoch $(ep): current train-loss: $(determ_vae_loss(m, Xtrain)), current test-loss: $(determ_vae_loss(m, Xtest))\"\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(collect(1:nepochs), hcat(log.(10, losslist_train .+ 1), log.(10, losslist_test .+1 ), fill(minimum(log.(10, losslist_test .+1 )), nepochs)), title = \"VAE loss\", label=[\"train-loss\" \"test-loss\" \"min test-loss\"], xlabel=\"number of epoch\", ylabel=\"log10(model loss)\", legend=:topright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot latent space \n",
    "zvals_train_vae = m.μ_encoder(m.encoder(Xtrain))\n",
    "@vlplot(:point, x = zvals_train_vae[1,:], y = zvals_train_vae[2,:], color = vec(gabagluta[randindex[1:1000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(:point, x = zvals_train_vae[1,:], y = zvals_train_vae[2,:], color = {vec(celltype[randindex[1:1000]]), scale={scheme=\"category20\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvals_test_vae = m.μ_encoder(m.encoder(Xtest))\n",
    "@vlplot(:point, x = zvals_test_vae[1,:], y = zvals_test_vae[2,:], color = vec(gabagluta[randindex[1001:end]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vlplot(:point, x = zvals_test_vae[1,:], y = zvals_test_vae[2,:], color = {vec(celltype[randindex[1001:end]]), scale={scheme=\"category20\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the trained model!\n",
    "\n",
    "Now we want to make use of the fact that we have trained a *generative* model, i.e., we want to sample synthetic observations from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "function priorsample(m::VAE, nsamples::Int; seed::Int=6789)\n",
    "    zdim = size(m.μ_encoder.bias,1)\n",
    "    z = latentz(zeros(Float32, (zdim, nsamples)), ones(Float32, (zdim, nsamples)))\n",
    "    decoded_ps = m.decoder(z)\n",
    "    synthetic_obs = rand.(Bernoulli.(decoded_ps))\n",
    "    return synthetic_obs\n",
    "end\n",
    "\n",
    "function posteriorsample(m::VAE, data; seed::Int=6789)\n",
    "    post_μ, post_logσ = m.μ_encoder(m.encoder(data)), m.logσ_encoder(m.encoder(data))\n",
    "    zdim = size(m.μ_encoder.bias,1)\n",
    "    z = latentz(post_μ, post_logσ)\n",
    "    decoded_ps = m.decoder(z)\n",
    "    synthetic_obs = rand.(Bernoulli.(decoded_ps))\n",
    "    return synthetic_obs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_prior = priorsample(m, 1000)\n",
    "synthetic_data_post = posteriorsample(m, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the distribution of some genes in the samples and the original data by looking at the corresponding heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Clustering\n",
    "using Distances\n",
    "\n",
    "# original data\n",
    "distmat_cells = pairwise(cosine_dist, X', dims=1)\n",
    "hclust_cells = hclust(distmat_cells; linkage=:average, branchorder=:optimal)\n",
    "distmat_genes = pairwise(cosine_dist, X', dims=2)\n",
    "hclust_genes = hclust(distmat_genes; linkage=:average, branchorder=:optimal)\n",
    "\n",
    "# prior sampling\n",
    "distmat_cells_syn_prior = pairwise(cosine_dist, synthetic_data_prior', dims=1)\n",
    "hclust_cells_syn_prior = hclust(distmat_cells_syn_prior; linkage=:average, branchorder=:optimal)\n",
    "distmat_genes_syn_prior = pairwise(cosine_dist, synthetic_data_prior', dims=2)\n",
    "hclust_genes_syn_prior = hclust(distmat_genes_syn_prior; linkage=:average, branchorder=:optimal)\n",
    "\n",
    "# posterior sampling\n",
    "distmat_cells_syn_post = pairwise(cosine_dist, synthetic_data_post', dims=1)\n",
    "hclust_cells_syn_post = hclust(distmat_cells_syn_post; linkage=:average, branchorder=:optimal)\n",
    "distmat_genes_syn_post = pairwise(cosine_dist, synthetic_data_post', dims=2)\n",
    "hclust_genes_syn_post = hclust(distmat_genes_syn_post; linkage=:average, branchorder=:optimal)\n",
    "\n",
    "plot(heatmap(X'[hclust_cells.order,hclust_genes.order], title = \"Original data\"),\n",
    "    heatmap(synthetic_data_prior'[hclust_cells_syn_prior.order, hclust_genes_syn_prior.order], title=\"Prior sampling\"), \n",
    "    heatmap(synthetic_data_post'[hclust_cells_syn_post.order, hclust_genes_syn_post.order], title = \"Posterior sampling\"), \n",
    "    layout = (3,1), size=(1000,800)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** \n",
    "> \n",
    "> Train the VAE in a supervised way to match the UMAP output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus material: How to build a simple classifier from scratch in Julia (even without usig Flux)\n",
    "\n",
    "This explains how to build a simple neural network from scratch, without relying on the `Flux` library, in Julia. With this we can gain an even deeper understanding of the inner workings of neural networks and their optimisation. \n",
    "We will build an example neural network to classify the cells into GABA- and Glutamatergic neurons. \n",
    "\n",
    "This is done via *binary classification* by identifying each GABA-eric neuron with a value of 1 and  the Glutamateric neurons with a value of 0.\n",
    "\n",
    "We start off with a very simple network consisting of just one layer. \n",
    "\n",
    "First, we randomly initialise the weight and bias vector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234) # for reproducibility \n",
    "W = randn(1, ngenes) # output is one-dimensional: GABA vs Gluat\n",
    "b = randn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our neural network according to the definition above, but for numerical stability we place the activation function as a part of our loss funtion (*see below*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x) = W*x .+ b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model output we need values between 0 and 1 to distinguish between the GABA- and the Glutamatergic cells. Therefore we define a function `sigmoid_model` which evaluates the sigmoidal activation of our simple affine `model` function.\n",
    "\n",
    "We can thus interpret the model output as an estimation by our model of how likely the input is to be a GABA-eric neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoid_model(X)\n",
    "    sigmoid.(model(X))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use the dichotomized version of the gene expression matrix `mat_norm` as input data. This produces a new data matrix `X`, which only contains $0$ and $1$ as values depending on the median value of each row of `mat_norm`. \n",
    "\n",
    "Based on this data, the model should be able to learn the GABA- vs. Glutamatergic labels. Additionally, after training our model, we want to figure out how good our modle generalizes to unseen data. Therefore we split the dataset `X` into *train-* and *test datasets* where we use the train data to train our model and the test data to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dichotomize(x)\n",
    "\tn,p=size(x)\n",
    "\tdicho = zeros(n,p)\n",
    "\tfor i=1:p\n",
    "\t\tdicho[:,i] = ifelse.(x[:,i] .>median(x[:,i]),1.0,0.0)\n",
    "\tend\n",
    "\treturn Float32.(dicho)\n",
    "end\n",
    "\n",
    "# data\n",
    "X = dichotomize(mat_norm')' \n",
    "\n",
    "# labels\n",
    "gabagluta_binary = collect(gabagluta.==\"GABA\")\n",
    "y = gabagluta_binary\n",
    "\n",
    "# random permutation of the cells in X\n",
    "p, n = size(X)\n",
    "Random.seed!(1234) # for reproducibility \n",
    "randindex = Random.randperm(n);\n",
    "\n",
    "X_perm = X[:, randindex]\n",
    "y_perm = y[randindex]\n",
    "\n",
    "# splitting in train and test data\n",
    "Xtrain = X_perm[:, 1:1000]\n",
    "Xtest = X_perm[:, 1001:end]\n",
    "\n",
    "# splitting in train and test labels\n",
    "ytrain = y_perm[1:1000]\n",
    "ytest = y_perm[1001:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common loss function for binary classification is the *binary cross entropy*, which measures how well the model output corresponds to the true labels. This also comes built-in with `Flux.jl`. \n",
    "\n",
    "As mentioned before we use a slightly modified version of the binary cross entropy, called *logit binary cross entropy*. This version of the binary cross entropy loss is just a composition of the binary cross entropy with the sigmoid activation function $\\mathrm{loss} = \\mathrm{binarycrossentropy}\\circ \\sigma$ , where the sigmoid activation (element-wise) is already a part of the loss function. The advantage of this version of the loss function is that it is numerically more stable as for binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: logitbinarycrossentropy\n",
    "loss(m, X, y) = logitbinarycrossentropy(m(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model loss currently is on the train data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(model, Xtrain, ytrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, this doesn't really tell us much. We would additionally want to also monitor the *accuracy*, i.e., how well does the current model classify GABA- vs. Glutamatergic neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function crosstab(x, y)\n",
    "    @assert length(x) == length(y)\n",
    "    tab = fill(0, (2,2))\n",
    "    for i in 1:length(x)\n",
    "        curvals = [x[i] y[i]] .+ 1\n",
    "        for j in 1:size(tab,1)\n",
    "            for k in 1:size(tab,2)\n",
    "                if curvals[1] == j && curvals[2] == k\n",
    "                    tab[j,k] += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    tab\n",
    "end\n",
    "\n",
    "function accuracy(m, X, y)\n",
    "    mx = round.(Int,m(X))\n",
    "    table = crosstab(mx,y) # tab_1,2 = x=0, y=1; tab_2,1 = x=1,y=0\n",
    "    sens = table[2,2] / (table[2,2] + table[1,2]) # tp / tp + fn\n",
    "    spez = table[1,1] / (table[1,1] + table[2,1]) # tn / tn + fp\n",
    "    acc = (table[1,1] + table[2,2]) / sum(table) # tp + tn / all\n",
    "    return sens, spez, acc\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at how many cells from the train data were classified into each of the two types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = round.(Int,sigmoid_model(Xtrain))\n",
    "perc_gaba = round(sum(model_output)/length(model_output)*100, digits=2)\n",
    "@info \"$perc_gaba % of all cells classified as GABA-ergic\"\n",
    "\n",
    "# sanity check \n",
    "perc_gluta = round(sum(model_output.==0.0)/length(model_output)*100, digits=2)\n",
    "@info \"$perc_gluta % of all cells classified as Glutamatergic\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the accuracy for the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens, spez, overall_acc = accuracy(sigmoid_model, Xtrain, ytrain)\n",
    "sens, spez, overall_acc = accuracy(sigmoid_model, Xtrain, ytrain)\n",
    "\n",
    "@info \"% of cells correctly classified as GABA-ergic:\" round(sens*100,digits=2)\n",
    "@info \"% of cells correctly classified as Glutamatergic:\" round(spez*100,digits=2)\n",
    "@info \"% of cells classified as Glutamatergic:\" round(overall_acc*100,digits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so at the moment, our network classifies the cells from the training data mostly as Glutamatergic, which is of course not what we want. So we need to train our network, i.e., optimise the loss function with respect to the parameters `W` and `b` of our network. \n",
    "\n",
    "Let's see how we can do that using `Flux`. \n",
    "\n",
    "To optimise the loss function, we use a stochastic and modified version of gradient descent, specifically implemented in the ADAM optimiser. We thus need to define the learningrate (=stepsize in the gradient descent algorithm), the optimizer and the number of training epochs for which we want to run the stochastic gradient descent algorithm. \n",
    "Finally, we need to tell `Flux` which parameters we want to optimise, i.e., with respect to which parameters we would like to get the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "lr = 0.003 \n",
    "opt = ADAM(lr)\n",
    "nepochs = 100  \n",
    "ps = Flux.params(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we also need to pass the data in the right form. As typically done in neural network training, we partition the data into *batches*. During training, the loss function is calculated for one batch at a time, and after each batch, one parameter update is made by the optimiser. Thus, the parameters are not updated after each individual observation, but after each batch, i.e., each small group of observations. This increases training stability, as single outliers in the data cannot drastically the parameters and the effect of outliers is somewhat mitigated. \n",
    "\n",
    "So we need to partition the train data into batches of a pre-specified size, and each batch should contain the data observations, i.e., the gene expression profiles of the cells in the batch, together with the correct GABA- vs. Glutamatergic label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = Flux.DataLoader((Xtrain, ytrain'), batchsize=10, shuffle=true)\n",
    "\n",
    "println(\"dimensions of the first train-batch for Xtrain:$(size(first(trainingdata)[1]))\")\n",
    "println(\"dimensions of the first train-batch for ytrain:$(size(first(trainingdata)[2]))\")\n",
    "\n",
    "println(\"first train-batch:\")\n",
    "first(trainingdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training! In every epoch, we go through all batches of trainingdata. For each batch, we calculate the gradient of the loss w.r.t the parameters of the model, and update the parameters based on the SDG optimiser. \n",
    "\n",
    "During training, we track the loss and the accuracy of the model on the train- and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to track loss and accuracy for plotting:\n",
    "trainlosses = []\n",
    "trainaccuracies = []\n",
    "testlosses = []\n",
    "testaccuracies = []\n",
    "\n",
    "#train-loop:\n",
    "for ep in 1:nepochs\n",
    "    for batch in trainingdata\n",
    "        gs = Flux.gradient(ps) do \n",
    "            loss(model, batch...)\n",
    "        end\n",
    "        Flux.Optimise.update!(opt, ps, gs)\n",
    "    end\n",
    "    push!(trainlosses, loss(model, Xtrain, ytrain'))\n",
    "    push!(trainaccuracies, accuracy(sigmoid_model, Xtrain, ytrain')[3])  \n",
    "    push!(testlosses, loss(model, Xtest, ytest'))\n",
    "    push!(testaccuracies, accuracy(sigmoid_model, Xtest, ytest')[3]) \n",
    "     \n",
    "    if ep % Int(nepochs/5) == 0 # show loss only at 20 timepoints\n",
    "        @info \"epoch$(ep): current train-loss: $(loss(model, Xtrain, ytrain')), train-accuracy: $(accuracy(sigmoid_model, Xtrain, ytrain')[3]), current test-loss: $(loss(model, Xtest, ytest')), test-accuracy: $(accuracy(sigmoid_model, Xtest, ytest')[3]) ])\"\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the losses and accuracies for train and test data which we collected in the training of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot(collect(1:nepochs), hcat(log.(10, trainlosses .+ 1), log.(10, testlosses .+1 )), title = \"Loss\", label=[\"train-loss\" \"test-loss\"], xlabel=\"number of epoch\", ylabel=\"log10(model loss)\", legend=:topright)\n",
    "p2 = plot(collect(1:nepochs), hcat(trainaccuracies, testaccuracies), title = \"Accuracy\", label=[\"train-accuracy\" \"test-accuracy\"], xlabel=\"number of epoch\", ylabel=\"model accuracy\", legend=:bottomright)\n",
    "\n",
    "plot(p1,p2, layout=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks nice! The left graphic shows that our modle can reach nearly 0 loss on the train dataset and a very low loss on the test dataset. Also our model can classify nearly any cell from the train- and test data correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick any cell from the test dataset and have a look if our model can classify it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can pick any number from 1 up to 525 to check our model prediction:\n",
    "testcell = 112 # test data contains 525 cells\n",
    "\n",
    "@info \"Model prediction for testcell $(testcell): $(1 == Int(round(sigmoid_model(Xtest[:, testcell])[1])) ? \"GABAeric\" : \"Glutamatergic\")\"\n",
    "@info \"True label of the testcell $(testcell): $(1 == Int(ytest[testcell]) ? \"GABAeric\" : \"Glutamatergic\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** How does the number of epochs and the learning rate effect the loss and the accuracy? What happens if you add more layers? \n",
    ">\n",
    "> *Hint 1:* To re-start training from scratch, you have to re-run the cell where you can choose different hyperparameters for the training and re-initialise the parameters for our neural network (*see next code cell*). Choose your preferred hyperparameters and then re-run the training cell (*the one with the train-loop above*).\n",
    ">\n",
    "> *Hint 2:* If you want to add more layers, keep in mind that you can place an arbitrary activation function after every middle-layer but not after the output-layer, because we reserve the sigmoid activation function for this one in our binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to make writing models easier, we can define a `struct` to represent a fully connected layer and tell Julia how the layer operates on its input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct FullyConnected\n",
    "    W\n",
    "    b \n",
    "end\n",
    "\n",
    "FullyConnected(indim::Int,outdim::Int) = \n",
    "    FullyConnected(randn(outdim,indim), randn(outdim))\n",
    "\n",
    "(l::FullyConnected)(x) = l.W * x .+ l.b\n",
    "\n",
    "l = FullyConnected(10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much exactly what is implemented in the Flux `Dense` layer. Additionally, `Flux` provides convenience functions to quickly construct different layers, chain them together, and automatically extract their parameters for optimisation. \n",
    "\n",
    "Written with `Flux.jl`, a simple two-layer model would become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Chain(Dense(ngenes, 100, tanh), Dense(100, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
