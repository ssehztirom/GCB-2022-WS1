{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Practicals:  Integrated Gradients (IG) as an Explainable Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, we will \n",
    " * learn about Integrated Gradients (IG) implemeted in Julia on Gene Expression data.\n",
    " * build a simple neural network and train it to classify cells.\n",
    " * apply the IG on the gene expression data and invistigate the results. \n",
    " * Discuss: how to choose a baseline for IG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretablity in Machine Learning\n",
    "\n",
    "we will start off by having a look at the taxonomy of the interpretablity methods. (probably was shown by my colleague **Martin** ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/taxonomy.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated gradients (IG)\n",
    "\n",
    "IG is an explainable technique with many uses case, including but not limited to feature importance. The main aim of IG is to explain the relationship between the **model's prediction** & the **features** of the data without sacrificing accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see IG applied on images\n",
    "![](figures/mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory behind Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will walk through the mathematics behind integrated gradients; then we will turn the equations into a Julia code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at the formula and break it down:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/IG_eq1.png)\n",
    "\n",
    "**Where**<br>\n",
    "$i$ = featrure <br>\n",
    "$x_{i}$ = input <br> \n",
    "$x^\\prime$ = baseline <br> \n",
    "$\\alpha$ = interpolation constant <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We find an approximation to compute the integral numerically "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/IG_app.png)\n",
    "\n",
    "**Where**<br>\n",
    "$i$ = featrure <br>\n",
    "$x_{i}$ = input <br> \n",
    "$x^\\prime$ = baseline <br> \n",
    "$k$ = scaled feature perturbation constant <br> \n",
    "$m$ = number of steps in the Riemann sum approximation of the integral <br> \n",
    "$(x_{i}^{\\prime} - x_{i})$ a term for the difference from the baselin <br> \n",
    " <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the above-mentioned formula as a Julia code ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "(Same from first practical session)\n",
    "In the `Project.toml` and `Manifest.toml` file in the folder in the repository, all the necessary packages and their versions are specified. We don't have to manually install them, but can simply tell Julia to configure the environment as specified in these files, including installing all necessary dependencies. This can be done with the Julia package manager with the following commands: \n",
    "\n",
    "For more info on the Julia package manager, see [here](https://docs.julialang.org/en/v1/stdlib/Pkg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/GCB_Workshop/gcb2022-sc-gen-tutorial/PracticalsIII`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/GCB_Workshop/gcb2022-sc-gen-tutorial/PracticalsIII/Project.toml`\n",
      " \u001b[90m [4fba245c] \u001b[39mArrayInterface v6.0.22\n",
      " \u001b[90m [159f3aea] \u001b[39mCairo v1.0.5\n",
      " \u001b[90m [aaaa29a8] \u001b[39mClustering v0.14.2\n",
      " \u001b[90m [5ae59095] \u001b[39mColors v0.12.8\n",
      " \u001b[90m [a93c6f00] \u001b[39mDataFrames v1.3.4\n",
      " \u001b[90m [b4f34e82] \u001b[39mDistances v0.10.7\n",
      " \u001b[90m [31c24e10] \u001b[39mDistributions v0.25.68\n",
      " \u001b[90m [587475ba] \u001b[39mFlux v0.13.5\n",
      " \u001b[90m [f6369f11] \u001b[39mForwardDiff v0.10.32\n",
      " \u001b[90m [28b8d3ca] \u001b[39mGR v0.66.2\n",
      " \u001b[90m [92fee26a] \u001b[39mGZip v0.5.1\n",
      " \u001b[90m [c91e804a] \u001b[39mGadfly v1.3.4\n",
      " \u001b[90m [916415d5] \u001b[39mImages v0.25.2\n",
      "\u001b[32m⌃\u001b[39m\u001b[90m [eb30cadb] \u001b[39mMLDatasets v0.5.16\n",
      " \u001b[90m [3b7a836e] \u001b[39mPGFPlots v3.4.2\n",
      " \u001b[90m [d96e819e] \u001b[39mParameters v0.12.3\n",
      " \u001b[90m [91a5bcdd] \u001b[39mPlots v1.31.7\n",
      " \u001b[90m [92933f4c] \u001b[39mProgressMeter v1.7.2\n",
      " \u001b[90m [d330b81b] \u001b[39mPyPlot v2.11.0\n",
      " \u001b[90m [2913bbd2] \u001b[39mStatsBase v0.33.21\n",
      " \u001b[90m [24678dba] \u001b[39mTSne v1.3.0\n",
      " \u001b[90m [c4f8c510] \u001b[39mUMAP v0.1.9\n",
      " \u001b[90m [112f6efa] \u001b[39mVegaLite v2.6.0\n",
      "\u001b[32m⌃\u001b[39m\u001b[90m [fdbf4ff8] \u001b[39mXLSX v0.8.2\n",
      " \u001b[90m [add38077] \u001b[39mscVI v0.1.0 `https://github.com/maren-ha/scVI.jl#main`\n",
      " \u001b[90m [8bb1440f] \u001b[39mDelimitedFiles\n",
      " \u001b[90m [10745b16] \u001b[39mStatistics\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m have new versions available\n"
     ]
    }
   ],
   "source": [
    "using Pkg; # loading the package manager\n",
    "cd(@__DIR__) # change into the directory where the current file (this notebook) is located\n",
    "Pkg.activate(\".\") # activate the environment specified in the .toml files\n",
    "Pkg.instantiate() # install the necessary dependencies\n",
    "Pkg.status() # show which packages are currently installed in the environment\n",
    "using ForwardDiff\n",
    "using StatsBase\n",
    "using Images, Plots;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model that classifies the cells ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data (same dataset from first practical session)\n",
    "\n",
    "We will use a dataset from Tasic et al. (2016), where gene expression is profiled in 1525 single neurons, extracted from the cortices of mice. The authors reported a set of 104 genes (Figure 3 of Tasic et al.), based on which different subtypes of neurons can be discriminated and a set of 80 neurotransmitter receptor genes (Figure S15 of Tasic et al.), totalling to 184 genes. \n",
    "\n",
    "The data from Tasic et al. (2016) has been stored in the [Gene expression Omnibus (GEO)](https://www.ncbi.nlm.nih.gov/geo/) under accession number GSE71585. \n",
    "\n",
    "We have downloaded the expression data and the correspoding sample description sheet from the GEO and removed the samples from the expression data which have been flagged by Tasic et al. (2016) as being of bad quality. Since the samples of bad quality are not contained within the sample description sheet, we can filter the samples of bad quality by comparing the sample IDs in the expression data with the IDs in the sample description sheet. \n",
    "\n",
    "Further, we have removed genes which are expressed with less than 10 aligned reads in less than 20 samples. This is necessary since many genes are usually expressed only in a single cell and consequently these genes do not substantially represent the structure in the data.\n",
    "\n",
    "Next, the data has been normalized for sequencing depth using an algorithm from DESeq (Anders and Huber, 2010).\n",
    "Normalization for sequencing depth is important since the number of aligned reads per sample fluctuate which affects the estimated expression level for each gene. The algorithm of Anders and Huber (2010) calculates correction factors, one for each sequencing library, i.e. sample, by which the sequencing data are divided. \n",
    "\n",
    "The data contains expression levels of over 15.119 genes in 1679 samples. To lower computational demands of the following analysis while still maintaining most of the biological signal int he dataset, we have subsetted the data using a set of genes identified as most relevant by Tasic et al. (2016): Specifically, we use the marker genes which are known to indicate cell type membership ([Figure 3](https://www.nature.com/articles/nn.4216/figures/3) in Tasic et al. (2016)) and the neurotransmitter receptorgenes which are differentially expressed in different neuron cell types but have a less pronounced expression pattern (supplementary material, [Figure S 15](https://static-content.springer.com/esm/art%3A10.1038%2Fnn.4216/MediaObjects/41593_2016_BFnn4216_MOESM67_ESM.pdf))\n",
    " \n",
    "Finally, we selected only the neural cells using the cell type annotation provided by Tasic et al. (2016), and annotated the cell types as GABAergic vs. Glutamatergic. \n",
    "\n",
    "The complete preprocessing can be reproduced by running the script `preprocess_Tasic.jl`, where at the end the following four files are saved to the `data` subfolder: \n",
    "\n",
    " * `genes.txt` - list of selected genes \n",
    " * `single_cell_mat` - the normalized count matrix, subsetted to the relevant genes and neural cells \n",
    " * `celltype.txt` - the cell type annotation of the cells in `single_cell_mat.txt`, as provided by Tasic et al. (2016)\n",
    " * `gabagluta.txt` - the annotation of cells as GABAergic vs. Glutamatergic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "genenames = readdlm(\"data/genes.txt\")\n",
    "mat_norm = readdlm(\"data/single_cell_mat.txt\")\n",
    "celltype = readdlm(\"data/celltype.txt\")\n",
    "gabagluta = readdlm(\"data/gabagluta.txt\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many genes and cells we have, and some more information about the number of different cell types and Gaba- vs Glutamatergic cells: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix is of size (180, 1525)\n",
      "Number of genes is 180\n",
      "Number of cells is 1525\n",
      "There are 15 different cell types\n",
      "There are 761 GABA-ergic cells and 764 Glutamatergic cells\n"
     ]
    }
   ],
   "source": [
    "ngenes, ncells = size(mat_norm)\n",
    "ncelltypes = length(unique(celltype))\n",
    "ngaba = length(gabagluta[gabagluta.==\"GABA\"])\n",
    "ngluta = length(gabagluta[gabagluta.==\"Glutamate\"])\n",
    "\n",
    "println(\"Data matrix is of size \", size(mat_norm))\n",
    "println(\"Number of genes is \", length(genenames))\n",
    "println(\"Number of cells is \", length(celltype))\n",
    "\n",
    "println(\"There are $ncelltypes different cell types\")\n",
    "println(\"There are $ngaba GABA-ergic cells and $ngluta Glutamatergic cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with a countmatrix with 1679 cells and 15119 genes\n",
       "   layers dict with the following keys: [\"normalized_counts\", \"counts\"]\n",
       "   unique celltypes: [\"Vip\", \"L4\", \"L2/3\", \"L2\", \"Pvalb\", \"Ndnf\", \"L5a\", \"SMC\", \"Astro\", \"L5\", \"Micro\", \"Endo\", \"Sst\", \"L6b\", \"Sncg\", \"Igtp\", \"Oligo\", \"Smad3\", \"OPC\", \"L5b\", \"L6a\"]\n",
       "   training status: not trained\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using scVI\n",
    "adata = scVI.load_tasic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with a countmatrix with 1525 cells and 180 genes\n",
       "   layers dict with the following keys: [\"normalized_counts\", \"counts\"]\n",
       "   unique celltypes: [\"Vip\", \"L4\", \"L2/3\", \"L2\", \"Pvalb\", \"Ndnf\", \"L5a\", \"L5\", \"Sst\", \"L6b\", \"Sncg\", \"Igtp\", \"Smad3\", \"L5b\", \"L6a\"]\n",
       "   training status: not trained\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we take a subset of the data ... \n",
    "subset_tasic!(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes: 15\n"
     ]
    }
   ],
   "source": [
    "nclasses = length(unique(adata.celltypes))\n",
    "println(\"The number of classes: \",  nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple classification model using `Flux`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassNet (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ClassNet(; ngenes=180, nclasses=15) \n",
    "    \n",
    "    nhidden = 128\n",
    "    class_model = Chain(Dense(ngenes, nhidden, relu),\n",
    "                    Dense(nhidden,nclasses))\n",
    "    return class_model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(180 => 128, relu),              \u001b[90m# 23_168 parameters\u001b[39m\n",
       "  Dense(128 => 15),                     \u001b[90m# 1_935 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m25_103 parameters, 98.309 KiB."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_model = ClassNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Classification Model model has: 25103 trainable params\n",
      "└ @ Main In[11]:5\n"
     ]
    }
   ],
   "source": [
    "## Utility function returns the number of parameters ... \n",
    "num_params(model) = sum(length, Flux.params(model)) \n",
    "round4(x) = round(x, digits=4)\n",
    "#num_of_params = num_params(conv_model)\n",
    "@info \"Classification Model model has: $(num_params(class_model)) trainable params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Parameters \n",
    "# we set up training parameters \n",
    "Base.@kwdef mutable struct Args\n",
    "    η = 1e-5            ## learning rate\n",
    "    λ = 0                ## L2 regularizer param, implemented as weight decay\n",
    "    batchsize = 128      ## batch size\n",
    "    epochs = 1000          ## number of epochs\n",
    "    seed = 0             ## set seed > 0 for reproducibility\n",
    "    use_cuda = false      ## if true use cuda (if available)\n",
    "    infotime = 1 \t     ## report every `infotime` epochs\n",
    "    checktime = 5        ## Save the model every `checktime` epochs. Set to 0 for no checkpoints.\n",
    "    tblogger = true      ## log training with tensorboard\n",
    "    savepath = \"runs/\"   ## results path\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_data (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data to train and test .. \n",
    "function get_data(args)\n",
    "    \n",
    "    labels = unique(adata.celltypes)\n",
    "    \n",
    "    ytrain = Flux.onehotbatch(adata.celltypes[1:1000], labels)\n",
    "    Xtrain = (log.(adata.countmatrix'[:, 1:1000].+1))\n",
    "    train_loader = DataLoader((Xtrain,ytrain), batchsize=args.batchsize,shuffle=true)\n",
    "    \n",
    "    # Load train & test in dataloaders as mini-batches of data of size: 128 \n",
    "    ytest = Flux.onehotbatch(adata.celltypes[1001:end], labels)\n",
    "    Xtest = (log.(adata.countmatrix'[:, 1001:end].+1))\n",
    "    test_loader = DataLoader((Xtest, ytest),  batchsize=args.batchsize)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Loss Function: Cross Entropy\n",
    "\n",
    "![](figures/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(ŷ, y) = logitcrossentropy(ŷ, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_loss_accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eval_loss_accuracy(loader, model, device) # device = cpu since I don't have gpu on my machine ....\n",
    "    l = 0f0\n",
    "    acc = 0\n",
    "    ntot = 0\n",
    "    for (x, y) in loader\n",
    "        x, y = x |> device, y |> device\n",
    "        ŷ = model(x)\n",
    "        l += loss(ŷ, y) * size(x)[end]        \n",
    "        acc += sum(onecold(ŷ |> cpu) .== onecold(y |> cpu))\n",
    "        ntot += size(x)[end]\n",
    "    end\n",
    "    return (loss = l/ntot |> round4, acc = acc/ntot*100 |> round4)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Flux.Optimise: Optimiser, WeightDecay\n",
    "using Flux: onehotbatch, onecold, flatten\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Statistics, Random\n",
    "using Logging: with_logger\n",
    "using TensorBoardLogger: TBLogger, tb_overwrite, set_step!, set_step_increment!\n",
    "using ProgressMeter: @showprogress\n",
    "import MLDatasets\n",
    "import BSON\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classification Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's train the model ... \n",
    "\n",
    "function train(; kws...)\n",
    "    args = Args(; kws...)\n",
    "    # create seed ....\n",
    "    args.seed > 0 && Random.seed!(args.seed)\n",
    "    use_cuda = args.use_cuda && CUDA.functional()\n",
    "    \n",
    "    if use_cuda\n",
    "        device = gpu\n",
    "        @info \"Training on GPU\"\n",
    "    else\n",
    "        device = cpu\n",
    "        @info \"Training on CPU\"\n",
    "    end\n",
    "\n",
    "    ## Get the data ... \n",
    "    train_loader, test_loader = get_data(args)\n",
    "    @info \"Dataset: $(train_loader.nobs) train and $(test_loader.nobs) test examples\"\n",
    "\n",
    "    ## Create MODEL & optimizer \n",
    "    model = ClassNet()|> device\n",
    "    @info \"ClassNet5 model: $(num_params(model)) trainable params\"    \n",
    "    \n",
    "    ps = Flux.params(model)  \n",
    "    \n",
    "    # We use ADAM ... \n",
    "    opt = ADAM(args.η) \n",
    "    if args.λ > 0 ## add weight decay, equivalent to L2 regularization\n",
    "        opt = Optimiser(WeightDecay(args.λ), opt)\n",
    "    end\n",
    "    \n",
    "    ## Log with tensorboard ...\n",
    "    if args.tblogger \n",
    "        tblogger = TBLogger(args.savepath, tb_overwrite)\n",
    "        set_step_increment!(tblogger, 0) ## 0 auto increment since we manually set_step!\n",
    "        @info \"TensorBoard logging at \\\"$(args.savepath)\\\"\"\n",
    "    end\n",
    "    ## Report a validation loss and accuracy ...\n",
    "    function report(epoch)\n",
    "        train = eval_loss_accuracy(train_loader, model, device)\n",
    "        test = eval_loss_accuracy(test_loader, model, device)        \n",
    "        println(\"Epoch: $epoch   Train: $(train)   Test: $(test)\")\n",
    "        if args.tblogger\n",
    "            set_step!(tblogger, epoch)\n",
    "            with_logger(tblogger) do\n",
    "                @info \"train\" loss=train.loss  acc=train.acc\n",
    "                @info \"test\"  loss=test.loss   acc=test.acc\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## START TRAINING\n",
    "    @info \"Start Training\"\n",
    "    report(0)\n",
    "    trainloss = []\n",
    "    testloss = []\n",
    "    \n",
    "    trainacc = []\n",
    "    testacc = []\n",
    "    for epoch in 1:args.epochs\n",
    "        @showprogress for (x, y) in train_loader\n",
    "            x, y = x |> device, y |> device\n",
    "            gs = Flux.gradient(ps) do\n",
    "                    ŷ = model(x)\n",
    "                    loss(ŷ, y)\n",
    "                end\n",
    "\n",
    "            Flux.Optimise.update!(opt, ps, gs)\n",
    "        end\n",
    "        \n",
    "        ## Printing and logging\n",
    "        epoch % args.infotime == 0 && report(epoch)\n",
    "        if args.checktime > 0 && epoch % args.checktime == 0\n",
    "            !ispath(args.savepath) && mkpath(args.savepath)\n",
    "            modelpath = joinpath(args.savepath, \"model.bson\") \n",
    "            let model = cpu(model) ## return model to cpu before serialization\n",
    "                BSON.@save modelpath model epoch\n",
    "            end\n",
    "            @info \"Model saved in \\\"$(modelpath)\\\"\"\n",
    "        end\n",
    "            train_eval_acc = eval_loss_accuracy(train_loader, model, device)\n",
    "            test_eval_acc = eval_loss_accuracy(test_loader, model, device)\n",
    "            push!(trainloss,train_eval_acc.loss )\n",
    "            push!(testloss, test_eval_acc.loss)\n",
    "            \n",
    "            push!(trainacc,train_eval_acc.acc )\n",
    "            push!(testacc,train_eval_acc.acc )\n",
    "            \n",
    "    end\n",
    "    return model, test_loader ,trainloss, testloss, trainacc, testacc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training on CPU\n",
      "└ @ Main In[22]:14\n",
      "┌ Info: Dataset: 1000 train and 525 test examples\n",
      "└ @ Main In[22]:19\n",
      "┌ Info: ClassNet5 model: 25103 trainable params\n",
      "└ @ Main In[22]:23\n",
      "┌ Info: TensorBoard logging at \"runs/\"\n",
      "└ @ Main In[22]:37\n",
      "┌ Info: Start Training\n",
      "└ @ Main In[22]:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Train: (loss = 5.0721f0, acc = 16.5)   Test: (loss = 5.3188f0, acc = 6.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:38\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Train: (loss = 4.9948f0, acc = 16.0)   Test: (loss = 5.2392f0, acc = 6.6667)\n",
      "Epoch: 2   Train: (loss = 4.9204f0, acc = 16.1)   Test: (loss = 5.1615f0, acc = 6.8571)\n",
      "Epoch: 3   Train: (loss = 4.8482f0, acc = 15.9)   Test: (loss = 5.0868f0, acc = 7.0476)\n",
      "Epoch: 4   Train: (loss = 4.7784f0, acc = 15.6)   Test: (loss = 5.0143f0, acc = 7.0476)\n",
      "Epoch: 5   Train: (loss = 4.71f0, acc = 15.6)   Test: (loss = 4.9436f0, acc = 6.8571)\n",
      "Epoch: 6   Train: (loss = 4.644f0, acc = 15.2)   Test: (loss = 4.8755f0, acc = 6.6667)\n",
      "Epoch: 7   Train: (loss = 4.5793f0, acc = 15.0)   Test: (loss = 4.8101f0, acc = 6.6667)\n",
      "Epoch: 8   Train: (loss = 4.5162f0, acc = 14.8)   Test: (loss = 4.7459f0, acc = 6.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9   Train: (loss = 4.4556f0, acc = 14.7)   Test: (loss = 4.685f0, acc = 6.4762)\n",
      "Epoch: 10   Train: (loss = 4.3964f0, acc = 14.7)   Test: (loss = 4.6251f0, acc = 6.6667)\n",
      "Epoch: 11   Train: (loss = 4.3378f0, acc = 14.5)   Test: (loss = 4.5665f0, acc = 6.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12   Train: (loss = 4.2809f0, acc = 14.6)   Test: (loss = 4.5102f0, acc = 6.6667)\n",
      "Epoch: 13   Train: (loss = 4.2254f0, acc = 15.0)   Test: (loss = 4.456f0, acc = 6.6667)\n",
      "Epoch: 14   Train: (loss = 4.171f0, acc = 14.8)   Test: (loss = 4.4029f0, acc = 6.8571)\n",
      "Epoch: 15   Train: (loss = 4.1176f0, acc = 14.9)   Test: (loss = 4.3512f0, acc = 6.6667)\n",
      "Epoch: 16   Train: (loss = 4.0658f0, acc = 14.7)   Test: (loss = 4.3015f0, acc = 6.4762)\n",
      "Epoch: 17   Train: (loss = 4.0147f0, acc = 15.0)   Test: (loss = 4.2524f0, acc = 6.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18   Train: (loss = 3.9645f0, acc = 15.0)   Test: (loss = 4.205f0, acc = 6.6667)\n",
      "Epoch: 19   Train: (loss = 3.9156f0, acc = 15.0)   Test: (loss = 4.1581f0, acc = 7.0476)\n",
      "Epoch: 20   Train: (loss = 3.8678f0, acc = 15.2)   Test: (loss = 4.1131f0, acc = 7.0476)\n",
      "Epoch: 21   Train: (loss = 3.8207f0, acc = 15.4)   Test: (loss = 4.0695f0, acc = 7.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22   Train: (loss = 3.7743f0, acc = 15.7)   Test: (loss = 4.0266f0, acc = 7.2381)\n",
      "Epoch: 23   Train: (loss = 3.7289f0, acc = 15.6)   Test: (loss = 3.9849f0, acc = 7.4286)\n",
      "Epoch: 24   Train: (loss = 3.6844f0, acc = 15.7)   Test: (loss = 3.9444f0, acc = 7.619)\n",
      "Epoch: 25   Train: (loss = 3.6409f0, acc = 16.0)   Test: (loss = 3.9046f0, acc = 7.619)\n",
      "Epoch: 26   Train: (loss = 3.5977f0, acc = 16.4)   Test: (loss = 3.8658f0, acc = 7.619)\n",
      "Epoch: 27   Train: (loss = 3.5555f0, acc = 16.5)   Test: (loss = 3.8265f0, acc = 7.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28   Train: (loss = 3.5141f0, acc = 16.6)   Test: (loss = 3.7896f0, acc = 7.8095)\n",
      "Epoch: 29   Train: (loss = 3.4734f0, acc = 16.5)   Test: (loss = 3.7534f0, acc = 7.8095)\n",
      "Epoch: 30   Train: (loss = 3.4336f0, acc = 16.5)   Test: (loss = 3.7184f0, acc = 7.8095)\n",
      "Epoch: 31   Train: (loss = 3.3944f0, acc = 16.6)   Test: (loss = 3.6839f0, acc = 8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32   Train: (loss = 3.3562f0, acc = 16.6)   Test: (loss = 3.6501f0, acc = 8.0)\n",
      "Epoch: 33   Train: (loss = 3.3186f0, acc = 16.7)   Test: (loss = 3.6166f0, acc = 8.1905)\n",
      "Epoch: 34   Train: (loss = 3.282f0, acc = 16.9)   Test: (loss = 3.5851f0, acc = 8.1905)\n",
      "Epoch: 35   Train: (loss = 3.246f0, acc = 17.4)   Test: (loss = 3.5539f0, acc = 8.1905)\n",
      "Epoch: 36   Train: (loss = 3.2101f0, acc = 17.8)   Test: (loss = 3.5231f0, acc = 8.1905)\n",
      "Epoch: 37   Train: (loss = 3.1753f0, acc = 17.8)   Test: (loss = 3.4929f0, acc = 8.381)\n",
      "Epoch: 38   Train: (loss = 3.1414f0, acc = 17.9)   Test: (loss = 3.4641f0, acc = 8.5714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39   Train: (loss = 3.1077f0, acc = 18.4)   Test: (loss = 3.4349f0, acc = 8.5714)\n",
      "Epoch: 40   Train: (loss = 3.0747f0, acc = 19.0)   Test: (loss = 3.4077f0, acc = 8.7619)\n",
      "Epoch: 41   Train: (loss = 3.0421f0, acc = 19.0)   Test: (loss = 3.3801f0, acc = 8.7619)\n",
      "Epoch: 42   Train: (loss = 3.01f0, acc = 19.2)   Test: (loss = 3.3534f0, acc = 9.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43   Train: (loss = 2.979f0, acc = 19.7)   Test: (loss = 3.3283f0, acc = 9.9048)\n",
      "Epoch: 44   Train: (loss = 2.9485f0, acc = 20.0)   Test: (loss = 3.3026f0, acc = 10.2857)\n",
      "Epoch: 45   Train: (loss = 2.9182f0, acc = 20.1)   Test: (loss = 3.2779f0, acc = 10.6667)\n",
      "Epoch: 46   Train: (loss = 2.8882f0, acc = 20.5)   Test: (loss = 3.2536f0, acc = 10.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47   Train: (loss = 2.8591f0, acc = 20.4)   Test: (loss = 3.229f0, acc = 10.8571)\n",
      "Epoch: 48   Train: (loss = 2.8304f0, acc = 20.5)   Test: (loss = 3.2062f0, acc = 11.0476)\n",
      "Epoch: 49   Train: (loss = 2.8021f0, acc = 20.9)   Test: (loss = 3.1829f0, acc = 11.2381)\n",
      "Epoch: 50   Train: (loss = 2.7749f0, acc = 21.1)   Test: (loss = 3.1609f0, acc = 11.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51   Train: (loss = 2.7478f0, acc = 21.3)   Test: (loss = 3.1394f0, acc = 11.8095)\n",
      "Epoch: 52   Train: (loss = 2.7213f0, acc = 22.1)   Test: (loss = 3.1175f0, acc = 12.381)\n",
      "Epoch: 53   Train: (loss = 2.6951f0, acc = 22.5)   Test: (loss = 3.0962f0, acc = 12.7619)\n",
      "Epoch: 54   Train: (loss = 2.6696f0, acc = 22.6)   Test: (loss = 3.0764f0, acc = 13.1429)\n",
      "Epoch: 55   Train: (loss = 2.6443f0, acc = 23.2)   Test: (loss = 3.0561f0, acc = 13.1429)\n",
      "Epoch: 56   Train: (loss = 2.6195f0, acc = 23.6)   Test: (loss = 3.0361f0, acc = 13.5238)\n",
      "Epoch: 57   Train: (loss = 2.5951f0, acc = 23.7)   Test: (loss = 3.0167f0, acc = 14.2857)\n",
      "Epoch: 58   Train: (loss = 2.5708f0, acc = 24.3)   Test: (loss = 2.9977f0, acc = 14.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59   Train: (loss = 2.5472f0, acc = 24.8)   Test: (loss = 2.9789f0, acc = 14.6667)\n",
      "Epoch: 60   Train: (loss = 2.5238f0, acc = 25.4)   Test: (loss = 2.9603f0, acc = 15.4286)\n",
      "Epoch: 61   Train: (loss = 2.5009f0, acc = 25.6)   Test: (loss = 2.9416f0, acc = 16.1905)\n",
      "Epoch: 62   Train: (loss = 2.478f0, acc = 25.9)   Test: (loss = 2.9235f0, acc = 16.5714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63   Train: (loss = 2.4556f0, acc = 26.0)   Test: (loss = 2.9057f0, acc = 16.5714)\n",
      "Epoch: 64   Train: (loss = 2.4335f0, acc = 26.3)   Test: (loss = 2.8879f0, acc = 17.3333)\n",
      "Epoch: 65   Train: (loss = 2.4117f0, acc = 26.6)   Test: (loss = 2.8702f0, acc = 17.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66   Train: (loss = 2.3901f0, acc = 27.2)   Test: (loss = 2.8535f0, acc = 18.0952)\n",
      "Epoch: 67   Train: (loss = 2.369f0, acc = 27.6)   Test: (loss = 2.8367f0, acc = 18.4762)\n",
      "Epoch: 68   Train: (loss = 2.3482f0, acc = 27.7)   Test: (loss = 2.8202f0, acc = 19.0476)\n",
      "Epoch: 69   Train: (loss = 2.3279f0, acc = 28.0)   Test: (loss = 2.8038f0, acc = 19.619)\n",
      "Epoch: 70   Train: (loss = 2.3075f0, acc = 28.3)   Test: (loss = 2.7881f0, acc = 20.1905)\n",
      "Epoch: 71   Train: (loss = 2.2874f0, acc = 28.7)   Test: (loss = 2.7715f0, acc = 20.5714)\n",
      "Epoch: 72   Train: (loss = 2.2678f0, acc = 29.2)   Test: (loss = 2.7561f0, acc = 20.7619)\n",
      "Epoch: 73   Train: (loss = 2.2484f0, acc = 29.4)   Test: (loss = 2.7412f0, acc = 20.7619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74   Train: (loss = 2.2291f0, acc = 29.6)   Test: (loss = 2.7264f0, acc = 21.3333)\n",
      "Epoch: 75   Train: (loss = 2.2102f0, acc = 30.0)   Test: (loss = 2.7115f0, acc = 21.9048)\n",
      "Epoch: 76   Train: (loss = 2.1914f0, acc = 30.6)   Test: (loss = 2.6964f0, acc = 22.2857)\n",
      "Epoch: 77   Train: (loss = 2.1728f0, acc = 30.6)   Test: (loss = 2.6815f0, acc = 22.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78   Train: (loss = 2.1544f0, acc = 31.1)   Test: (loss = 2.6675f0, acc = 22.6667)\n",
      "Epoch: 79   Train: (loss = 2.1363f0, acc = 31.7)   Test: (loss = 2.6534f0, acc = 22.8571)\n",
      "Epoch: 80   Train: (loss = 2.1184f0, acc = 32.2)   Test: (loss = 2.6389f0, acc = 23.0476)\n",
      "Epoch: 81   Train: (loss = 2.1009f0, acc = 32.3)   Test: (loss = 2.6247f0, acc = 23.0476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82   Train: (loss = 2.0835f0, acc = 32.5)   Test: (loss = 2.6101f0, acc = 23.4286)\n",
      "Epoch: 83   Train: (loss = 2.0663f0, acc = 32.6)   Test: (loss = 2.597f0, acc = 23.4286)\n",
      "Epoch: 84   Train: (loss = 2.0493f0, acc = 32.9)   Test: (loss = 2.5833f0, acc = 23.8095)\n",
      "Epoch: 85   Train: (loss = 2.0326f0, acc = 32.9)   Test: (loss = 2.5708f0, acc = 23.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86   Train: (loss = 2.0159f0, acc = 33.2)   Test: (loss = 2.5567f0, acc = 23.8095)\n",
      "Epoch: 87   Train: (loss = 1.9994f0, acc = 33.4)   Test: (loss = 2.5442f0, acc = 24.1905)\n",
      "Epoch: 88   Train: (loss = 1.9832f0, acc = 33.9)   Test: (loss = 2.5305f0, acc = 24.381)\n",
      "Epoch: 89   Train: (loss = 1.967f0, acc = 34.4)   Test: (loss = 2.5176f0, acc = 24.7619)\n",
      "Epoch: 90   Train: (loss = 1.951f0, acc = 34.6)   Test: (loss = 2.505f0, acc = 25.3333)\n",
      "Epoch: 91   Train: (loss = 1.9353f0, acc = 35.2)   Test: (loss = 2.4927f0, acc = 25.7143)\n",
      "Epoch: 92   Train: (loss = 1.9197f0, acc = 35.8)   Test: (loss = 2.4806f0, acc = 26.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93   Train: (loss = 1.9044f0, acc = 36.3)   Test: (loss = 2.4684f0, acc = 26.8571)\n",
      "Epoch: 94   Train: (loss = 1.8891f0, acc = 36.4)   Test: (loss = 2.4567f0, acc = 27.2381)\n",
      "Epoch: 95   Train: (loss = 1.874f0, acc = 37.2)   Test: (loss = 2.4449f0, acc = 27.2381)\n",
      "Epoch: 96   Train: (loss = 1.859f0, acc = 37.9)   Test: (loss = 2.4334f0, acc = 27.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97   Train: (loss = 1.8443f0, acc = 38.6)   Test: (loss = 2.4216f0, acc = 28.0)\n",
      "Epoch: 98   Train: (loss = 1.8298f0, acc = 38.9)   Test: (loss = 2.4097f0, acc = 28.7619)\n",
      "Epoch: 99   Train: (loss = 1.8153f0, acc = 39.6)   Test: (loss = 2.3983f0, acc = 28.7619)\n",
      "Epoch: 100   Train: (loss = 1.801f0, acc = 40.1)   Test: (loss = 2.3867f0, acc = 29.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101   Train: (loss = 1.787f0, acc = 40.6)   Test: (loss = 2.3757f0, acc = 29.7143)\n",
      "Epoch: 102   Train: (loss = 1.773f0, acc = 41.1)   Test: (loss = 2.3651f0, acc = 30.0952)\n",
      "Epoch: 103   Train: (loss = 1.7592f0, acc = 41.8)   Test: (loss = 2.3541f0, acc = 30.8571)\n",
      "Epoch: 104   Train: (loss = 1.7455f0, acc = 42.4)   Test: (loss = 2.343f0, acc = 31.0476)\n",
      "Epoch: 105   Train: (loss = 1.7319f0, acc = 43.1)   Test: (loss = 2.3324f0, acc = 32.1905)\n",
      "Epoch: 106   Train: (loss = 1.7184f0, acc = 43.8)   Test: (loss = 2.3218f0, acc = 32.9524)\n",
      "Epoch: 107   Train: (loss = 1.7051f0, acc = 44.5)   Test: (loss = 2.3113f0, acc = 33.5238)\n",
      "Epoch: 108   Train: (loss = 1.6919f0, acc = 45.1)   Test: (loss = 2.3004f0, acc = 33.9048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109   Train: (loss = 1.6788f0, acc = 45.6)   Test: (loss = 2.2903f0, acc = 34.0952)\n",
      "Epoch: 110   Train: (loss = 1.6659f0, acc = 46.3)   Test: (loss = 2.2806f0, acc = 34.2857)\n",
      "Epoch: 111   Train: (loss = 1.6531f0, acc = 46.6)   Test: (loss = 2.2713f0, acc = 34.6667)\n",
      "Epoch: 112   Train: (loss = 1.6404f0, acc = 47.2)   Test: (loss = 2.2614f0, acc = 35.0476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113   Train: (loss = 1.6277f0, acc = 47.8)   Test: (loss = 2.251f0, acc = 35.0476)\n",
      "Epoch: 114   Train: (loss = 1.6153f0, acc = 48.5)   Test: (loss = 2.2418f0, acc = 35.2381)\n",
      "Epoch: 115   Train: (loss = 1.6031f0, acc = 49.0)   Test: (loss = 2.2318f0, acc = 35.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116   Train: (loss = 1.5909f0, acc = 49.3)   Test: (loss = 2.2225f0, acc = 36.0)\n",
      "Epoch: 117   Train: (loss = 1.5788f0, acc = 49.6)   Test: (loss = 2.2127f0, acc = 36.381)\n",
      "Epoch: 118   Train: (loss = 1.5669f0, acc = 50.0)   Test: (loss = 2.204f0, acc = 36.7619)\n",
      "Epoch: 119   Train: (loss = 1.5551f0, acc = 50.2)   Test: (loss = 2.1941f0, acc = 37.1429)\n",
      "Epoch: 120   Train: (loss = 1.5434f0, acc = 50.3)   Test: (loss = 2.1847f0, acc = 37.5238)\n",
      "Epoch: 121   Train: (loss = 1.5318f0, acc = 50.8)   Test: (loss = 2.1763f0, acc = 37.9048)\n",
      "Epoch: 122   Train: (loss = 1.5203f0, acc = 51.5)   Test: (loss = 2.1666f0, acc = 38.0952)\n",
      "Epoch: 123   Train: (loss = 1.5091f0, acc = 51.9)   Test: (loss = 2.1584f0, acc = 38.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124   Train: (loss = 1.4979f0, acc = 52.0)   Test: (loss = 2.1491f0, acc = 38.4762)\n",
      "Epoch: 125   Train: (loss = 1.4869f0, acc = 52.3)   Test: (loss = 2.1403f0, acc = 39.0476)\n",
      "Epoch: 126   Train: (loss = 1.476f0, acc = 52.7)   Test: (loss = 2.1321f0, acc = 39.4286)\n",
      "Epoch: 127   Train: (loss = 1.4652f0, acc = 52.8)   Test: (loss = 2.1238f0, acc = 39.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128   Train: (loss = 1.4545f0, acc = 53.4)   Test: (loss = 2.1161f0, acc = 39.8095)\n",
      "Epoch: 129   Train: (loss = 1.4439f0, acc = 53.6)   Test: (loss = 2.1069f0, acc = 40.1905)\n",
      "Epoch: 130   Train: (loss = 1.4335f0, acc = 53.9)   Test: (loss = 2.0981f0, acc = 40.1905)\n",
      "Epoch: 131   Train: (loss = 1.4232f0, acc = 54.1)   Test: (loss = 2.0901f0, acc = 41.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132   Train: (loss = 1.4129f0, acc = 54.5)   Test: (loss = 2.0828f0, acc = 41.7143)\n",
      "Epoch: 133   Train: (loss = 1.4028f0, acc = 55.0)   Test: (loss = 2.0746f0, acc = 41.9048)\n",
      "Epoch: 134   Train: (loss = 1.3928f0, acc = 55.5)   Test: (loss = 2.0662f0, acc = 42.4762)\n",
      "Epoch: 135   Train: (loss = 1.3829f0, acc = 55.9)   Test: (loss = 2.0578f0, acc = 42.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136   Train: (loss = 1.3731f0, acc = 56.5)   Test: (loss = 2.0504f0, acc = 42.6667)\n",
      "Epoch: 137   Train: (loss = 1.3635f0, acc = 56.9)   Test: (loss = 2.0428f0, acc = 42.6667)\n",
      "Epoch: 138   Train: (loss = 1.3538f0, acc = 57.5)   Test: (loss = 2.0356f0, acc = 43.0476)\n",
      "Epoch: 139   Train: (loss = 1.3442f0, acc = 58.0)   Test: (loss = 2.0272f0, acc = 43.619)\n",
      "Epoch: 140   Train: (loss = 1.3348f0, acc = 58.7)   Test: (loss = 2.0199f0, acc = 43.619)\n",
      "Epoch: 141   Train: (loss = 1.3255f0, acc = 59.0)   Test: (loss = 2.0131f0, acc = 44.1905)\n",
      "Epoch: 142   Train: (loss = 1.3162f0, acc = 59.2)   Test: (loss = 2.0056f0, acc = 44.381)\n",
      "Epoch: 143   Train: (loss = 1.3071f0, acc = 59.6)   Test: (loss = 1.9977f0, acc = 44.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144   Train: (loss = 1.298f0, acc = 59.8)   Test: (loss = 1.9895f0, acc = 45.1429)\n",
      "Epoch: 145   Train: (loss = 1.2891f0, acc = 60.0)   Test: (loss = 1.9834f0, acc = 45.5238)\n",
      "Epoch: 146   Train: (loss = 1.2802f0, acc = 60.6)   Test: (loss = 1.9762f0, acc = 45.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147   Train: (loss = 1.2714f0, acc = 61.0)   Test: (loss = 1.9694f0, acc = 45.9048)\n",
      "Epoch: 148   Train: (loss = 1.2627f0, acc = 61.1)   Test: (loss = 1.9619f0, acc = 46.2857)\n",
      "Epoch: 149   Train: (loss = 1.2542f0, acc = 61.6)   Test: (loss = 1.9543f0, acc = 47.2381)\n",
      "Epoch: 150   Train: (loss = 1.2457f0, acc = 61.8)   Test: (loss = 1.9478f0, acc = 47.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151   Train: (loss = 1.2373f0, acc = 62.0)   Test: (loss = 1.9409f0, acc = 48.1905)\n",
      "Epoch: 152   Train: (loss = 1.2289f0, acc = 62.3)   Test: (loss = 1.9332f0, acc = 48.1905)\n",
      "Epoch: 153   Train: (loss = 1.2206f0, acc = 62.3)   Test: (loss = 1.9264f0, acc = 48.5714)\n",
      "Epoch: 154   Train: (loss = 1.2125f0, acc = 62.8)   Test: (loss = 1.9198f0, acc = 49.3333)\n",
      "Epoch: 155   Train: (loss = 1.2043f0, acc = 62.9)   Test: (loss = 1.9132f0, acc = 49.7143)\n",
      "Epoch: 156   Train: (loss = 1.1963f0, acc = 63.4)   Test: (loss = 1.9064f0, acc = 49.9048)\n",
      "Epoch: 157   Train: (loss = 1.1883f0, acc = 63.7)   Test: (loss = 1.8997f0, acc = 49.9048)\n",
      "Epoch: 158   Train: (loss = 1.1804f0, acc = 63.8)   Test: (loss = 1.8934f0, acc = 49.9048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159   Train: (loss = 1.1726f0, acc = 64.3)   Test: (loss = 1.8869f0, acc = 50.2857)\n",
      "Epoch: 160   Train: (loss = 1.1649f0, acc = 64.8)   Test: (loss = 1.8804f0, acc = 50.2857)\n",
      "Epoch: 161   Train: (loss = 1.1573f0, acc = 65.2)   Test: (loss = 1.8738f0, acc = 50.4762)\n",
      "Epoch: 162   Train: (loss = 1.1497f0, acc = 65.4)   Test: (loss = 1.867f0, acc = 50.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163   Train: (loss = 1.1421f0, acc = 65.5)   Test: (loss = 1.8606f0, acc = 50.8571)\n",
      "Epoch: 164   Train: (loss = 1.1348f0, acc = 65.6)   Test: (loss = 1.8551f0, acc = 51.2381)\n",
      "Epoch: 165   Train: (loss = 1.1275f0, acc = 66.1)   Test: (loss = 1.8486f0, acc = 51.4286)\n",
      "Epoch: 166   Train: (loss = 1.1202f0, acc = 66.4)   Test: (loss = 1.843f0, acc = 51.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 167   Train: (loss = 1.113f0, acc = 66.6)   Test: (loss = 1.8362f0, acc = 52.0)\n",
      "Epoch: 168   Train: (loss = 1.1058f0, acc = 66.9)   Test: (loss = 1.8307f0, acc = 52.381)\n",
      "Epoch: 169   Train: (loss = 1.0987f0, acc = 67.2)   Test: (loss = 1.8247f0, acc = 52.381)\n",
      "Epoch: 170   Train: (loss = 1.0918f0, acc = 67.8)   Test: (loss = 1.8188f0, acc = 52.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171   Train: (loss = 1.0847f0, acc = 68.0)   Test: (loss = 1.8124f0, acc = 52.5714)\n",
      "Epoch: 172   Train: (loss = 1.0778f0, acc = 68.1)   Test: (loss = 1.8067f0, acc = 52.7619)\n",
      "Epoch: 173   Train: (loss = 1.071f0, acc = 68.2)   Test: (loss = 1.8009f0, acc = 52.7619)\n",
      "Epoch: 174   Train: (loss = 1.0643f0, acc = 68.4)   Test: (loss = 1.7946f0, acc = 52.9524)\n",
      "Epoch: 175   Train: (loss = 1.0576f0, acc = 68.6)   Test: (loss = 1.7891f0, acc = 53.5238)\n",
      "Epoch: 176   Train: (loss = 1.051f0, acc = 68.7)   Test: (loss = 1.784f0, acc = 53.5238)\n",
      "Epoch: 177   Train: (loss = 1.0444f0, acc = 69.0)   Test: (loss = 1.7788f0, acc = 53.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178   Train: (loss = 1.0379f0, acc = 69.0)   Test: (loss = 1.7731f0, acc = 53.9048)\n",
      "Epoch: 179   Train: (loss = 1.0314f0, acc = 69.4)   Test: (loss = 1.7675f0, acc = 54.0952)\n",
      "Epoch: 180   Train: (loss = 1.025f0, acc = 69.8)   Test: (loss = 1.7612f0, acc = 54.2857)\n",
      "Epoch: 181   Train: (loss = 1.0186f0, acc = 70.1)   Test: (loss = 1.7554f0, acc = 54.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182   Train: (loss = 1.0124f0, acc = 70.6)   Test: (loss = 1.7504f0, acc = 54.6667)\n",
      "Epoch: 183   Train: (loss = 1.0062f0, acc = 70.7)   Test: (loss = 1.7456f0, acc = 55.0476)\n",
      "Epoch: 184   Train: (loss = 1.0f0, acc = 70.8)   Test: (loss = 1.7401f0, acc = 55.2381)\n",
      "Epoch: 185   Train: (loss = 0.9939f0, acc = 71.1)   Test: (loss = 1.7348f0, acc = 55.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186   Train: (loss = 0.9879f0, acc = 71.2)   Test: (loss = 1.7296f0, acc = 55.619)\n",
      "Epoch: 187   Train: (loss = 0.9819f0, acc = 71.3)   Test: (loss = 1.724f0, acc = 55.619)\n",
      "Epoch: 188   Train: (loss = 0.9759f0, acc = 71.5)   Test: (loss = 1.7192f0, acc = 55.619)\n",
      "Epoch: 189   Train: (loss = 0.97f0, acc = 71.5)   Test: (loss = 1.7141f0, acc = 55.619)\n",
      "Epoch: 190   Train: (loss = 0.9642f0, acc = 71.6)   Test: (loss = 1.7084f0, acc = 55.8095)\n",
      "Epoch: 191   Train: (loss = 0.9584f0, acc = 71.6)   Test: (loss = 1.7038f0, acc = 56.1905)\n",
      "Epoch: 192   Train: (loss = 0.9526f0, acc = 71.7)   Test: (loss = 1.699f0, acc = 56.381)\n",
      "Epoch: 193   Train: (loss = 0.947f0, acc = 72.1)   Test: (loss = 1.6934f0, acc = 56.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194   Train: (loss = 0.9413f0, acc = 72.2)   Test: (loss = 1.6889f0, acc = 56.7619)\n",
      "Epoch: 195   Train: (loss = 0.9358f0, acc = 72.3)   Test: (loss = 1.6842f0, acc = 56.7619)\n",
      "Epoch: 196   Train: (loss = 0.9302f0, acc = 72.5)   Test: (loss = 1.6796f0, acc = 56.9524)\n",
      "Epoch: 197   Train: (loss = 0.9247f0, acc = 72.6)   Test: (loss = 1.6743f0, acc = 57.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198   Train: (loss = 0.9193f0, acc = 72.8)   Test: (loss = 1.6693f0, acc = 57.1429)\n",
      "Epoch: 199   Train: (loss = 0.9139f0, acc = 72.8)   Test: (loss = 1.6641f0, acc = 57.3333)\n",
      "Epoch: 200   Train: (loss = 0.9087f0, acc = 72.9)   Test: (loss = 1.66f0, acc = 57.3333)\n",
      "Epoch: 201   Train: (loss = 0.9035f0, acc = 73.3)   Test: (loss = 1.6556f0, acc = 57.3333)\n",
      "Epoch: 202   Train: (loss = 0.8982f0, acc = 73.6)   Test: (loss = 1.6505f0, acc = 57.7143)\n",
      "Epoch: 203   Train: (loss = 0.8931f0, acc = 73.7)   Test: (loss = 1.6458f0, acc = 57.9048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204   Train: (loss = 0.888f0, acc = 73.8)   Test: (loss = 1.6411f0, acc = 57.9048)\n",
      "Epoch: 205   Train: (loss = 0.8829f0, acc = 73.8)   Test: (loss = 1.6371f0, acc = 57.9048)\n",
      "Epoch: 206   Train: (loss = 0.8779f0, acc = 74.2)   Test: (loss = 1.632f0, acc = 57.9048)\n",
      "Epoch: 207   Train: (loss = 0.8729f0, acc = 74.4)   Test: (loss = 1.6277f0, acc = 58.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208   Train: (loss = 0.8679f0, acc = 74.5)   Test: (loss = 1.623f0, acc = 58.0952)\n",
      "Epoch: 209   Train: (loss = 0.8631f0, acc = 74.5)   Test: (loss = 1.6181f0, acc = 58.0952)\n",
      "Epoch: 210   Train: (loss = 0.8582f0, acc = 74.6)   Test: (loss = 1.6136f0, acc = 58.0952)\n",
      "Epoch: 211   Train: (loss = 0.8534f0, acc = 74.9)   Test: (loss = 1.6092f0, acc = 58.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212   Train: (loss = 0.8487f0, acc = 75.1)   Test: (loss = 1.6041f0, acc = 58.4762)\n",
      "Epoch: 213   Train: (loss = 0.844f0, acc = 75.2)   Test: (loss = 1.6002f0, acc = 58.4762)\n",
      "Epoch: 214   Train: (loss = 0.8393f0, acc = 75.2)   Test: (loss = 1.5964f0, acc = 58.4762)\n",
      "Epoch: 215   Train: (loss = 0.8347f0, acc = 75.3)   Test: (loss = 1.5917f0, acc = 58.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216   Train: (loss = 0.83f0, acc = 75.3)   Test: (loss = 1.5875f0, acc = 58.6667)\n",
      "Epoch: 217   Train: (loss = 0.8255f0, acc = 75.5)   Test: (loss = 1.5831f0, acc = 58.8571)\n",
      "Epoch: 218   Train: (loss = 0.8209f0, acc = 75.6)   Test: (loss = 1.5786f0, acc = 58.8571)\n",
      "Epoch: 219   Train: (loss = 0.8164f0, acc = 75.7)   Test: (loss = 1.5749f0, acc = 58.8571)\n",
      "Epoch: 220   Train: (loss = 0.8119f0, acc = 75.8)   Test: (loss = 1.5714f0, acc = 59.0476)\n",
      "Epoch: 221   Train: (loss = 0.8074f0, acc = 75.9)   Test: (loss = 1.5671f0, acc = 59.2381)\n",
      "Epoch: 222   Train: (loss = 0.803f0, acc = 76.4)   Test: (loss = 1.5626f0, acc = 59.2381)\n",
      "Epoch: 223   Train: (loss = 0.7986f0, acc = 76.8)   Test: (loss = 1.5592f0, acc = 59.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224   Train: (loss = 0.7943f0, acc = 76.9)   Test: (loss = 1.5549f0, acc = 59.619)\n",
      "Epoch: 225   Train: (loss = 0.79f0, acc = 77.0)   Test: (loss = 1.551f0, acc = 60.0)\n",
      "Epoch: 226   Train: (loss = 0.7857f0, acc = 77.2)   Test: (loss = 1.547f0, acc = 60.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227   Train: (loss = 0.7814f0, acc = 77.2)   Test: (loss = 1.5432f0, acc = 60.1905)\n",
      "Epoch: 228   Train: (loss = 0.7772f0, acc = 77.3)   Test: (loss = 1.5395f0, acc = 60.1905)\n",
      "Epoch: 229   Train: (loss = 0.773f0, acc = 77.7)   Test: (loss = 1.5363f0, acc = 60.381)\n",
      "Epoch: 230   Train: (loss = 0.7689f0, acc = 77.9)   Test: (loss = 1.532f0, acc = 60.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 231   Train: (loss = 0.7648f0, acc = 78.1)   Test: (loss = 1.5287f0, acc = 60.9524)\n",
      "Epoch: 232   Train: (loss = 0.7607f0, acc = 78.1)   Test: (loss = 1.5244f0, acc = 60.9524)\n",
      "Epoch: 233   Train: (loss = 0.7567f0, acc = 78.4)   Test: (loss = 1.5201f0, acc = 61.1429)\n",
      "Epoch: 234   Train: (loss = 0.7526f0, acc = 78.5)   Test: (loss = 1.516f0, acc = 61.3333)\n",
      "Epoch: 235   Train: (loss = 0.7486f0, acc = 78.5)   Test: (loss = 1.5118f0, acc = 62.0952)\n",
      "Epoch: 236   Train: (loss = 0.7446f0, acc = 78.8)   Test: (loss = 1.5077f0, acc = 62.0952)\n",
      "Epoch: 237   Train: (loss = 0.7407f0, acc = 78.8)   Test: (loss = 1.5043f0, acc = 62.0952)\n",
      "Epoch: 238   Train: (loss = 0.7367f0, acc = 78.8)   Test: (loss = 1.5004f0, acc = 62.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239   Train: (loss = 0.7329f0, acc = 78.9)   Test: (loss = 1.497f0, acc = 62.2857)\n",
      "Epoch: 240   Train: (loss = 0.729f0, acc = 79.0)   Test: (loss = 1.4928f0, acc = 62.6667)\n",
      "Epoch: 241   Train: (loss = 0.7252f0, acc = 79.3)   Test: (loss = 1.4892f0, acc = 62.8571)\n",
      "Epoch: 242   Train: (loss = 0.7214f0, acc = 79.4)   Test: (loss = 1.4859f0, acc = 62.8571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243   Train: (loss = 0.7176f0, acc = 79.4)   Test: (loss = 1.4824f0, acc = 63.0476)\n",
      "Epoch: 244   Train: (loss = 0.7139f0, acc = 79.4)   Test: (loss = 1.4785f0, acc = 63.4286)\n",
      "Epoch: 245   Train: (loss = 0.7102f0, acc = 79.7)   Test: (loss = 1.475f0, acc = 63.4286)\n",
      "Epoch: 246   Train: (loss = 0.7065f0, acc = 79.8)   Test: (loss = 1.4713f0, acc = 63.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247   Train: (loss = 0.7028f0, acc = 79.9)   Test: (loss = 1.4687f0, acc = 63.8095)\n",
      "Epoch: 248   Train: (loss = 0.6992f0, acc = 79.9)   Test: (loss = 1.4651f0, acc = 63.8095)\n",
      "Epoch: 249   Train: (loss = 0.6956f0, acc = 79.9)   Test: (loss = 1.4623f0, acc = 63.8095)\n",
      "Epoch: 250   Train: (loss = 0.692f0, acc = 80.0)   Test: (loss = 1.4586f0, acc = 63.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 251   Train: (loss = 0.6885f0, acc = 80.1)   Test: (loss = 1.4553f0, acc = 64.0)\n",
      "Epoch: 252   Train: (loss = 0.685f0, acc = 80.2)   Test: (loss = 1.4514f0, acc = 64.0)\n",
      "Epoch: 253   Train: (loss = 0.6815f0, acc = 80.3)   Test: (loss = 1.4486f0, acc = 64.1905)\n",
      "Epoch: 254   Train: (loss = 0.678f0, acc = 80.5)   Test: (loss = 1.4438f0, acc = 64.1905)\n",
      "Epoch: 255   Train: (loss = 0.6746f0, acc = 80.6)   Test: (loss = 1.4402f0, acc = 64.381)\n",
      "Epoch: 256   Train: (loss = 0.6712f0, acc = 80.7)   Test: (loss = 1.4367f0, acc = 64.381)\n",
      "Epoch: 257   Train: (loss = 0.6678f0, acc = 80.7)   Test: (loss = 1.4331f0, acc = 64.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258   Train: (loss = 0.6644f0, acc = 80.7)   Test: (loss = 1.4298f0, acc = 64.381)\n",
      "Epoch: 259   Train: (loss = 0.6611f0, acc = 80.7)   Test: (loss = 1.4259f0, acc = 64.7619)\n",
      "Epoch: 260   Train: (loss = 0.6578f0, acc = 80.7)   Test: (loss = 1.4237f0, acc = 64.9524)\n",
      "Epoch: 261   Train: (loss = 0.6544f0, acc = 80.9)   Test: (loss = 1.4207f0, acc = 65.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262   Train: (loss = 0.6512f0, acc = 80.9)   Test: (loss = 1.4171f0, acc = 65.3333)\n",
      "Epoch: 263   Train: (loss = 0.6479f0, acc = 80.9)   Test: (loss = 1.414f0, acc = 65.3333)\n",
      "Epoch: 264   Train: (loss = 0.6447f0, acc = 80.9)   Test: (loss = 1.4102f0, acc = 65.3333)\n",
      "Epoch: 265   Train: (loss = 0.6415f0, acc = 80.9)   Test: (loss = 1.4073f0, acc = 65.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266   Train: (loss = 0.6383f0, acc = 80.9)   Test: (loss = 1.4044f0, acc = 65.3333)\n",
      "Epoch: 267   Train: (loss = 0.6352f0, acc = 81.1)   Test: (loss = 1.4011f0, acc = 65.3333)\n",
      "Epoch: 268   Train: (loss = 0.6321f0, acc = 81.3)   Test: (loss = 1.3978f0, acc = 65.7143)\n",
      "Epoch: 269   Train: (loss = 0.629f0, acc = 81.8)   Test: (loss = 1.3948f0, acc = 65.7143)\n",
      "Epoch: 270   Train: (loss = 0.6259f0, acc = 81.8)   Test: (loss = 1.3923f0, acc = 65.7143)\n",
      "Epoch: 271   Train: (loss = 0.6229f0, acc = 82.1)   Test: (loss = 1.3898f0, acc = 65.7143)\n",
      "Epoch: 272   Train: (loss = 0.6198f0, acc = 82.1)   Test: (loss = 1.3867f0, acc = 65.7143)\n",
      "Epoch: 273   Train: (loss = 0.6168f0, acc = 82.3)   Test: (loss = 1.3833f0, acc = 65.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274   Train: (loss = 0.6139f0, acc = 82.4)   Test: (loss = 1.3798f0, acc = 65.7143)\n",
      "Epoch: 275   Train: (loss = 0.6109f0, acc = 82.8)   Test: (loss = 1.3773f0, acc = 65.7143)\n",
      "Epoch: 276   Train: (loss = 0.608f0, acc = 82.8)   Test: (loss = 1.3742f0, acc = 65.7143)\n",
      "Epoch: 277   Train: (loss = 0.605f0, acc = 82.9)   Test: (loss = 1.3712f0, acc = 65.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278   Train: (loss = 0.6021f0, acc = 82.9)   Test: (loss = 1.3677f0, acc = 65.7143)\n",
      "Epoch: 279   Train: (loss = 0.5993f0, acc = 83.0)   Test: (loss = 1.3637f0, acc = 65.7143)\n",
      "Epoch: 280   Train: (loss = 0.5964f0, acc = 83.0)   Test: (loss = 1.3608f0, acc = 65.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 281   Train: (loss = 0.5936f0, acc = 83.1)   Test: (loss = 1.3577f0, acc = 65.7143)\n",
      "Epoch: 282   Train: (loss = 0.5908f0, acc = 83.4)   Test: (loss = 1.3549f0, acc = 65.9048)\n",
      "Epoch: 283   Train: (loss = 0.588f0, acc = 83.5)   Test: (loss = 1.3531f0, acc = 65.9048)\n",
      "Epoch: 284   Train: (loss = 0.5852f0, acc = 83.5)   Test: (loss = 1.3494f0, acc = 65.9048)\n",
      "Epoch: 285   Train: (loss = 0.5825f0, acc = 83.5)   Test: (loss = 1.3469f0, acc = 65.9048)\n",
      "Epoch: 286   Train: (loss = 0.5798f0, acc = 83.5)   Test: (loss = 1.3437f0, acc = 66.2857)\n",
      "Epoch: 287   Train: (loss = 0.577f0, acc = 83.5)   Test: (loss = 1.3405f0, acc = 66.2857)\n",
      "Epoch: 288   Train: (loss = 0.5744f0, acc = 83.6)   Test: (loss = 1.3379f0, acc = 66.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289   Train: (loss = 0.5717f0, acc = 83.6)   Test: (loss = 1.3355f0, acc = 66.2857)\n",
      "Epoch: 290   Train: (loss = 0.569f0, acc = 83.6)   Test: (loss = 1.3321f0, acc = 66.2857)\n",
      "Epoch: 291   Train: (loss = 0.5663f0, acc = 83.8)   Test: (loss = 1.3294f0, acc = 66.2857)\n",
      "Epoch: 292   Train: (loss = 0.5637f0, acc = 83.8)   Test: (loss = 1.327f0, acc = 66.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 293   Train: (loss = 0.5611f0, acc = 83.8)   Test: (loss = 1.3235f0, acc = 66.2857)\n",
      "Epoch: 294   Train: (loss = 0.5585f0, acc = 83.8)   Test: (loss = 1.3208f0, acc = 66.4762)\n",
      "Epoch: 295   Train: (loss = 0.5558f0, acc = 83.8)   Test: (loss = 1.3188f0, acc = 66.6667)\n",
      "Epoch: 296   Train: (loss = 0.5533f0, acc = 83.9)   Test: (loss = 1.3159f0, acc = 66.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297   Train: (loss = 0.5507f0, acc = 83.9)   Test: (loss = 1.3125f0, acc = 66.6667)\n",
      "Epoch: 298   Train: (loss = 0.5482f0, acc = 83.9)   Test: (loss = 1.3096f0, acc = 66.6667)\n",
      "Epoch: 299   Train: (loss = 0.5456f0, acc = 83.9)   Test: (loss = 1.307f0, acc = 66.8571)\n",
      "Epoch: 300   Train: (loss = 0.5431f0, acc = 84.2)   Test: (loss = 1.3044f0, acc = 66.8571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301   Train: (loss = 0.5406f0, acc = 84.4)   Test: (loss = 1.3021f0, acc = 66.8571)\n",
      "Epoch: 302   Train: (loss = 0.5381f0, acc = 84.4)   Test: (loss = 1.2994f0, acc = 66.8571)\n",
      "Epoch: 303   Train: (loss = 0.5357f0, acc = 84.4)   Test: (loss = 1.2964f0, acc = 66.8571)\n",
      "Epoch: 304   Train: (loss = 0.5333f0, acc = 84.5)   Test: (loss = 1.2936f0, acc = 66.8571)\n",
      "Epoch: 305   Train: (loss = 0.5308f0, acc = 84.7)   Test: (loss = 1.2909f0, acc = 66.8571)\n",
      "Epoch: 306   Train: (loss = 0.5284f0, acc = 84.7)   Test: (loss = 1.2887f0, acc = 66.8571)\n",
      "Epoch: 307   Train: (loss = 0.526f0, acc = 84.7)   Test: (loss = 1.285f0, acc = 66.8571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 308   Train: (loss = 0.5237f0, acc = 84.9)   Test: (loss = 1.2819f0, acc = 67.0476)\n",
      "Epoch: 309   Train: (loss = 0.5213f0, acc = 84.9)   Test: (loss = 1.2805f0, acc = 67.0476)\n",
      "Epoch: 310   Train: (loss = 0.519f0, acc = 85.0)   Test: (loss = 1.2774f0, acc = 67.2381)\n",
      "Epoch: 311   Train: (loss = 0.5167f0, acc = 85.0)   Test: (loss = 1.2748f0, acc = 67.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 312   Train: (loss = 0.5144f0, acc = 85.1)   Test: (loss = 1.2726f0, acc = 67.2381)\n",
      "Epoch: 313   Train: (loss = 0.5121f0, acc = 85.2)   Test: (loss = 1.2705f0, acc = 67.2381)\n",
      "Epoch: 314   Train: (loss = 0.5098f0, acc = 85.2)   Test: (loss = 1.2676f0, acc = 67.2381)\n",
      "Epoch: 315   Train: (loss = 0.5076f0, acc = 85.2)   Test: (loss = 1.2655f0, acc = 67.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 316   Train: (loss = 0.5053f0, acc = 85.3)   Test: (loss = 1.2629f0, acc = 67.2381)\n",
      "Epoch: 317   Train: (loss = 0.5031f0, acc = 85.3)   Test: (loss = 1.2602f0, acc = 67.2381)\n",
      "Epoch: 318   Train: (loss = 0.5008f0, acc = 85.5)   Test: (loss = 1.2576f0, acc = 67.2381)\n",
      "Epoch: 319   Train: (loss = 0.4986f0, acc = 85.6)   Test: (loss = 1.2548f0, acc = 67.2381)\n",
      "Epoch: 320   Train: (loss = 0.4964f0, acc = 85.8)   Test: (loss = 1.252f0, acc = 67.2381)\n",
      "Epoch: 321   Train: (loss = 0.4943f0, acc = 85.9)   Test: (loss = 1.2494f0, acc = 67.2381)\n",
      "Epoch: 322   Train: (loss = 0.4922f0, acc = 86.0)   Test: (loss = 1.247f0, acc = 67.2381)\n",
      "Epoch: 323   Train: (loss = 0.49f0, acc = 86.0)   Test: (loss = 1.2445f0, acc = 67.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 324   Train: (loss = 0.4879f0, acc = 86.0)   Test: (loss = 1.2429f0, acc = 67.2381)\n",
      "Epoch: 325   Train: (loss = 0.4857f0, acc = 86.1)   Test: (loss = 1.2409f0, acc = 67.4286)\n",
      "Epoch: 326   Train: (loss = 0.4836f0, acc = 86.2)   Test: (loss = 1.2377f0, acc = 67.619)\n",
      "Epoch: 327   Train: (loss = 0.4815f0, acc = 86.1)   Test: (loss = 1.2359f0, acc = 67.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 328   Train: (loss = 0.4794f0, acc = 86.1)   Test: (loss = 1.234f0, acc = 68.0)\n",
      "Epoch: 329   Train: (loss = 0.4774f0, acc = 86.1)   Test: (loss = 1.2312f0, acc = 68.0)\n",
      "Epoch: 330   Train: (loss = 0.4753f0, acc = 86.2)   Test: (loss = 1.2284f0, acc = 68.0)\n",
      "Epoch: 331   Train: (loss = 0.4733f0, acc = 86.2)   Test: (loss = 1.2257f0, acc = 68.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 332   Train: (loss = 0.4712f0, acc = 86.5)   Test: (loss = 1.2235f0, acc = 68.1905)\n",
      "Epoch: 333   Train: (loss = 0.4692f0, acc = 86.5)   Test: (loss = 1.2219f0, acc = 68.1905)\n",
      "Epoch: 334   Train: (loss = 0.4672f0, acc = 86.6)   Test: (loss = 1.2198f0, acc = 68.5714)\n",
      "Epoch: 335   Train: (loss = 0.4652f0, acc = 86.7)   Test: (loss = 1.2178f0, acc = 68.5714)\n",
      "Epoch: 336   Train: (loss = 0.4632f0, acc = 86.7)   Test: (loss = 1.215f0, acc = 68.5714)\n",
      "Epoch: 337   Train: (loss = 0.4612f0, acc = 86.7)   Test: (loss = 1.2132f0, acc = 68.5714)\n",
      "Epoch: 338   Train: (loss = 0.4593f0, acc = 86.7)   Test: (loss = 1.2111f0, acc = 68.5714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 339   Train: (loss = 0.4573f0, acc = 86.8)   Test: (loss = 1.2091f0, acc = 68.5714)\n",
      "Epoch: 340   Train: (loss = 0.4554f0, acc = 86.8)   Test: (loss = 1.2061f0, acc = 68.7619)\n",
      "Epoch: 341   Train: (loss = 0.4534f0, acc = 86.9)   Test: (loss = 1.2046f0, acc = 68.9524)\n",
      "Epoch: 342   Train: (loss = 0.4515f0, acc = 86.9)   Test: (loss = 1.2031f0, acc = 68.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 343   Train: (loss = 0.4496f0, acc = 87.0)   Test: (loss = 1.2001f0, acc = 68.9524)\n",
      "Epoch: 344   Train: (loss = 0.4477f0, acc = 87.0)   Test: (loss = 1.1972f0, acc = 68.9524)\n",
      "Epoch: 345   Train: (loss = 0.4458f0, acc = 87.1)   Test: (loss = 1.1953f0, acc = 68.9524)\n",
      "Epoch: 346   Train: (loss = 0.4439f0, acc = 87.1)   Test: (loss = 1.1932f0, acc = 68.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 347   Train: (loss = 0.4421f0, acc = 87.3)   Test: (loss = 1.1912f0, acc = 68.9524)\n",
      "Epoch: 348   Train: (loss = 0.4402f0, acc = 87.5)   Test: (loss = 1.1894f0, acc = 68.7619)\n",
      "Epoch: 349   Train: (loss = 0.4384f0, acc = 87.5)   Test: (loss = 1.1869f0, acc = 68.9524)\n",
      "Epoch: 350   Train: (loss = 0.4365f0, acc = 87.5)   Test: (loss = 1.1849f0, acc = 68.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 351   Train: (loss = 0.4347f0, acc = 87.5)   Test: (loss = 1.1826f0, acc = 68.9524)\n",
      "Epoch: 352   Train: (loss = 0.4329f0, acc = 87.5)   Test: (loss = 1.1799f0, acc = 68.9524)\n",
      "Epoch: 353   Train: (loss = 0.4311f0, acc = 87.5)   Test: (loss = 1.1777f0, acc = 68.9524)\n",
      "Epoch: 354   Train: (loss = 0.4293f0, acc = 87.6)   Test: (loss = 1.1755f0, acc = 68.9524)\n",
      "Epoch: 355   Train: (loss = 0.4275f0, acc = 87.8)   Test: (loss = 1.1737f0, acc = 69.1429)\n",
      "Epoch: 356   Train: (loss = 0.4257f0, acc = 88.0)   Test: (loss = 1.1721f0, acc = 69.1429)\n",
      "Epoch: 357   Train: (loss = 0.424f0, acc = 88.0)   Test: (loss = 1.1705f0, acc = 68.9524)\n",
      "Epoch: 358   Train: (loss = 0.4222f0, acc = 88.0)   Test: (loss = 1.1681f0, acc = 69.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359   Train: (loss = 0.4205f0, acc = 88.1)   Test: (loss = 1.1655f0, acc = 69.1429)\n",
      "Epoch: 360   Train: (loss = 0.4187f0, acc = 88.1)   Test: (loss = 1.1646f0, acc = 69.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 361   Train: (loss = 0.417f0, acc = 88.1)   Test: (loss = 1.1622f0, acc = 69.1429)\n",
      "Epoch: 362   Train: (loss = 0.4153f0, acc = 88.1)   Test: (loss = 1.1602f0, acc = 69.1429)\n",
      "Epoch: 363   Train: (loss = 0.4136f0, acc = 88.3)   Test: (loss = 1.158f0, acc = 69.3333)\n",
      "Epoch: 364   Train: (loss = 0.4119f0, acc = 88.5)   Test: (loss = 1.1559f0, acc = 69.1429)\n",
      "Epoch: 365   Train: (loss = 0.4102f0, acc = 88.5)   Test: (loss = 1.1538f0, acc = 69.1429)\n",
      "Epoch: 366   Train: (loss = 0.4085f0, acc = 88.5)   Test: (loss = 1.1518f0, acc = 69.1429)\n",
      "Epoch: 367   Train: (loss = 0.4068f0, acc = 88.6)   Test: (loss = 1.1489f0, acc = 69.1429)\n",
      "Epoch: 368   Train: (loss = 0.4052f0, acc = 88.6)   Test: (loss = 1.1469f0, acc = 69.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369   Train: (loss = 0.4035f0, acc = 88.7)   Test: (loss = 1.1449f0, acc = 69.3333)\n",
      "Epoch: 370   Train: (loss = 0.4019f0, acc = 88.7)   Test: (loss = 1.1419f0, acc = 69.3333)\n",
      "Epoch: 371   Train: (loss = 0.4002f0, acc = 88.8)   Test: (loss = 1.1408f0, acc = 69.3333)\n",
      "Epoch: 372   Train: (loss = 0.3986f0, acc = 88.8)   Test: (loss = 1.1393f0, acc = 69.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 373   Train: (loss = 0.397f0, acc = 88.9)   Test: (loss = 1.1374f0, acc = 69.3333)\n",
      "Epoch: 374   Train: (loss = 0.3954f0, acc = 89.0)   Test: (loss = 1.1348f0, acc = 69.7143)\n",
      "Epoch: 375   Train: (loss = 0.3938f0, acc = 89.0)   Test: (loss = 1.1331f0, acc = 69.7143)\n",
      "Epoch: 376   Train: (loss = 0.3922f0, acc = 89.0)   Test: (loss = 1.1314f0, acc = 69.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 377   Train: (loss = 0.3906f0, acc = 89.0)   Test: (loss = 1.1288f0, acc = 69.7143)\n",
      "Epoch: 378   Train: (loss = 0.389f0, acc = 89.2)   Test: (loss = 1.1269f0, acc = 69.7143)\n",
      "Epoch: 379   Train: (loss = 0.3875f0, acc = 89.2)   Test: (loss = 1.1253f0, acc = 69.7143)\n",
      "Epoch: 380   Train: (loss = 0.3859f0, acc = 89.2)   Test: (loss = 1.1246f0, acc = 69.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381   Train: (loss = 0.3843f0, acc = 89.2)   Test: (loss = 1.1229f0, acc = 69.7143)\n",
      "Epoch: 382   Train: (loss = 0.3828f0, acc = 89.5)   Test: (loss = 1.121f0, acc = 69.7143)\n",
      "Epoch: 383   Train: (loss = 0.3813f0, acc = 89.5)   Test: (loss = 1.1183f0, acc = 69.7143)\n",
      "Epoch: 384   Train: (loss = 0.3797f0, acc = 89.5)   Test: (loss = 1.1163f0, acc = 69.7143)\n",
      "Epoch: 385   Train: (loss = 0.3782f0, acc = 89.6)   Test: (loss = 1.1137f0, acc = 69.7143)\n",
      "Epoch: 386   Train: (loss = 0.3767f0, acc = 89.6)   Test: (loss = 1.1117f0, acc = 69.7143)\n",
      "Epoch: 387   Train: (loss = 0.3752f0, acc = 89.6)   Test: (loss = 1.1108f0, acc = 69.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 388   Train: (loss = 0.3737f0, acc = 89.6)   Test: (loss = 1.1098f0, acc = 69.7143)\n",
      "Epoch: 389   Train: (loss = 0.3722f0, acc = 89.6)   Test: (loss = 1.1078f0, acc = 69.7143)\n",
      "Epoch: 390   Train: (loss = 0.3707f0, acc = 89.6)   Test: (loss = 1.1051f0, acc = 69.7143)\n",
      "Epoch: 391   Train: (loss = 0.3692f0, acc = 89.6)   Test: (loss = 1.1036f0, acc = 69.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 392   Train: (loss = 0.3677f0, acc = 89.7)   Test: (loss = 1.1016f0, acc = 69.7143)\n",
      "Epoch: 393   Train: (loss = 0.3663f0, acc = 89.8)   Test: (loss = 1.0999f0, acc = 69.7143)\n",
      "Epoch: 394   Train: (loss = 0.3648f0, acc = 89.8)   Test: (loss = 1.0981f0, acc = 69.9048)\n",
      "Epoch: 395   Train: (loss = 0.3634f0, acc = 89.9)   Test: (loss = 1.0958f0, acc = 70.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 396   Train: (loss = 0.3619f0, acc = 89.9)   Test: (loss = 1.0948f0, acc = 69.9048)\n",
      "Epoch: 397   Train: (loss = 0.3605f0, acc = 90.2)   Test: (loss = 1.0938f0, acc = 70.0952)\n",
      "Epoch: 398   Train: (loss = 0.3591f0, acc = 90.2)   Test: (loss = 1.0907f0, acc = 70.0952)\n",
      "Epoch: 399   Train: (loss = 0.3576f0, acc = 90.2)   Test: (loss = 1.0899f0, acc = 70.0952)\n",
      "Epoch: 400   Train: (loss = 0.3562f0, acc = 90.2)   Test: (loss = 1.0883f0, acc = 70.0952)\n",
      "Epoch: 401   Train: (loss = 0.3548f0, acc = 90.2)   Test: (loss = 1.0862f0, acc = 70.0952)\n",
      "Epoch: 402   Train: (loss = 0.3534f0, acc = 90.4)   Test: (loss = 1.0846f0, acc = 70.0952)\n",
      "Epoch: 403   Train: (loss = 0.352f0, acc = 90.3)   Test: (loss = 1.0822f0, acc = 70.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404   Train: (loss = 0.3506f0, acc = 90.4)   Test: (loss = 1.081f0, acc = 70.0952)\n",
      "Epoch: 405   Train: (loss = 0.3492f0, acc = 90.4)   Test: (loss = 1.0792f0, acc = 70.0952)\n",
      "Epoch: 406   Train: (loss = 0.3479f0, acc = 90.4)   Test: (loss = 1.0776f0, acc = 70.0952)\n",
      "Epoch: 407   Train: (loss = 0.3465f0, acc = 90.4)   Test: (loss = 1.0756f0, acc = 70.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 408   Train: (loss = 0.3451f0, acc = 90.4)   Test: (loss = 1.0741f0, acc = 70.0952)\n",
      "Epoch: 409   Train: (loss = 0.3438f0, acc = 90.4)   Test: (loss = 1.0716f0, acc = 70.0952)\n",
      "Epoch: 410   Train: (loss = 0.3424f0, acc = 90.5)   Test: (loss = 1.0696f0, acc = 70.0952)\n",
      "Epoch: 411   Train: (loss = 0.3411f0, acc = 90.6)   Test: (loss = 1.0684f0, acc = 70.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 412   Train: (loss = 0.3397f0, acc = 90.6)   Test: (loss = 1.0672f0, acc = 70.0952)\n",
      "Epoch: 413   Train: (loss = 0.3384f0, acc = 90.6)   Test: (loss = 1.0649f0, acc = 70.0952)\n",
      "Epoch: 414   Train: (loss = 0.3371f0, acc = 90.7)   Test: (loss = 1.064f0, acc = 70.0952)\n",
      "Epoch: 415   Train: (loss = 0.3358f0, acc = 90.7)   Test: (loss = 1.0621f0, acc = 70.0952)\n",
      "Epoch: 416   Train: (loss = 0.3345f0, acc = 90.7)   Test: (loss = 1.0605f0, acc = 70.0952)\n",
      "Epoch: 417   Train: (loss = 0.3331f0, acc = 90.7)   Test: (loss = 1.0591f0, acc = 70.0952)\n",
      "Epoch: 418   Train: (loss = 0.3318f0, acc = 90.8)   Test: (loss = 1.0569f0, acc = 70.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 419   Train: (loss = 0.3305f0, acc = 90.9)   Test: (loss = 1.0544f0, acc = 70.2857)\n",
      "Epoch: 420   Train: (loss = 0.3292f0, acc = 91.1)   Test: (loss = 1.0533f0, acc = 70.2857)\n",
      "Epoch: 421   Train: (loss = 0.3279f0, acc = 91.0)   Test: (loss = 1.0515f0, acc = 70.2857)\n",
      "Epoch: 422   Train: (loss = 0.3266f0, acc = 91.1)   Test: (loss = 1.049f0, acc = 70.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 423   Train: (loss = 0.3254f0, acc = 91.1)   Test: (loss = 1.0482f0, acc = 70.2857)\n",
      "Epoch: 424   Train: (loss = 0.3241f0, acc = 91.1)   Test: (loss = 1.0462f0, acc = 70.2857)\n",
      "Epoch: 425   Train: (loss = 0.3228f0, acc = 91.2)   Test: (loss = 1.0455f0, acc = 70.2857)\n",
      "Epoch: 426   Train: (loss = 0.3216f0, acc = 91.3)   Test: (loss = 1.0443f0, acc = 70.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 427   Train: (loss = 0.3203f0, acc = 91.3)   Test: (loss = 1.0422f0, acc = 70.2857)\n",
      "Epoch: 428   Train: (loss = 0.3191f0, acc = 91.3)   Test: (loss = 1.0397f0, acc = 70.2857)\n",
      "Epoch: 429   Train: (loss = 0.3178f0, acc = 91.4)   Test: (loss = 1.0377f0, acc = 70.2857)\n",
      "Epoch: 430   Train: (loss = 0.3166f0, acc = 91.4)   Test: (loss = 1.0362f0, acc = 70.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 431   Train: (loss = 0.3154f0, acc = 91.5)   Test: (loss = 1.0341f0, acc = 70.2857)\n",
      "Epoch: 432   Train: (loss = 0.3142f0, acc = 91.7)   Test: (loss = 1.0329f0, acc = 70.2857)\n",
      "Epoch: 433   Train: (loss = 0.3129f0, acc = 91.7)   Test: (loss = 1.0318f0, acc = 70.2857)\n",
      "Epoch: 434   Train: (loss = 0.3117f0, acc = 91.8)   Test: (loss = 1.0299f0, acc = 70.2857)\n",
      "Epoch: 435   Train: (loss = 0.3105f0, acc = 91.8)   Test: (loss = 1.0277f0, acc = 70.2857)\n",
      "Epoch: 436   Train: (loss = 0.3093f0, acc = 91.8)   Test: (loss = 1.0267f0, acc = 70.2857)\n",
      "Epoch: 437   Train: (loss = 0.3081f0, acc = 91.8)   Test: (loss = 1.0255f0, acc = 70.2857)\n",
      "Epoch: 438   Train: (loss = 0.3069f0, acc = 91.8)   Test: (loss = 1.0237f0, acc = 70.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 439   Train: (loss = 0.3057f0, acc = 91.8)   Test: (loss = 1.0226f0, acc = 70.2857)\n",
      "Epoch: 440   Train: (loss = 0.3046f0, acc = 91.8)   Test: (loss = 1.0203f0, acc = 70.2857)\n",
      "Epoch: 441   Train: (loss = 0.3034f0, acc = 91.8)   Test: (loss = 1.0195f0, acc = 70.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 442   Train: (loss = 0.3022f0, acc = 91.8)   Test: (loss = 1.0177f0, acc = 70.2857)\n",
      "Epoch: 443   Train: (loss = 0.301f0, acc = 91.8)   Test: (loss = 1.0167f0, acc = 70.2857)\n",
      "Epoch: 444   Train: (loss = 0.2999f0, acc = 91.8)   Test: (loss = 1.0147f0, acc = 70.4762)\n",
      "Epoch: 445   Train: (loss = 0.2987f0, acc = 91.7)   Test: (loss = 1.0135f0, acc = 70.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446   Train: (loss = 0.2976f0, acc = 91.9)   Test: (loss = 1.0117f0, acc = 70.4762)\n",
      "Epoch: 447   Train: (loss = 0.2965f0, acc = 91.9)   Test: (loss = 1.0107f0, acc = 70.4762)\n",
      "Epoch: 448   Train: (loss = 0.2953f0, acc = 92.0)   Test: (loss = 1.0086f0, acc = 70.4762)\n",
      "Epoch: 449   Train: (loss = 0.2942f0, acc = 92.0)   Test: (loss = 1.0064f0, acc = 70.8571)\n",
      "Epoch: 450   Train: (loss = 0.293f0, acc = 92.0)   Test: (loss = 1.0057f0, acc = 70.8571)\n",
      "Epoch: 451   Train: (loss = 0.2919f0, acc = 91.9)   Test: (loss = 1.0036f0, acc = 70.8571)\n",
      "Epoch: 452   Train: (loss = 0.2908f0, acc = 92.0)   Test: (loss = 1.0026f0, acc = 70.8571)\n",
      "Epoch: 453   Train: (loss = 0.2896f0, acc = 92.0)   Test: (loss = 1.0012f0, acc = 71.0476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 454   Train: (loss = 0.2885f0, acc = 92.1)   Test: (loss = 0.9989f0, acc = 71.0476)\n",
      "Epoch: 455   Train: (loss = 0.2874f0, acc = 92.0)   Test: (loss = 0.9968f0, acc = 71.4286)\n",
      "Epoch: 456   Train: (loss = 0.2863f0, acc = 92.1)   Test: (loss = 0.9954f0, acc = 71.4286)\n",
      "Epoch: 457   Train: (loss = 0.2852f0, acc = 92.3)   Test: (loss = 0.9936f0, acc = 71.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 458   Train: (loss = 0.2842f0, acc = 92.3)   Test: (loss = 0.9921f0, acc = 71.8095)\n",
      "Epoch: 459   Train: (loss = 0.2831f0, acc = 92.3)   Test: (loss = 0.9905f0, acc = 71.8095)\n",
      "Epoch: 460   Train: (loss = 0.282f0, acc = 92.4)   Test: (loss = 0.9894f0, acc = 71.8095)\n",
      "Epoch: 461   Train: (loss = 0.2809f0, acc = 92.5)   Test: (loss = 0.9874f0, acc = 72.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 462   Train: (loss = 0.2798f0, acc = 92.5)   Test: (loss = 0.986f0, acc = 72.0)\n",
      "Epoch: 463   Train: (loss = 0.2788f0, acc = 92.5)   Test: (loss = 0.9845f0, acc = 71.8095)\n",
      "Epoch: 464   Train: (loss = 0.2777f0, acc = 92.5)   Test: (loss = 0.9832f0, acc = 71.8095)\n",
      "Epoch: 465   Train: (loss = 0.2767f0, acc = 92.4)   Test: (loss = 0.9818f0, acc = 71.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 466   Train: (loss = 0.2757f0, acc = 92.5)   Test: (loss = 0.9804f0, acc = 71.8095)\n",
      "Epoch: 467   Train: (loss = 0.2746f0, acc = 92.6)   Test: (loss = 0.9782f0, acc = 71.8095)\n",
      "Epoch: 468   Train: (loss = 0.2736f0, acc = 92.8)   Test: (loss = 0.9767f0, acc = 71.8095)\n",
      "Epoch: 469   Train: (loss = 0.2725f0, acc = 92.9)   Test: (loss = 0.9748f0, acc = 72.0)\n",
      "Epoch: 470   Train: (loss = 0.2715f0, acc = 92.8)   Test: (loss = 0.9735f0, acc = 72.0)\n",
      "Epoch: 471   Train: (loss = 0.2705f0, acc = 92.9)   Test: (loss = 0.9723f0, acc = 72.0)\n",
      "Epoch: 472   Train: (loss = 0.2695f0, acc = 92.9)   Test: (loss = 0.9708f0, acc = 72.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 473   Train: (loss = 0.2685f0, acc = 92.9)   Test: (loss = 0.9701f0, acc = 72.0)\n",
      "Epoch: 474   Train: (loss = 0.2674f0, acc = 93.0)   Test: (loss = 0.9687f0, acc = 72.0)\n",
      "Epoch: 475   Train: (loss = 0.2664f0, acc = 92.9)   Test: (loss = 0.9667f0, acc = 72.0)\n",
      "Epoch: 476   Train: (loss = 0.2654f0, acc = 92.9)   Test: (loss = 0.9653f0, acc = 72.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477   Train: (loss = 0.2644f0, acc = 92.9)   Test: (loss = 0.9647f0, acc = 72.0)\n",
      "Epoch: 478   Train: (loss = 0.2634f0, acc = 93.0)   Test: (loss = 0.9627f0, acc = 72.0)\n",
      "Epoch: 479   Train: (loss = 0.2624f0, acc = 92.9)   Test: (loss = 0.9612f0, acc = 72.0)\n",
      "Epoch: 480   Train: (loss = 0.2614f0, acc = 93.0)   Test: (loss = 0.9596f0, acc = 72.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 481   Train: (loss = 0.2604f0, acc = 93.0)   Test: (loss = 0.9586f0, acc = 72.0)\n",
      "Epoch: 482   Train: (loss = 0.2595f0, acc = 93.0)   Test: (loss = 0.9572f0, acc = 72.0)\n",
      "Epoch: 483   Train: (loss = 0.2585f0, acc = 93.0)   Test: (loss = 0.9554f0, acc = 72.1905)\n",
      "Epoch: 484   Train: (loss = 0.2575f0, acc = 93.3)   Test: (loss = 0.9537f0, acc = 72.1905)\n",
      "Epoch: 485   Train: (loss = 0.2565f0, acc = 93.3)   Test: (loss = 0.9521f0, acc = 72.1905)\n",
      "Epoch: 486   Train: (loss = 0.2556f0, acc = 93.3)   Test: (loss = 0.9512f0, acc = 72.1905)\n",
      "Epoch: 487   Train: (loss = 0.2546f0, acc = 93.5)   Test: (loss = 0.9491f0, acc = 72.1905)\n",
      "Epoch: 488   Train: (loss = 0.2536f0, acc = 93.4)   Test: (loss = 0.9481f0, acc = 72.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489   Train: (loss = 0.2527f0, acc = 93.5)   Test: (loss = 0.9475f0, acc = 72.1905)\n",
      "Epoch: 490   Train: (loss = 0.2518f0, acc = 93.5)   Test: (loss = 0.9455f0, acc = 72.1905)\n",
      "Epoch: 491   Train: (loss = 0.2508f0, acc = 93.5)   Test: (loss = 0.9449f0, acc = 72.1905)\n",
      "Epoch: 492   Train: (loss = 0.2499f0, acc = 93.5)   Test: (loss = 0.9434f0, acc = 72.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 493   Train: (loss = 0.2489f0, acc = 93.5)   Test: (loss = 0.9424f0, acc = 72.1905)\n",
      "Epoch: 494   Train: (loss = 0.248f0, acc = 93.6)   Test: (loss = 0.9402f0, acc = 72.1905)\n",
      "Epoch: 495   Train: (loss = 0.2471f0, acc = 93.7)   Test: (loss = 0.9397f0, acc = 72.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 496   Train: (loss = 0.2462f0, acc = 93.7)   Test: (loss = 0.9371f0, acc = 72.1905)\n",
      "Epoch: 497   Train: (loss = 0.2453f0, acc = 93.7)   Test: (loss = 0.9357f0, acc = 72.1905)\n",
      "Epoch: 498   Train: (loss = 0.2443f0, acc = 93.7)   Test: (loss = 0.9343f0, acc = 72.381)\n",
      "Epoch: 499   Train: (loss = 0.2434f0, acc = 93.7)   Test: (loss = 0.9335f0, acc = 72.381)\n",
      "Epoch: 500   Train: (loss = 0.2425f0, acc = 93.7)   Test: (loss = 0.9316f0, acc = 72.5714)\n",
      "Epoch: 501   Train: (loss = 0.2416f0, acc = 93.7)   Test: (loss = 0.9307f0, acc = 72.5714)\n",
      "Epoch: 502   Train: (loss = 0.2407f0, acc = 93.8)   Test: (loss = 0.9301f0, acc = 72.5714)\n",
      "Epoch: 503   Train: (loss = 0.2398f0, acc = 93.8)   Test: (loss = 0.9288f0, acc = 72.5714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 504   Train: (loss = 0.2389f0, acc = 93.8)   Test: (loss = 0.9279f0, acc = 72.5714)\n",
      "Epoch: 505   Train: (loss = 0.238f0, acc = 93.9)   Test: (loss = 0.9255f0, acc = 72.5714)\n",
      "Epoch: 506   Train: (loss = 0.2371f0, acc = 93.9)   Test: (loss = 0.9244f0, acc = 72.5714)\n",
      "Epoch: 507   Train: (loss = 0.2362f0, acc = 93.9)   Test: (loss = 0.9226f0, acc = 72.5714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 508   Train: (loss = 0.2353f0, acc = 93.9)   Test: (loss = 0.9211f0, acc = 72.5714)\n",
      "Epoch: 509   Train: (loss = 0.2345f0, acc = 93.9)   Test: (loss = 0.9197f0, acc = 72.7619)\n",
      "Epoch: 510   Train: (loss = 0.2336f0, acc = 93.9)   Test: (loss = 0.9189f0, acc = 72.7619)\n",
      "Epoch: 511   Train: (loss = 0.2327f0, acc = 93.9)   Test: (loss = 0.9176f0, acc = 72.7619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 512   Train: (loss = 0.2319f0, acc = 93.9)   Test: (loss = 0.9165f0, acc = 72.7619)\n",
      "Epoch: 513   Train: (loss = 0.231f0, acc = 94.0)   Test: (loss = 0.9151f0, acc = 72.7619)\n",
      "Epoch: 514   Train: (loss = 0.2301f0, acc = 94.0)   Test: (loss = 0.9135f0, acc = 72.9524)\n",
      "Epoch: 515   Train: (loss = 0.2293f0, acc = 94.2)   Test: (loss = 0.9122f0, acc = 72.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 516   Train: (loss = 0.2284f0, acc = 94.3)   Test: (loss = 0.9108f0, acc = 72.9524)\n",
      "Epoch: 517   Train: (loss = 0.2276f0, acc = 94.3)   Test: (loss = 0.9101f0, acc = 72.9524)\n",
      "Epoch: 518   Train: (loss = 0.2268f0, acc = 94.3)   Test: (loss = 0.9097f0, acc = 72.9524)\n",
      "Epoch: 519   Train: (loss = 0.2259f0, acc = 94.3)   Test: (loss = 0.9081f0, acc = 72.9524)\n",
      "Epoch: 520   Train: (loss = 0.2251f0, acc = 94.3)   Test: (loss = 0.9066f0, acc = 72.9524)\n",
      "Epoch: 521   Train: (loss = 0.2242f0, acc = 94.3)   Test: (loss = 0.9059f0, acc = 72.9524)\n",
      "Epoch: 522   Train: (loss = 0.2234f0, acc = 94.3)   Test: (loss = 0.9047f0, acc = 72.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 523   Train: (loss = 0.2226f0, acc = 94.3)   Test: (loss = 0.9039f0, acc = 72.9524)\n",
      "Epoch: 524   Train: (loss = 0.2218f0, acc = 94.4)   Test: (loss = 0.9017f0, acc = 72.9524)\n",
      "Epoch: 525   Train: (loss = 0.2209f0, acc = 94.3)   Test: (loss = 0.9f0, acc = 72.9524)\n",
      "Epoch: 526   Train: (loss = 0.2201f0, acc = 94.5)   Test: (loss = 0.8994f0, acc = 72.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 527   Train: (loss = 0.2193f0, acc = 94.6)   Test: (loss = 0.8981f0, acc = 72.9524)\n",
      "Epoch: 528   Train: (loss = 0.2185f0, acc = 94.5)   Test: (loss = 0.8964f0, acc = 72.9524)\n",
      "Epoch: 529   Train: (loss = 0.2177f0, acc = 94.6)   Test: (loss = 0.8948f0, acc = 72.9524)\n",
      "Epoch: 530   Train: (loss = 0.2169f0, acc = 94.6)   Test: (loss = 0.8933f0, acc = 72.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 531   Train: (loss = 0.2161f0, acc = 94.6)   Test: (loss = 0.892f0, acc = 72.9524)\n",
      "Epoch: 532   Train: (loss = 0.2153f0, acc = 94.7)   Test: (loss = 0.8914f0, acc = 72.9524)\n",
      "Epoch: 533   Train: (loss = 0.2145f0, acc = 94.7)   Test: (loss = 0.8897f0, acc = 72.9524)\n",
      "Epoch: 534   Train: (loss = 0.2137f0, acc = 94.7)   Test: (loss = 0.8886f0, acc = 72.9524)\n",
      "Epoch: 535   Train: (loss = 0.2129f0, acc = 94.7)   Test: (loss = 0.8879f0, acc = 72.9524)\n",
      "Epoch: 536   Train: (loss = 0.2121f0, acc = 94.7)   Test: (loss = 0.8876f0, acc = 72.9524)\n",
      "Epoch: 537   Train: (loss = 0.2114f0, acc = 94.8)   Test: (loss = 0.8859f0, acc = 72.9524)\n",
      "Epoch: 538   Train: (loss = 0.2106f0, acc = 94.8)   Test: (loss = 0.8839f0, acc = 72.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 539   Train: (loss = 0.2098f0, acc = 94.9)   Test: (loss = 0.8828f0, acc = 72.9524)\n",
      "Epoch: 540   Train: (loss = 0.209f0, acc = 94.9)   Test: (loss = 0.8812f0, acc = 72.9524)\n",
      "Epoch: 541   Train: (loss = 0.2083f0, acc = 94.9)   Test: (loss = 0.8796f0, acc = 72.9524)\n",
      "Epoch: 542   Train: (loss = 0.2075f0, acc = 94.9)   Test: (loss = 0.8789f0, acc = 72.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 543   Train: (loss = 0.2067f0, acc = 94.9)   Test: (loss = 0.8779f0, acc = 72.9524)\n",
      "Epoch: 544   Train: (loss = 0.206f0, acc = 95.0)   Test: (loss = 0.8772f0, acc = 72.9524)\n",
      "Epoch: 545   Train: (loss = 0.2052f0, acc = 95.0)   Test: (loss = 0.8758f0, acc = 73.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 546   Train: (loss = 0.2045f0, acc = 95.0)   Test: (loss = 0.8743f0, acc = 73.3333)\n",
      "Epoch: 547   Train: (loss = 0.2038f0, acc = 95.0)   Test: (loss = 0.8732f0, acc = 73.3333)\n",
      "Epoch: 548   Train: (loss = 0.203f0, acc = 95.0)   Test: (loss = 0.8709f0, acc = 73.3333)\n",
      "Epoch: 549   Train: (loss = 0.2023f0, acc = 95.0)   Test: (loss = 0.8703f0, acc = 73.3333)\n",
      "Epoch: 550   Train: (loss = 0.2016f0, acc = 95.0)   Test: (loss = 0.8695f0, acc = 73.3333)\n",
      "Epoch: 551   Train: (loss = 0.2008f0, acc = 95.0)   Test: (loss = 0.8686f0, acc = 73.3333)\n",
      "Epoch: 552   Train: (loss = 0.2001f0, acc = 95.0)   Test: (loss = 0.8683f0, acc = 73.3333)\n",
      "Epoch: 553   Train: (loss = 0.1994f0, acc = 95.0)   Test: (loss = 0.8668f0, acc = 73.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 554   Train: (loss = 0.1987f0, acc = 95.0)   Test: (loss = 0.8656f0, acc = 73.3333)\n",
      "Epoch: 555   Train: (loss = 0.198f0, acc = 95.0)   Test: (loss = 0.8637f0, acc = 73.5238)\n",
      "Epoch: 556   Train: (loss = 0.1973f0, acc = 95.1)   Test: (loss = 0.8625f0, acc = 73.5238)\n",
      "Epoch: 557   Train: (loss = 0.1965f0, acc = 95.0)   Test: (loss = 0.8623f0, acc = 73.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 558   Train: (loss = 0.1958f0, acc = 95.0)   Test: (loss = 0.861f0, acc = 73.5238)\n",
      "Epoch: 559   Train: (loss = 0.1951f0, acc = 95.1)   Test: (loss = 0.86f0, acc = 73.5238)\n",
      "Epoch: 560   Train: (loss = 0.1944f0, acc = 95.2)   Test: (loss = 0.8593f0, acc = 73.5238)\n",
      "Epoch: 561   Train: (loss = 0.1938f0, acc = 95.2)   Test: (loss = 0.8572f0, acc = 73.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 562   Train: (loss = 0.1931f0, acc = 95.2)   Test: (loss = 0.856f0, acc = 73.5238)\n",
      "Epoch: 563   Train: (loss = 0.1924f0, acc = 95.2)   Test: (loss = 0.8554f0, acc = 73.5238)\n",
      "Epoch: 564   Train: (loss = 0.1917f0, acc = 95.3)   Test: (loss = 0.8543f0, acc = 73.5238)\n",
      "Epoch: 565   Train: (loss = 0.191f0, acc = 95.3)   Test: (loss = 0.8539f0, acc = 73.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 566   Train: (loss = 0.1903f0, acc = 95.3)   Test: (loss = 0.8523f0, acc = 73.5238)\n",
      "Epoch: 567   Train: (loss = 0.1897f0, acc = 95.3)   Test: (loss = 0.8512f0, acc = 73.7143)\n",
      "Epoch: 568   Train: (loss = 0.189f0, acc = 95.3)   Test: (loss = 0.8513f0, acc = 73.7143)\n",
      "Epoch: 569   Train: (loss = 0.1883f0, acc = 95.3)   Test: (loss = 0.8504f0, acc = 73.7143)\n",
      "Epoch: 570   Train: (loss = 0.1877f0, acc = 95.3)   Test: (loss = 0.8484f0, acc = 73.7143)\n",
      "Epoch: 571   Train: (loss = 0.187f0, acc = 95.3)   Test: (loss = 0.8478f0, acc = 73.7143)\n",
      "Epoch: 572   Train: (loss = 0.1863f0, acc = 95.3)   Test: (loss = 0.8465f0, acc = 73.7143)\n",
      "Epoch: 573   Train: (loss = 0.1857f0, acc = 95.3)   Test: (loss = 0.8453f0, acc = 73.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 574   Train: (loss = 0.185f0, acc = 95.3)   Test: (loss = 0.8448f0, acc = 73.7143)\n",
      "Epoch: 575   Train: (loss = 0.1843f0, acc = 95.3)   Test: (loss = 0.8438f0, acc = 73.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 576   Train: (loss = 0.1837f0, acc = 95.3)   Test: (loss = 0.8434f0, acc = 73.7143)\n",
      "Epoch: 577   Train: (loss = 0.183f0, acc = 95.3)   Test: (loss = 0.8422f0, acc = 73.7143)\n",
      "Epoch: 578   Train: (loss = 0.1824f0, acc = 95.4)   Test: (loss = 0.8411f0, acc = 73.7143)\n",
      "Epoch: 579   Train: (loss = 0.1817f0, acc = 95.4)   Test: (loss = 0.8392f0, acc = 73.7143)\n",
      "Epoch: 580   Train: (loss = 0.1811f0, acc = 95.4)   Test: (loss = 0.8385f0, acc = 73.7143)\n",
      "Epoch: 581   Train: (loss = 0.1804f0, acc = 95.4)   Test: (loss = 0.8376f0, acc = 73.7143)\n",
      "Epoch: 582   Train: (loss = 0.1798f0, acc = 95.4)   Test: (loss = 0.8368f0, acc = 73.9048)\n",
      "Epoch: 583   Train: (loss = 0.1791f0, acc = 95.5)   Test: (loss = 0.8352f0, acc = 73.9048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 584   Train: (loss = 0.1785f0, acc = 95.5)   Test: (loss = 0.8336f0, acc = 74.0952)\n",
      "Epoch: 585   Train: (loss = 0.1779f0, acc = 95.5)   Test: (loss = 0.8334f0, acc = 74.0952)\n",
      "Epoch: 586   Train: (loss = 0.1772f0, acc = 95.7)   Test: (loss = 0.8311f0, acc = 74.2857)\n",
      "Epoch: 587   Train: (loss = 0.1766f0, acc = 95.7)   Test: (loss = 0.8296f0, acc = 74.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 588   Train: (loss = 0.1759f0, acc = 95.7)   Test: (loss = 0.8292f0, acc = 74.4762)\n",
      "Epoch: 589   Train: (loss = 0.1753f0, acc = 95.7)   Test: (loss = 0.8275f0, acc = 74.4762)\n",
      "Epoch: 590   Train: (loss = 0.1747f0, acc = 95.8)   Test: (loss = 0.8277f0, acc = 74.4762)\n",
      "Epoch: 591   Train: (loss = 0.1741f0, acc = 95.8)   Test: (loss = 0.8265f0, acc = 74.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 592   Train: (loss = 0.1735f0, acc = 95.8)   Test: (loss = 0.8252f0, acc = 74.4762)\n",
      "Epoch: 593   Train: (loss = 0.1729f0, acc = 95.8)   Test: (loss = 0.8245f0, acc = 74.4762)\n",
      "Epoch: 594   Train: (loss = 0.1722f0, acc = 95.8)   Test: (loss = 0.8235f0, acc = 74.6667)\n",
      "Epoch: 595   Train: (loss = 0.1717f0, acc = 95.9)   Test: (loss = 0.8229f0, acc = 74.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 596   Train: (loss = 0.171f0, acc = 95.9)   Test: (loss = 0.8215f0, acc = 74.6667)\n",
      "Epoch: 597   Train: (loss = 0.1704f0, acc = 96.0)   Test: (loss = 0.8205f0, acc = 74.6667)\n",
      "Epoch: 598   Train: (loss = 0.1699f0, acc = 96.0)   Test: (loss = 0.8201f0, acc = 74.6667)\n",
      "Epoch: 599   Train: (loss = 0.1693f0, acc = 96.0)   Test: (loss = 0.8189f0, acc = 74.6667)\n",
      "Epoch: 600   Train: (loss = 0.1687f0, acc = 96.3)   Test: (loss = 0.8174f0, acc = 74.6667)\n",
      "Epoch: 601   Train: (loss = 0.1681f0, acc = 96.3)   Test: (loss = 0.8169f0, acc = 74.6667)\n",
      "Epoch: 602   Train: (loss = 0.1675f0, acc = 96.3)   Test: (loss = 0.8161f0, acc = 74.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 603   Train: (loss = 0.1669f0, acc = 96.4)   Test: (loss = 0.8151f0, acc = 74.6667)\n",
      "Epoch: 604   Train: (loss = 0.1663f0, acc = 96.4)   Test: (loss = 0.8143f0, acc = 74.6667)\n",
      "Epoch: 605   Train: (loss = 0.1658f0, acc = 96.4)   Test: (loss = 0.8136f0, acc = 74.6667)\n",
      "Epoch: 606   Train: (loss = 0.1652f0, acc = 96.5)   Test: (loss = 0.8131f0, acc = 74.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 607   Train: (loss = 0.1646f0, acc = 96.5)   Test: (loss = 0.8124f0, acc = 74.6667)\n",
      "Epoch: 608   Train: (loss = 0.164f0, acc = 96.5)   Test: (loss = 0.8102f0, acc = 74.8571)\n",
      "Epoch: 609   Train: (loss = 0.1635f0, acc = 96.5)   Test: (loss = 0.8101f0, acc = 74.8571)\n",
      "Epoch: 610   Train: (loss = 0.1629f0, acc = 96.5)   Test: (loss = 0.8085f0, acc = 74.8571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 611   Train: (loss = 0.1623f0, acc = 96.6)   Test: (loss = 0.8072f0, acc = 75.0476)\n",
      "Epoch: 612   Train: (loss = 0.1618f0, acc = 96.6)   Test: (loss = 0.807f0, acc = 75.0476)\n",
      "Epoch: 613   Train: (loss = 0.1612f0, acc = 96.6)   Test: (loss = 0.8061f0, acc = 75.0476)\n",
      "Epoch: 614   Train: (loss = 0.1607f0, acc = 96.6)   Test: (loss = 0.8048f0, acc = 75.0476)\n",
      "Epoch: 615   Train: (loss = 0.1601f0, acc = 96.7)   Test: (loss = 0.8044f0, acc = 75.0476)\n",
      "Epoch: 616   Train: (loss = 0.1595f0, acc = 96.7)   Test: (loss = 0.8034f0, acc = 75.0476)\n",
      "Epoch: 617   Train: (loss = 0.159f0, acc = 96.7)   Test: (loss = 0.803f0, acc = 75.0476)\n",
      "Epoch: 618   Train: (loss = 0.1584f0, acc = 96.7)   Test: (loss = 0.8015f0, acc = 75.0476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 619   Train: (loss = 0.1579f0, acc = 96.7)   Test: (loss = 0.8007f0, acc = 75.0476)\n",
      "Epoch: 620   Train: (loss = 0.1573f0, acc = 96.7)   Test: (loss = 0.7992f0, acc = 75.0476)\n",
      "Epoch: 621   Train: (loss = 0.1568f0, acc = 96.7)   Test: (loss = 0.7978f0, acc = 75.2381)\n",
      "Epoch: 622   Train: (loss = 0.1562f0, acc = 96.7)   Test: (loss = 0.7978f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 623   Train: (loss = 0.1557f0, acc = 96.7)   Test: (loss = 0.7968f0, acc = 75.2381)\n",
      "Epoch: 624   Train: (loss = 0.1552f0, acc = 96.7)   Test: (loss = 0.7968f0, acc = 75.2381)\n",
      "Epoch: 625   Train: (loss = 0.1546f0, acc = 96.7)   Test: (loss = 0.7958f0, acc = 75.2381)\n",
      "Epoch: 626   Train: (loss = 0.1541f0, acc = 96.7)   Test: (loss = 0.7947f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 627   Train: (loss = 0.1535f0, acc = 96.7)   Test: (loss = 0.794f0, acc = 75.2381)\n",
      "Epoch: 628   Train: (loss = 0.153f0, acc = 96.7)   Test: (loss = 0.7929f0, acc = 75.2381)\n",
      "Epoch: 629   Train: (loss = 0.1525f0, acc = 96.7)   Test: (loss = 0.791f0, acc = 75.2381)\n",
      "Epoch: 630   Train: (loss = 0.1519f0, acc = 96.8)   Test: (loss = 0.7905f0, acc = 75.2381)\n",
      "Epoch: 631   Train: (loss = 0.1514f0, acc = 96.8)   Test: (loss = 0.7896f0, acc = 75.2381)\n",
      "Epoch: 632   Train: (loss = 0.1509f0, acc = 96.8)   Test: (loss = 0.7895f0, acc = 75.2381)\n",
      "Epoch: 633   Train: (loss = 0.1504f0, acc = 96.8)   Test: (loss = 0.7878f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 634   Train: (loss = 0.1498f0, acc = 96.8)   Test: (loss = 0.7867f0, acc = 75.2381)\n",
      "Epoch: 635   Train: (loss = 0.1493f0, acc = 96.9)   Test: (loss = 0.7863f0, acc = 75.2381)\n",
      "Epoch: 636   Train: (loss = 0.1488f0, acc = 96.9)   Test: (loss = 0.7852f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 637   Train: (loss = 0.1483f0, acc = 96.9)   Test: (loss = 0.7842f0, acc = 75.2381)\n",
      "Epoch: 638   Train: (loss = 0.1478f0, acc = 96.9)   Test: (loss = 0.7838f0, acc = 75.2381)\n",
      "Epoch: 639   Train: (loss = 0.1472f0, acc = 96.9)   Test: (loss = 0.7824f0, acc = 75.2381)\n",
      "Epoch: 640   Train: (loss = 0.1467f0, acc = 97.0)   Test: (loss = 0.7819f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 641   Train: (loss = 0.1462f0, acc = 97.0)   Test: (loss = 0.7821f0, acc = 75.2381)\n",
      "Epoch: 642   Train: (loss = 0.1457f0, acc = 97.0)   Test: (loss = 0.7804f0, acc = 75.0476)\n",
      "Epoch: 643   Train: (loss = 0.1452f0, acc = 97.0)   Test: (loss = 0.7783f0, acc = 75.2381)\n",
      "Epoch: 644   Train: (loss = 0.1447f0, acc = 97.0)   Test: (loss = 0.7778f0, acc = 75.2381)\n",
      "Epoch: 645   Train: (loss = 0.1442f0, acc = 97.0)   Test: (loss = 0.7775f0, acc = 75.2381)\n",
      "Epoch: 646   Train: (loss = 0.1437f0, acc = 97.0)   Test: (loss = 0.7771f0, acc = 75.2381)\n",
      "Epoch: 647   Train: (loss = 0.1432f0, acc = 97.0)   Test: (loss = 0.7756f0, acc = 75.2381)\n",
      "Epoch: 648   Train: (loss = 0.1427f0, acc = 97.0)   Test: (loss = 0.7744f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 649   Train: (loss = 0.1423f0, acc = 97.0)   Test: (loss = 0.7739f0, acc = 75.2381)\n",
      "Epoch: 650   Train: (loss = 0.1418f0, acc = 97.0)   Test: (loss = 0.7732f0, acc = 75.2381)\n",
      "Epoch: 651   Train: (loss = 0.1413f0, acc = 97.0)   Test: (loss = 0.7729f0, acc = 75.2381)\n",
      "Epoch: 652   Train: (loss = 0.1408f0, acc = 97.0)   Test: (loss = 0.7712f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 653   Train: (loss = 0.1403f0, acc = 97.0)   Test: (loss = 0.7709f0, acc = 75.2381)\n",
      "Epoch: 654   Train: (loss = 0.1399f0, acc = 97.0)   Test: (loss = 0.7702f0, acc = 75.2381)\n",
      "Epoch: 655   Train: (loss = 0.1394f0, acc = 97.1)   Test: (loss = 0.7689f0, acc = 75.2381)\n",
      "Epoch: 656   Train: (loss = 0.1389f0, acc = 97.1)   Test: (loss = 0.7677f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 657   Train: (loss = 0.1385f0, acc = 97.1)   Test: (loss = 0.7665f0, acc = 75.2381)\n",
      "Epoch: 658   Train: (loss = 0.138f0, acc = 97.1)   Test: (loss = 0.7666f0, acc = 75.2381)\n",
      "Epoch: 659   Train: (loss = 0.1375f0, acc = 97.1)   Test: (loss = 0.766f0, acc = 75.2381)\n",
      "Epoch: 660   Train: (loss = 0.1371f0, acc = 97.1)   Test: (loss = 0.7645f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 661   Train: (loss = 0.1366f0, acc = 97.1)   Test: (loss = 0.7641f0, acc = 75.2381)\n",
      "Epoch: 662   Train: (loss = 0.1361f0, acc = 97.1)   Test: (loss = 0.764f0, acc = 75.2381)\n",
      "Epoch: 663   Train: (loss = 0.1357f0, acc = 97.1)   Test: (loss = 0.7625f0, acc = 75.2381)\n",
      "Epoch: 664   Train: (loss = 0.1352f0, acc = 97.1)   Test: (loss = 0.7613f0, acc = 75.2381)\n",
      "Epoch: 665   Train: (loss = 0.1348f0, acc = 97.1)   Test: (loss = 0.7602f0, acc = 75.2381)\n",
      "Epoch: 666   Train: (loss = 0.1343f0, acc = 97.1)   Test: (loss = 0.7597f0, acc = 75.2381)\n",
      "Epoch: 667   Train: (loss = 0.1339f0, acc = 97.2)   Test: (loss = 0.7592f0, acc = 75.2381)\n",
      "Epoch: 668   Train: (loss = 0.1334f0, acc = 97.2)   Test: (loss = 0.7584f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 669   Train: (loss = 0.133f0, acc = 97.2)   Test: (loss = 0.7575f0, acc = 75.2381)\n",
      "Epoch: 670   Train: (loss = 0.1325f0, acc = 97.2)   Test: (loss = 0.7569f0, acc = 75.2381)\n",
      "Epoch: 671   Train: (loss = 0.1321f0, acc = 97.2)   Test: (loss = 0.7563f0, acc = 75.2381)\n",
      "Epoch: 672   Train: (loss = 0.1316f0, acc = 97.2)   Test: (loss = 0.7555f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 673   Train: (loss = 0.1312f0, acc = 97.3)   Test: (loss = 0.7547f0, acc = 75.2381)\n",
      "Epoch: 674   Train: (loss = 0.1307f0, acc = 97.3)   Test: (loss = 0.7536f0, acc = 75.2381)\n",
      "Epoch: 675   Train: (loss = 0.1303f0, acc = 97.3)   Test: (loss = 0.7533f0, acc = 75.2381)\n",
      "Epoch: 676   Train: (loss = 0.1299f0, acc = 97.3)   Test: (loss = 0.7519f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 677   Train: (loss = 0.1294f0, acc = 97.4)   Test: (loss = 0.7514f0, acc = 75.2381)\n",
      "Epoch: 678   Train: (loss = 0.129f0, acc = 97.4)   Test: (loss = 0.7517f0, acc = 75.2381)\n",
      "Epoch: 679   Train: (loss = 0.1286f0, acc = 97.4)   Test: (loss = 0.7509f0, acc = 75.2381)\n",
      "Epoch: 680   Train: (loss = 0.1281f0, acc = 97.4)   Test: (loss = 0.7492f0, acc = 75.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 681   Train: (loss = 0.1277f0, acc = 97.4)   Test: (loss = 0.7483f0, acc = 75.4286)\n",
      "Epoch: 682   Train: (loss = 0.1273f0, acc = 97.5)   Test: (loss = 0.7475f0, acc = 75.4286)\n",
      "Epoch: 683   Train: (loss = 0.1269f0, acc = 97.5)   Test: (loss = 0.7471f0, acc = 75.619)\n",
      "Epoch: 684   Train: (loss = 0.1264f0, acc = 97.5)   Test: (loss = 0.7471f0, acc = 75.619)\n",
      "Epoch: 685   Train: (loss = 0.126f0, acc = 97.5)   Test: (loss = 0.7465f0, acc = 75.619)\n",
      "Epoch: 686   Train: (loss = 0.1256f0, acc = 97.5)   Test: (loss = 0.7455f0, acc = 75.619)\n",
      "Epoch: 687   Train: (loss = 0.1252f0, acc = 97.5)   Test: (loss = 0.7439f0, acc = 75.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 688   Train: (loss = 0.1248f0, acc = 97.5)   Test: (loss = 0.7446f0, acc = 75.619)\n",
      "Epoch: 689   Train: (loss = 0.1244f0, acc = 97.5)   Test: (loss = 0.7437f0, acc = 75.619)\n",
      "Epoch: 690   Train: (loss = 0.1239f0, acc = 97.5)   Test: (loss = 0.7426f0, acc = 75.619)\n",
      "Epoch: 691   Train: (loss = 0.1235f0, acc = 97.6)   Test: (loss = 0.7418f0, acc = 75.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 692   Train: (loss = 0.1231f0, acc = 97.6)   Test: (loss = 0.7412f0, acc = 75.619)\n",
      "Epoch: 693   Train: (loss = 0.1227f0, acc = 97.6)   Test: (loss = 0.7404f0, acc = 75.8095)\n",
      "Epoch: 694   Train: (loss = 0.1223f0, acc = 97.6)   Test: (loss = 0.7397f0, acc = 75.8095)\n",
      "Epoch: 695   Train: (loss = 0.1219f0, acc = 97.6)   Test: (loss = 0.739f0, acc = 75.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 696   Train: (loss = 0.1215f0, acc = 97.6)   Test: (loss = 0.7386f0, acc = 75.8095)\n",
      "Epoch: 697   Train: (loss = 0.1211f0, acc = 97.6)   Test: (loss = 0.7384f0, acc = 75.8095)\n",
      "Epoch: 698   Train: (loss = 0.1207f0, acc = 97.6)   Test: (loss = 0.7377f0, acc = 75.8095)\n",
      "Epoch: 699   Train: (loss = 0.1203f0, acc = 97.6)   Test: (loss = 0.7366f0, acc = 75.8095)\n",
      "Epoch: 700   Train: (loss = 0.1199f0, acc = 97.6)   Test: (loss = 0.7363f0, acc = 75.8095)\n",
      "Epoch: 701   Train: (loss = 0.1195f0, acc = 97.6)   Test: (loss = 0.7353f0, acc = 75.8095)\n",
      "Epoch: 702   Train: (loss = 0.1191f0, acc = 97.6)   Test: (loss = 0.7345f0, acc = 75.8095)\n",
      "Epoch: 703   Train: (loss = 0.1187f0, acc = 97.6)   Test: (loss = 0.7338f0, acc = 75.8095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 704   Train: (loss = 0.1183f0, acc = 97.7)   Test: (loss = 0.7336f0, acc = 75.8095)\n",
      "Epoch: 705   Train: (loss = 0.118f0, acc = 97.7)   Test: (loss = 0.7336f0, acc = 75.8095)\n",
      "Epoch: 706   Train: (loss = 0.1176f0, acc = 97.7)   Test: (loss = 0.7326f0, acc = 76.0)\n",
      "Epoch: 707   Train: (loss = 0.1172f0, acc = 97.7)   Test: (loss = 0.7315f0, acc = 76.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 708   Train: (loss = 0.1168f0, acc = 97.7)   Test: (loss = 0.7315f0, acc = 76.0)\n",
      "Epoch: 709   Train: (loss = 0.1164f0, acc = 97.7)   Test: (loss = 0.7304f0, acc = 76.0)\n",
      "Epoch: 710   Train: (loss = 0.116f0, acc = 97.7)   Test: (loss = 0.7298f0, acc = 76.0)\n",
      "Epoch: 711   Train: (loss = 0.1157f0, acc = 97.7)   Test: (loss = 0.7292f0, acc = 76.0)\n",
      "Epoch: 712   Train: (loss = 0.1153f0, acc = 97.7)   Test: (loss = 0.729f0, acc = 76.0)\n",
      "Epoch: 713   Train: (loss = 0.1149f0, acc = 97.7)   Test: (loss = 0.7281f0, acc = 76.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 714   Train: (loss = 0.1145f0, acc = 97.7)   Test: (loss = 0.7281f0, acc = 76.0)\n",
      "Epoch: 715   Train: (loss = 0.1141f0, acc = 97.7)   Test: (loss = 0.7274f0, acc = 76.0)\n",
      "Epoch: 716   Train: (loss = 0.1138f0, acc = 97.7)   Test: (loss = 0.7263f0, acc = 76.0)\n",
      "Epoch: 717   Train: (loss = 0.1134f0, acc = 97.7)   Test: (loss = 0.7256f0, acc = 76.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 718   Train: (loss = 0.113f0, acc = 97.7)   Test: (loss = 0.7244f0, acc = 76.0)\n",
      "Epoch: 719   Train: (loss = 0.1126f0, acc = 97.7)   Test: (loss = 0.7234f0, acc = 76.1905)\n",
      "Epoch: 720   Train: (loss = 0.1123f0, acc = 97.7)   Test: (loss = 0.7234f0, acc = 76.1905)\n",
      "Epoch: 721   Train: (loss = 0.1119f0, acc = 97.7)   Test: (loss = 0.723f0, acc = 76.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 722   Train: (loss = 0.1115f0, acc = 97.8)   Test: (loss = 0.7221f0, acc = 76.1905)\n",
      "Epoch: 723   Train: (loss = 0.1112f0, acc = 97.8)   Test: (loss = 0.7216f0, acc = 76.1905)\n",
      "Epoch: 724   Train: (loss = 0.1108f0, acc = 97.8)   Test: (loss = 0.7211f0, acc = 76.1905)\n",
      "Epoch: 725   Train: (loss = 0.1104f0, acc = 97.8)   Test: (loss = 0.7204f0, acc = 76.1905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 726   Train: (loss = 0.1101f0, acc = 97.8)   Test: (loss = 0.7202f0, acc = 76.1905)\n",
      "Epoch: 727   Train: (loss = 0.1097f0, acc = 97.8)   Test: (loss = 0.7185f0, acc = 76.1905)\n",
      "Epoch: 728   Train: (loss = 0.1094f0, acc = 97.8)   Test: (loss = 0.7184f0, acc = 76.1905)\n",
      "Epoch: 729   Train: (loss = 0.109f0, acc = 97.8)   Test: (loss = 0.7178f0, acc = 76.381)\n",
      "Epoch: 730   Train: (loss = 0.1086f0, acc = 97.8)   Test: (loss = 0.7173f0, acc = 76.381)\n",
      "Epoch: 731   Train: (loss = 0.1083f0, acc = 97.8)   Test: (loss = 0.7172f0, acc = 76.381)\n",
      "Epoch: 732   Train: (loss = 0.1079f0, acc = 97.8)   Test: (loss = 0.7169f0, acc = 76.381)\n",
      "Epoch: 733   Train: (loss = 0.1076f0, acc = 97.8)   Test: (loss = 0.7168f0, acc = 76.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 734   Train: (loss = 0.1072f0, acc = 97.8)   Test: (loss = 0.7161f0, acc = 76.381)\n",
      "Epoch: 735   Train: (loss = 0.1069f0, acc = 97.8)   Test: (loss = 0.7164f0, acc = 76.381)\n",
      "Epoch: 736   Train: (loss = 0.1065f0, acc = 97.8)   Test: (loss = 0.7158f0, acc = 76.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 737   Train: (loss = 0.1062f0, acc = 97.8)   Test: (loss = 0.715f0, acc = 76.381)\n",
      "Epoch: 738   Train: (loss = 0.1058f0, acc = 97.9)   Test: (loss = 0.7144f0, acc = 76.381)\n",
      "Epoch: 739   Train: (loss = 0.1055f0, acc = 97.9)   Test: (loss = 0.7137f0, acc = 76.381)\n",
      "Epoch: 740   Train: (loss = 0.1051f0, acc = 97.9)   Test: (loss = 0.7126f0, acc = 76.381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 741   Train: (loss = 0.1048f0, acc = 97.9)   Test: (loss = 0.7132f0, acc = 76.381)\n",
      "Epoch: 742   Train: (loss = 0.1045f0, acc = 97.9)   Test: (loss = 0.7127f0, acc = 76.381)\n",
      "Epoch: 743   Train: (loss = 0.1041f0, acc = 97.9)   Test: (loss = 0.711f0, acc = 76.5714)\n",
      "Epoch: 744   Train: (loss = 0.1038f0, acc = 97.9)   Test: (loss = 0.7105f0, acc = 76.5714)\n",
      "Epoch: 745   Train: (loss = 0.1034f0, acc = 97.9)   Test: (loss = 0.7096f0, acc = 76.5714)\n",
      "Epoch: 746   Train: (loss = 0.1031f0, acc = 97.9)   Test: (loss = 0.71f0, acc = 76.5714)\n",
      "Epoch: 747   Train: (loss = 0.1028f0, acc = 98.0)   Test: (loss = 0.7095f0, acc = 76.5714)\n",
      "Epoch: 748   Train: (loss = 0.1024f0, acc = 98.0)   Test: (loss = 0.7078f0, acc = 76.5714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 749   Train: (loss = 0.1021f0, acc = 98.0)   Test: (loss = 0.7069f0, acc = 76.5714)\n",
      "Epoch: 750   Train: (loss = 0.1018f0, acc = 98.0)   Test: (loss = 0.7073f0, acc = 76.5714)\n",
      "Epoch: 751   Train: (loss = 0.1014f0, acc = 98.0)   Test: (loss = 0.7063f0, acc = 76.5714)\n",
      "Epoch: 752   Train: (loss = 0.1011f0, acc = 98.0)   Test: (loss = 0.7057f0, acc = 76.7619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 753   Train: (loss = 0.1008f0, acc = 98.0)   Test: (loss = 0.7049f0, acc = 76.7619)\n",
      "Epoch: 754   Train: (loss = 0.1005f0, acc = 98.0)   Test: (loss = 0.705f0, acc = 76.5714)\n",
      "Epoch: 755   Train: (loss = 0.1001f0, acc = 98.0)   Test: (loss = 0.7042f0, acc = 76.7619)\n",
      "Epoch: 756   Train: (loss = 0.0998f0, acc = 98.1)   Test: (loss = 0.704f0, acc = 76.7619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 757   Train: (loss = 0.0995f0, acc = 98.1)   Test: (loss = 0.7029f0, acc = 76.7619)\n",
      "Epoch: 758   Train: (loss = 0.0992f0, acc = 98.1)   Test: (loss = 0.7023f0, acc = 76.7619)\n",
      "Epoch: 759   Train: (loss = 0.0989f0, acc = 98.1)   Test: (loss = 0.702f0, acc = 76.7619)\n",
      "Epoch: 760   Train: (loss = 0.0985f0, acc = 98.1)   Test: (loss = 0.7017f0, acc = 76.7619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 761   Train: (loss = 0.0982f0, acc = 98.1)   Test: (loss = 0.7016f0, acc = 76.7619)\n",
      "Epoch: 762   Train: (loss = 0.0979f0, acc = 98.2)   Test: (loss = 0.701f0, acc = 76.7619)\n",
      "Epoch: 763   Train: (loss = 0.0976f0, acc = 98.2)   Test: (loss = 0.7007f0, acc = 76.7619)\n",
      "Epoch: 764   Train: (loss = 0.0973f0, acc = 98.2)   Test: (loss = 0.6991f0, acc = 76.7619)\n",
      "Epoch: 765   Train: (loss = 0.097f0, acc = 98.2)   Test: (loss = 0.6984f0, acc = 76.9524)\n",
      "Epoch: 766   Train: (loss = 0.0967f0, acc = 98.2)   Test: (loss = 0.6972f0, acc = 76.9524)\n",
      "Epoch: 767   Train: (loss = 0.0964f0, acc = 98.2)   Test: (loss = 0.6977f0, acc = 76.7619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 768   Train: (loss = 0.096f0, acc = 98.2)   Test: (loss = 0.6969f0, acc = 76.9524)\n",
      "Epoch: 769   Train: (loss = 0.0957f0, acc = 98.2)   Test: (loss = 0.6957f0, acc = 76.9524)\n",
      "Epoch: 770   Train: (loss = 0.0954f0, acc = 98.2)   Test: (loss = 0.6955f0, acc = 76.9524)\n",
      "Epoch: 771   Train: (loss = 0.0951f0, acc = 98.3)   Test: (loss = 0.6949f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 772   Train: (loss = 0.0948f0, acc = 98.3)   Test: (loss = 0.6936f0, acc = 76.9524)\n",
      "Epoch: 773   Train: (loss = 0.0945f0, acc = 98.3)   Test: (loss = 0.6944f0, acc = 76.9524)\n",
      "Epoch: 774   Train: (loss = 0.0942f0, acc = 98.3)   Test: (loss = 0.6942f0, acc = 76.9524)\n",
      "Epoch: 775   Train: (loss = 0.0939f0, acc = 98.3)   Test: (loss = 0.6935f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 776   Train: (loss = 0.0936f0, acc = 98.3)   Test: (loss = 0.6927f0, acc = 76.9524)\n",
      "Epoch: 777   Train: (loss = 0.0933f0, acc = 98.3)   Test: (loss = 0.6919f0, acc = 76.9524)\n",
      "Epoch: 778   Train: (loss = 0.093f0, acc = 98.3)   Test: (loss = 0.6916f0, acc = 76.9524)\n",
      "Epoch: 779   Train: (loss = 0.0927f0, acc = 98.3)   Test: (loss = 0.6909f0, acc = 77.1429)\n",
      "Epoch: 780   Train: (loss = 0.0924f0, acc = 98.4)   Test: (loss = 0.6904f0, acc = 77.1429)\n",
      "Epoch: 781   Train: (loss = 0.0921f0, acc = 98.4)   Test: (loss = 0.6904f0, acc = 77.1429)\n",
      "Epoch: 782   Train: (loss = 0.0918f0, acc = 98.4)   Test: (loss = 0.69f0, acc = 77.1429)\n",
      "Epoch: 783   Train: (loss = 0.0915f0, acc = 98.4)   Test: (loss = 0.6895f0, acc = 77.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 784   Train: (loss = 0.0912f0, acc = 98.4)   Test: (loss = 0.6883f0, acc = 77.1429)\n",
      "Epoch: 785   Train: (loss = 0.0909f0, acc = 98.4)   Test: (loss = 0.6879f0, acc = 77.1429)\n",
      "Epoch: 786   Train: (loss = 0.0906f0, acc = 98.5)   Test: (loss = 0.6872f0, acc = 77.1429)\n",
      "Epoch: 787   Train: (loss = 0.0904f0, acc = 98.6)   Test: (loss = 0.6866f0, acc = 77.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 788   Train: (loss = 0.0901f0, acc = 98.5)   Test: (loss = 0.6861f0, acc = 77.1429)\n",
      "Epoch: 789   Train: (loss = 0.0898f0, acc = 98.6)   Test: (loss = 0.686f0, acc = 77.1429)\n",
      "Epoch: 790   Train: (loss = 0.0895f0, acc = 98.5)   Test: (loss = 0.6853f0, acc = 77.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 791   Train: (loss = 0.0892f0, acc = 98.5)   Test: (loss = 0.6848f0, acc = 77.1429)\n",
      "Epoch: 792   Train: (loss = 0.0889f0, acc = 98.6)   Test: (loss = 0.6843f0, acc = 77.1429)\n",
      "Epoch: 793   Train: (loss = 0.0886f0, acc = 98.6)   Test: (loss = 0.6839f0, acc = 77.1429)\n",
      "Epoch: 794   Train: (loss = 0.0884f0, acc = 98.7)   Test: (loss = 0.6833f0, acc = 77.1429)\n",
      "Epoch: 795   Train: (loss = 0.0881f0, acc = 98.7)   Test: (loss = 0.6821f0, acc = 77.1429)\n",
      "Epoch: 796   Train: (loss = 0.0878f0, acc = 98.7)   Test: (loss = 0.6824f0, acc = 76.9524)\n",
      "Epoch: 797   Train: (loss = 0.0875f0, acc = 98.7)   Test: (loss = 0.6824f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 798   Train: (loss = 0.0872f0, acc = 98.7)   Test: (loss = 0.6808f0, acc = 77.1429)\n",
      "Epoch: 799   Train: (loss = 0.087f0, acc = 98.7)   Test: (loss = 0.6815f0, acc = 76.9524)\n",
      "Epoch: 800   Train: (loss = 0.0867f0, acc = 98.7)   Test: (loss = 0.6802f0, acc = 77.1429)\n",
      "Epoch: 801   Train: (loss = 0.0864f0, acc = 98.6)   Test: (loss = 0.6796f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 802   Train: (loss = 0.0861f0, acc = 98.6)   Test: (loss = 0.6795f0, acc = 76.9524)\n",
      "Epoch: 803   Train: (loss = 0.0859f0, acc = 98.7)   Test: (loss = 0.6787f0, acc = 76.9524)\n",
      "Epoch: 804   Train: (loss = 0.0856f0, acc = 98.7)   Test: (loss = 0.6778f0, acc = 76.9524)\n",
      "Epoch: 805   Train: (loss = 0.0853f0, acc = 98.7)   Test: (loss = 0.6782f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 806   Train: (loss = 0.085f0, acc = 98.7)   Test: (loss = 0.6774f0, acc = 76.9524)\n",
      "Epoch: 807   Train: (loss = 0.0848f0, acc = 98.7)   Test: (loss = 0.6776f0, acc = 76.9524)\n",
      "Epoch: 808   Train: (loss = 0.0845f0, acc = 98.8)   Test: (loss = 0.6764f0, acc = 76.9524)\n",
      "Epoch: 809   Train: (loss = 0.0842f0, acc = 98.8)   Test: (loss = 0.674f0, acc = 77.1429)\n",
      "Epoch: 810   Train: (loss = 0.084f0, acc = 98.8)   Test: (loss = 0.6748f0, acc = 76.9524)\n",
      "Epoch: 811   Train: (loss = 0.0837f0, acc = 98.8)   Test: (loss = 0.6741f0, acc = 76.9524)\n",
      "Epoch: 812   Train: (loss = 0.0834f0, acc = 98.8)   Test: (loss = 0.6729f0, acc = 76.9524)\n",
      "Epoch: 813   Train: (loss = 0.0832f0, acc = 98.8)   Test: (loss = 0.6728f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 814   Train: (loss = 0.0829f0, acc = 98.8)   Test: (loss = 0.6726f0, acc = 76.9524)\n",
      "Epoch: 815   Train: (loss = 0.0826f0, acc = 98.8)   Test: (loss = 0.6716f0, acc = 76.9524)\n",
      "Epoch: 816   Train: (loss = 0.0824f0, acc = 98.8)   Test: (loss = 0.6713f0, acc = 76.9524)\n",
      "Epoch: 817   Train: (loss = 0.0821f0, acc = 98.8)   Test: (loss = 0.6706f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 818   Train: (loss = 0.0818f0, acc = 98.8)   Test: (loss = 0.6708f0, acc = 76.9524)\n",
      "Epoch: 819   Train: (loss = 0.0816f0, acc = 98.8)   Test: (loss = 0.6708f0, acc = 76.9524)\n",
      "Epoch: 820   Train: (loss = 0.0813f0, acc = 98.8)   Test: (loss = 0.6701f0, acc = 76.9524)\n",
      "Epoch: 821   Train: (loss = 0.0811f0, acc = 98.8)   Test: (loss = 0.6704f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 822   Train: (loss = 0.0808f0, acc = 98.8)   Test: (loss = 0.6694f0, acc = 76.9524)\n",
      "Epoch: 823   Train: (loss = 0.0806f0, acc = 98.8)   Test: (loss = 0.669f0, acc = 76.9524)\n",
      "Epoch: 824   Train: (loss = 0.0803f0, acc = 98.8)   Test: (loss = 0.6681f0, acc = 76.9524)\n",
      "Epoch: 825   Train: (loss = 0.0801f0, acc = 98.8)   Test: (loss = 0.6672f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 826   Train: (loss = 0.0798f0, acc = 98.8)   Test: (loss = 0.6673f0, acc = 76.9524)\n",
      "Epoch: 827   Train: (loss = 0.0795f0, acc = 98.8)   Test: (loss = 0.6667f0, acc = 76.9524)\n",
      "Epoch: 828   Train: (loss = 0.0793f0, acc = 98.8)   Test: (loss = 0.666f0, acc = 76.9524)\n",
      "Epoch: 829   Train: (loss = 0.079f0, acc = 98.8)   Test: (loss = 0.6656f0, acc = 76.9524)\n",
      "Epoch: 830   Train: (loss = 0.0788f0, acc = 98.8)   Test: (loss = 0.6651f0, acc = 76.9524)\n",
      "Epoch: 831   Train: (loss = 0.0785f0, acc = 98.8)   Test: (loss = 0.6651f0, acc = 76.9524)\n",
      "Epoch: 832   Train: (loss = 0.0783f0, acc = 98.8)   Test: (loss = 0.665f0, acc = 76.9524)\n",
      "Epoch: 833   Train: (loss = 0.078f0, acc = 98.8)   Test: (loss = 0.6646f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 834   Train: (loss = 0.0778f0, acc = 98.8)   Test: (loss = 0.6638f0, acc = 76.9524)\n",
      "Epoch: 835   Train: (loss = 0.0776f0, acc = 98.8)   Test: (loss = 0.6632f0, acc = 76.9524)\n",
      "Epoch: 836   Train: (loss = 0.0773f0, acc = 98.8)   Test: (loss = 0.6629f0, acc = 76.9524)\n",
      "Epoch: 837   Train: (loss = 0.0771f0, acc = 98.8)   Test: (loss = 0.663f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 838   Train: (loss = 0.0768f0, acc = 98.8)   Test: (loss = 0.6624f0, acc = 76.9524)\n",
      "Epoch: 839   Train: (loss = 0.0766f0, acc = 98.8)   Test: (loss = 0.6622f0, acc = 76.9524)\n",
      "Epoch: 840   Train: (loss = 0.0763f0, acc = 98.8)   Test: (loss = 0.6623f0, acc = 76.9524)\n",
      "Epoch: 841   Train: (loss = 0.0761f0, acc = 98.8)   Test: (loss = 0.6611f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 842   Train: (loss = 0.0759f0, acc = 98.8)   Test: (loss = 0.6609f0, acc = 76.9524)\n",
      "Epoch: 843   Train: (loss = 0.0756f0, acc = 98.8)   Test: (loss = 0.6613f0, acc = 76.9524)\n",
      "Epoch: 844   Train: (loss = 0.0754f0, acc = 98.8)   Test: (loss = 0.6608f0, acc = 76.9524)\n",
      "Epoch: 845   Train: (loss = 0.0751f0, acc = 98.8)   Test: (loss = 0.6602f0, acc = 76.9524)\n",
      "Epoch: 846   Train: (loss = 0.0749f0, acc = 98.8)   Test: (loss = 0.6597f0, acc = 76.9524)\n",
      "Epoch: 847   Train: (loss = 0.0747f0, acc = 98.8)   Test: (loss = 0.659f0, acc = 76.9524)\n",
      "Epoch: 848   Train: (loss = 0.0744f0, acc = 98.8)   Test: (loss = 0.6583f0, acc = 76.9524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 849   Train: (loss = 0.0742f0, acc = 98.8)   Test: (loss = 0.6581f0, acc = 76.9524)\n",
      "Epoch: 850   Train: (loss = 0.074f0, acc = 98.8)   Test: (loss = 0.6569f0, acc = 76.9524)\n",
      "Epoch: 851   Train: (loss = 0.0737f0, acc = 98.8)   Test: (loss = 0.6558f0, acc = 76.9524)\n",
      "Epoch: 852   Train: (loss = 0.0735f0, acc = 98.8)   Test: (loss = 0.6549f0, acc = 77.1429)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 853   Train: (loss = 0.0732f0, acc = 98.8)   Test: (loss = 0.6548f0, acc = 77.1429)\n",
      "Epoch: 854   Train: (loss = 0.073f0, acc = 98.8)   Test: (loss = 0.6547f0, acc = 77.3333)\n",
      "Epoch: 855   Train: (loss = 0.0728f0, acc = 98.8)   Test: (loss = 0.6543f0, acc = 77.3333)\n",
      "Epoch: 856   Train: (loss = 0.0725f0, acc = 98.8)   Test: (loss = 0.6542f0, acc = 77.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 857   Train: (loss = 0.0723f0, acc = 98.8)   Test: (loss = 0.6546f0, acc = 77.3333)\n",
      "Epoch: 858   Train: (loss = 0.0721f0, acc = 98.8)   Test: (loss = 0.6539f0, acc = 77.3333)\n",
      "Epoch: 859   Train: (loss = 0.0719f0, acc = 98.8)   Test: (loss = 0.654f0, acc = 77.3333)\n",
      "Epoch: 860   Train: (loss = 0.0716f0, acc = 98.8)   Test: (loss = 0.6535f0, acc = 77.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 861   Train: (loss = 0.0714f0, acc = 98.9)   Test: (loss = 0.6531f0, acc = 77.3333)\n",
      "Epoch: 862   Train: (loss = 0.0712f0, acc = 98.9)   Test: (loss = 0.6527f0, acc = 77.3333)\n",
      "Epoch: 863   Train: (loss = 0.0709f0, acc = 98.8)   Test: (loss = 0.6524f0, acc = 77.3333)\n",
      "Epoch: 864   Train: (loss = 0.0707f0, acc = 98.9)   Test: (loss = 0.6523f0, acc = 77.3333)\n",
      "Epoch: 865   Train: (loss = 0.0705f0, acc = 98.9)   Test: (loss = 0.6518f0, acc = 77.3333)\n",
      "Epoch: 866   Train: (loss = 0.0703f0, acc = 98.9)   Test: (loss = 0.6515f0, acc = 77.3333)\n",
      "Epoch: 867   Train: (loss = 0.07f0, acc = 98.9)   Test: (loss = 0.6505f0, acc = 77.3333)\n",
      "Epoch: 868   Train: (loss = 0.0698f0, acc = 98.9)   Test: (loss = 0.6502f0, acc = 77.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 869   Train: (loss = 0.0696f0, acc = 98.9)   Test: (loss = 0.6498f0, acc = 77.3333)\n",
      "Epoch: 870   Train: (loss = 0.0694f0, acc = 98.9)   Test: (loss = 0.6503f0, acc = 77.3333)\n",
      "Epoch: 871   Train: (loss = 0.0692f0, acc = 98.9)   Test: (loss = 0.6497f0, acc = 77.3333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 872   Train: (loss = 0.0689f0, acc = 98.9)   Test: (loss = 0.6495f0, acc = 77.3333)\n",
      "Epoch: 873   Train: (loss = 0.0687f0, acc = 98.9)   Test: (loss = 0.6486f0, acc = 77.5238)\n",
      "Epoch: 874   Train: (loss = 0.0685f0, acc = 98.9)   Test: (loss = 0.6481f0, acc = 77.5238)\n",
      "Epoch: 875   Train: (loss = 0.0683f0, acc = 98.9)   Test: (loss = 0.6484f0, acc = 77.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 876   Train: (loss = 0.0681f0, acc = 98.9)   Test: (loss = 0.6478f0, acc = 77.5238)\n",
      "Epoch: 877   Train: (loss = 0.0679f0, acc = 98.9)   Test: (loss = 0.647f0, acc = 77.5238)\n",
      "Epoch: 878   Train: (loss = 0.0676f0, acc = 98.9)   Test: (loss = 0.6459f0, acc = 77.5238)\n",
      "Epoch: 879   Train: (loss = 0.0674f0, acc = 98.9)   Test: (loss = 0.6456f0, acc = 77.5238)\n",
      "Epoch: 880   Train: (loss = 0.0672f0, acc = 98.9)   Test: (loss = 0.6463f0, acc = 77.5238)\n",
      "Epoch: 881   Train: (loss = 0.067f0, acc = 98.9)   Test: (loss = 0.646f0, acc = 77.5238)\n",
      "Epoch: 882   Train: (loss = 0.0668f0, acc = 98.9)   Test: (loss = 0.6448f0, acc = 77.5238)\n",
      "Epoch: 883   Train: (loss = 0.0666f0, acc = 98.9)   Test: (loss = 0.6447f0, acc = 77.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 884   Train: (loss = 0.0664f0, acc = 98.9)   Test: (loss = 0.6436f0, acc = 77.5238)\n",
      "Epoch: 885   Train: (loss = 0.0661f0, acc = 98.9)   Test: (loss = 0.6432f0, acc = 77.5238)\n",
      "Epoch: 886   Train: (loss = 0.0659f0, acc = 98.9)   Test: (loss = 0.643f0, acc = 77.5238)\n",
      "Epoch: 887   Train: (loss = 0.0657f0, acc = 98.9)   Test: (loss = 0.6428f0, acc = 77.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 888   Train: (loss = 0.0655f0, acc = 98.9)   Test: (loss = 0.6423f0, acc = 77.5238)\n",
      "Epoch: 889   Train: (loss = 0.0653f0, acc = 98.9)   Test: (loss = 0.6415f0, acc = 77.7143)\n",
      "Epoch: 890   Train: (loss = 0.0651f0, acc = 98.9)   Test: (loss = 0.6408f0, acc = 77.7143)\n",
      "Epoch: 891   Train: (loss = 0.0649f0, acc = 98.9)   Test: (loss = 0.6408f0, acc = 77.5238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 892   Train: (loss = 0.0647f0, acc = 98.9)   Test: (loss = 0.6406f0, acc = 77.7143)\n",
      "Epoch: 893   Train: (loss = 0.0645f0, acc = 98.9)   Test: (loss = 0.6406f0, acc = 77.7143)\n",
      "Epoch: 894   Train: (loss = 0.0643f0, acc = 98.9)   Test: (loss = 0.6396f0, acc = 77.7143)\n",
      "Epoch: 895   Train: (loss = 0.0641f0, acc = 98.9)   Test: (loss = 0.639f0, acc = 77.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 896   Train: (loss = 0.0639f0, acc = 98.9)   Test: (loss = 0.6387f0, acc = 77.7143)\n",
      "Epoch: 897   Train: (loss = 0.0637f0, acc = 99.0)   Test: (loss = 0.6385f0, acc = 77.7143)\n",
      "Epoch: 898   Train: (loss = 0.0635f0, acc = 99.0)   Test: (loss = 0.6379f0, acc = 77.7143)\n",
      "Epoch: 899   Train: (loss = 0.0633f0, acc = 99.0)   Test: (loss = 0.6379f0, acc = 77.7143)\n",
      "Epoch: 900   Train: (loss = 0.0631f0, acc = 99.0)   Test: (loss = 0.6372f0, acc = 77.7143)\n",
      "Epoch: 901   Train: (loss = 0.0629f0, acc = 99.1)   Test: (loss = 0.6373f0, acc = 77.7143)\n",
      "Epoch: 902   Train: (loss = 0.0627f0, acc = 99.1)   Test: (loss = 0.6373f0, acc = 77.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 903   Train: (loss = 0.0625f0, acc = 99.1)   Test: (loss = 0.6364f0, acc = 77.7143)\n",
      "Epoch: 904   Train: (loss = 0.0623f0, acc = 99.1)   Test: (loss = 0.6361f0, acc = 77.7143)\n",
      "Epoch: 905   Train: (loss = 0.0621f0, acc = 99.1)   Test: (loss = 0.6369f0, acc = 77.7143)\n",
      "Epoch: 906   Train: (loss = 0.0619f0, acc = 99.1)   Test: (loss = 0.6363f0, acc = 77.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 907   Train: (loss = 0.0617f0, acc = 99.1)   Test: (loss = 0.635f0, acc = 77.7143)\n",
      "Epoch: 908   Train: (loss = 0.0615f0, acc = 99.2)   Test: (loss = 0.6344f0, acc = 77.7143)\n",
      "Epoch: 909   Train: (loss = 0.0613f0, acc = 99.2)   Test: (loss = 0.6332f0, acc = 77.7143)\n",
      "Epoch: 910   Train: (loss = 0.0611f0, acc = 99.2)   Test: (loss = 0.634f0, acc = 77.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 911   Train: (loss = 0.0609f0, acc = 99.3)   Test: (loss = 0.633f0, acc = 77.7143)\n",
      "Epoch: 912   Train: (loss = 0.0607f0, acc = 99.3)   Test: (loss = 0.6326f0, acc = 77.7143)\n",
      "Epoch: 913   Train: (loss = 0.0605f0, acc = 99.3)   Test: (loss = 0.6331f0, acc = 77.7143)\n",
      "Epoch: 914   Train: (loss = 0.0603f0, acc = 99.3)   Test: (loss = 0.6324f0, acc = 77.7143)\n",
      "Epoch: 915   Train: (loss = 0.0601f0, acc = 99.3)   Test: (loss = 0.6314f0, acc = 77.7143)\n",
      "Epoch: 916   Train: (loss = 0.0599f0, acc = 99.3)   Test: (loss = 0.6313f0, acc = 77.9048)\n",
      "Epoch: 917   Train: (loss = 0.0597f0, acc = 99.3)   Test: (loss = 0.6306f0, acc = 77.9048)\n",
      "Epoch: 918   Train: (loss = 0.0596f0, acc = 99.3)   Test: (loss = 0.6302f0, acc = 78.0952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 919   Train: (loss = 0.0594f0, acc = 99.3)   Test: (loss = 0.6302f0, acc = 78.2857)\n",
      "Epoch: 920   Train: (loss = 0.0592f0, acc = 99.3)   Test: (loss = 0.6303f0, acc = 78.2857)\n",
      "Epoch: 921   Train: (loss = 0.059f0, acc = 99.3)   Test: (loss = 0.6299f0, acc = 78.2857)\n",
      "Epoch: 922   Train: (loss = 0.0588f0, acc = 99.3)   Test: (loss = 0.6296f0, acc = 78.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 923   Train: (loss = 0.0586f0, acc = 99.3)   Test: (loss = 0.629f0, acc = 78.2857)\n",
      "Epoch: 924   Train: (loss = 0.0584f0, acc = 99.3)   Test: (loss = 0.6275f0, acc = 78.2857)\n",
      "Epoch: 925   Train: (loss = 0.0583f0, acc = 99.3)   Test: (loss = 0.6274f0, acc = 78.2857)\n",
      "Epoch: 926   Train: (loss = 0.0581f0, acc = 99.3)   Test: (loss = 0.6274f0, acc = 78.2857)\n",
      "Epoch: 927   Train: (loss = 0.0579f0, acc = 99.3)   Test: (loss = 0.6264f0, acc = 78.2857)\n",
      "Epoch: 928   Train: (loss = 0.0577f0, acc = 99.4)   Test: (loss = 0.6264f0, acc = 78.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 929   Train: (loss = 0.0575f0, acc = 99.4)   Test: (loss = 0.626f0, acc = 78.2857)\n",
      "Epoch: 930   Train: (loss = 0.0573f0, acc = 99.4)   Test: (loss = 0.6256f0, acc = 78.2857)\n",
      "Epoch: 931   Train: (loss = 0.0572f0, acc = 99.5)   Test: (loss = 0.6257f0, acc = 78.2857)\n",
      "Epoch: 932   Train: (loss = 0.057f0, acc = 99.5)   Test: (loss = 0.6245f0, acc = 78.2857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 933   Train: (loss = 0.0568f0, acc = 99.5)   Test: (loss = 0.624f0, acc = 78.2857)\n",
      "Epoch: 934   Train: (loss = 0.0566f0, acc = 99.5)   Test: (loss = 0.6234f0, acc = 78.4762)\n",
      "Epoch: 935   Train: (loss = 0.0564f0, acc = 99.5)   Test: (loss = 0.6237f0, acc = 78.4762)\n",
      "Epoch: 936   Train: (loss = 0.0563f0, acc = 99.5)   Test: (loss = 0.623f0, acc = 78.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 937   Train: (loss = 0.0561f0, acc = 99.5)   Test: (loss = 0.6232f0, acc = 78.4762)\n",
      "Epoch: 938   Train: (loss = 0.0559f0, acc = 99.5)   Test: (loss = 0.6228f0, acc = 78.4762)\n",
      "Epoch: 939   Train: (loss = 0.0557f0, acc = 99.5)   Test: (loss = 0.6226f0, acc = 78.4762)\n",
      "Epoch: 940   Train: (loss = 0.0556f0, acc = 99.5)   Test: (loss = 0.622f0, acc = 78.4762)\n",
      "Epoch: 941   Train: (loss = 0.0554f0, acc = 99.5)   Test: (loss = 0.6219f0, acc = 78.4762)\n",
      "Epoch: 942   Train: (loss = 0.0552f0, acc = 99.5)   Test: (loss = 0.6216f0, acc = 78.4762)\n",
      "Epoch: 943   Train: (loss = 0.055f0, acc = 99.5)   Test: (loss = 0.6218f0, acc = 78.2857)\n",
      "Epoch: 944   Train: (loss = 0.0549f0, acc = 99.5)   Test: (loss = 0.622f0, acc = 78.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 945   Train: (loss = 0.0547f0, acc = 99.5)   Test: (loss = 0.6204f0, acc = 78.4762)\n",
      "Epoch: 946   Train: (loss = 0.0545f0, acc = 99.5)   Test: (loss = 0.6197f0, acc = 78.6667)\n",
      "Epoch: 947   Train: (loss = 0.0543f0, acc = 99.6)   Test: (loss = 0.6197f0, acc = 78.6667)\n",
      "Epoch: 948   Train: (loss = 0.0542f0, acc = 99.6)   Test: (loss = 0.6194f0, acc = 78.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 949   Train: (loss = 0.054f0, acc = 99.6)   Test: (loss = 0.6191f0, acc = 78.4762)\n",
      "Epoch: 950   Train: (loss = 0.0538f0, acc = 99.6)   Test: (loss = 0.619f0, acc = 78.4762)\n",
      "Epoch: 951   Train: (loss = 0.0537f0, acc = 99.6)   Test: (loss = 0.6193f0, acc = 78.4762)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 952   Train: (loss = 0.0535f0, acc = 99.6)   Test: (loss = 0.6188f0, acc = 78.6667)\n",
      "Epoch: 953   Train: (loss = 0.0533f0, acc = 99.6)   Test: (loss = 0.6186f0, acc = 78.6667)\n",
      "Epoch: 954   Train: (loss = 0.0532f0, acc = 99.6)   Test: (loss = 0.6178f0, acc = 78.8571)\n",
      "Epoch: 955   Train: (loss = 0.053f0, acc = 99.6)   Test: (loss = 0.6177f0, acc = 78.8571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 956   Train: (loss = 0.0528f0, acc = 99.6)   Test: (loss = 0.6177f0, acc = 78.8571)\n",
      "Epoch: 957   Train: (loss = 0.0527f0, acc = 99.6)   Test: (loss = 0.6177f0, acc = 78.8571)\n",
      "Epoch: 958   Train: (loss = 0.0525f0, acc = 99.6)   Test: (loss = 0.6171f0, acc = 78.8571)\n",
      "Epoch: 959   Train: (loss = 0.0523f0, acc = 99.6)   Test: (loss = 0.6168f0, acc = 79.0476)\n",
      "Epoch: 960   Train: (loss = 0.0522f0, acc = 99.7)   Test: (loss = 0.6168f0, acc = 78.8571)\n",
      "Epoch: 961   Train: (loss = 0.052f0, acc = 99.7)   Test: (loss = 0.6163f0, acc = 78.8571)\n",
      "Epoch: 962   Train: (loss = 0.0518f0, acc = 99.7)   Test: (loss = 0.6161f0, acc = 78.8571)\n",
      "Epoch: 963   Train: (loss = 0.0517f0, acc = 99.8)   Test: (loss = 0.615f0, acc = 79.0476)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 964   Train: (loss = 0.0515f0, acc = 99.8)   Test: (loss = 0.6147f0, acc = 79.0476)\n",
      "Epoch: 965   Train: (loss = 0.0514f0, acc = 99.8)   Test: (loss = 0.6145f0, acc = 79.2381)\n",
      "Epoch: 966   Train: (loss = 0.0512f0, acc = 99.8)   Test: (loss = 0.6142f0, acc = 79.4286)\n",
      "Epoch: 967   Train: (loss = 0.051f0, acc = 99.8)   Test: (loss = 0.6143f0, acc = 79.2381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 968   Train: (loss = 0.0509f0, acc = 99.8)   Test: (loss = 0.6134f0, acc = 79.619)\n",
      "Epoch: 969   Train: (loss = 0.0507f0, acc = 99.9)   Test: (loss = 0.6133f0, acc = 79.4286)\n",
      "Epoch: 970   Train: (loss = 0.0505f0, acc = 99.9)   Test: (loss = 0.6132f0, acc = 79.4286)\n",
      "Epoch: 971   Train: (loss = 0.0504f0, acc = 99.9)   Test: (loss = 0.6132f0, acc = 79.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 972   Train: (loss = 0.0502f0, acc = 99.9)   Test: (loss = 0.6128f0, acc = 79.4286)\n",
      "Epoch: 973   Train: (loss = 0.0501f0, acc = 99.9)   Test: (loss = 0.6123f0, acc = 79.4286)\n",
      "Epoch: 974   Train: (loss = 0.0499f0, acc = 99.9)   Test: (loss = 0.6117f0, acc = 79.4286)\n",
      "Epoch: 975   Train: (loss = 0.0498f0, acc = 99.9)   Test: (loss = 0.6114f0, acc = 79.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 976   Train: (loss = 0.0496f0, acc = 99.9)   Test: (loss = 0.6114f0, acc = 79.4286)\n",
      "Epoch: 977   Train: (loss = 0.0494f0, acc = 99.9)   Test: (loss = 0.61f0, acc = 79.4286)\n",
      "Epoch: 978   Train: (loss = 0.0493f0, acc = 99.9)   Test: (loss = 0.6105f0, acc = 79.4286)\n",
      "Epoch: 979   Train: (loss = 0.0491f0, acc = 99.9)   Test: (loss = 0.6098f0, acc = 79.4286)\n",
      "Epoch: 980   Train: (loss = 0.049f0, acc = 99.9)   Test: (loss = 0.6097f0, acc = 79.4286)\n",
      "Epoch: 981   Train: (loss = 0.0488f0, acc = 99.9)   Test: (loss = 0.6095f0, acc = 79.4286)\n",
      "Epoch: 982   Train: (loss = 0.0487f0, acc = 99.9)   Test: (loss = 0.6092f0, acc = 79.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 983   Train: (loss = 0.0485f0, acc = 99.9)   Test: (loss = 0.6086f0, acc = 79.4286)\n",
      "Epoch: 984   Train: (loss = 0.0484f0, acc = 99.9)   Test: (loss = 0.6082f0, acc = 79.4286)\n",
      "Epoch: 985   Train: (loss = 0.0482f0, acc = 99.9)   Test: (loss = 0.6075f0, acc = 79.4286)\n",
      "Epoch: 986   Train: (loss = 0.0481f0, acc = 99.9)   Test: (loss = 0.6071f0, acc = 79.4286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 987   Train: (loss = 0.0479f0, acc = 99.9)   Test: (loss = 0.6074f0, acc = 79.4286)\n",
      "Epoch: 988   Train: (loss = 0.0478f0, acc = 99.9)   Test: (loss = 0.6064f0, acc = 79.619)\n",
      "Epoch: 989   Train: (loss = 0.0476f0, acc = 99.9)   Test: (loss = 0.6067f0, acc = 79.619)\n",
      "Epoch: 990   Train: (loss = 0.0475f0, acc = 99.9)   Test: (loss = 0.6061f0, acc = 79.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 991   Train: (loss = 0.0473f0, acc = 99.9)   Test: (loss = 0.6057f0, acc = 79.619)\n",
      "Epoch: 992   Train: (loss = 0.0472f0, acc = 99.9)   Test: (loss = 0.6059f0, acc = 79.619)\n",
      "Epoch: 993   Train: (loss = 0.047f0, acc = 99.9)   Test: (loss = 0.6057f0, acc = 79.619)\n",
      "Epoch: 994   Train: (loss = 0.0469f0, acc = 99.9)   Test: (loss = 0.6053f0, acc = 79.619)\n",
      "Epoch: 995   Train: (loss = 0.0467f0, acc = 99.9)   Test: (loss = 0.6057f0, acc = 79.619)\n",
      "Epoch: 996   Train: (loss = 0.0466f0, acc = 99.9)   Test: (loss = 0.6059f0, acc = 79.619)\n",
      "Epoch: 997   Train: (loss = 0.0464f0, acc = 99.9)   Test: (loss = 0.6051f0, acc = 79.619)\n",
      "Epoch: 998   Train: (loss = 0.0463f0, acc = 99.9)   Test: (loss = 0.6043f0, acc = 79.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 999   Train: (loss = 0.0462f0, acc = 99.9)   Test: (loss = 0.6038f0, acc = 79.619)\n",
      "Epoch: 1000   Train: (loss = 0.046f0, acc = 99.9)   Test: (loss = 0.6037f0, acc = 79.619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Model saved in \"runs/model.bson\"\n",
      "└ @ Main In[22]:80\n"
     ]
    }
   ],
   "source": [
    "model, test_loader,trainloss, testloss, trainacc, testacc = train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the train and test losses ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip550\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip550)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip551\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip550)\" d=\"\n",
       "M219.866 1423.18 L2352.76 1423.18 L2352.76 123.472 L219.866 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip552\">\n",
       "    <rect x=\"219\" y=\"123\" width=\"2134\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  278.217,1423.18 278.217,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  781.76,1423.18 781.76,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1285.3,1423.18 1285.3,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1788.85,1423.18 1788.85,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.39,1423.18 2292.39,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  278.217,1423.18 278.217,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  781.76,1423.18 781.76,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1285.3,1423.18 1285.3,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1788.85,1423.18 1788.85,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.39,1423.18 2292.39,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip550)\" d=\"M278.217 1454.1 Q274.606 1454.1 272.777 1457.66 Q270.972 1461.2 270.972 1468.33 Q270.972 1475.44 272.777 1479.01 Q274.606 1482.55 278.217 1482.55 Q281.851 1482.55 283.657 1479.01 Q285.485 1475.44 285.485 1468.33 Q285.485 1461.2 283.657 1457.66 Q281.851 1454.1 278.217 1454.1 M278.217 1450.39 Q284.027 1450.39 287.083 1455 Q290.161 1459.58 290.161 1468.33 Q290.161 1477.06 287.083 1481.67 Q284.027 1486.25 278.217 1486.25 Q272.407 1486.25 269.328 1481.67 Q266.273 1477.06 266.273 1468.33 Q266.273 1459.58 269.328 1455 Q272.407 1450.39 278.217 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M745.453 1481.64 L761.772 1481.64 L761.772 1485.58 L739.828 1485.58 L739.828 1481.64 Q742.49 1478.89 747.073 1474.26 Q751.68 1469.61 752.86 1468.27 Q755.105 1465.74 755.985 1464.01 Q756.888 1462.25 756.888 1460.56 Q756.888 1457.8 754.943 1456.07 Q753.022 1454.33 749.92 1454.33 Q747.721 1454.33 745.268 1455.09 Q742.837 1455.86 740.059 1457.41 L740.059 1452.69 Q742.883 1451.55 745.337 1450.97 Q747.791 1450.39 749.828 1450.39 Q755.198 1450.39 758.393 1453.08 Q761.587 1455.77 761.587 1460.26 Q761.587 1462.39 760.777 1464.31 Q759.99 1466.2 757.883 1468.8 Q757.305 1469.47 754.203 1472.69 Q751.101 1475.88 745.453 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M771.633 1451.02 L789.99 1451.02 L789.99 1454.96 L775.916 1454.96 L775.916 1463.43 Q776.934 1463.08 777.953 1462.92 Q778.971 1462.73 779.99 1462.73 Q785.777 1462.73 789.156 1465.9 Q792.536 1469.08 792.536 1474.49 Q792.536 1480.07 789.064 1483.17 Q785.591 1486.25 779.272 1486.25 Q777.096 1486.25 774.828 1485.88 Q772.582 1485.51 770.175 1484.77 L770.175 1480.07 Q772.258 1481.2 774.48 1481.76 Q776.703 1482.32 779.179 1482.32 Q783.184 1482.32 785.522 1480.21 Q787.86 1478.1 787.86 1474.49 Q787.86 1470.88 785.522 1468.77 Q783.184 1466.67 779.179 1466.67 Q777.304 1466.67 775.429 1467.08 Q773.578 1467.5 771.633 1468.38 L771.633 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M811.749 1454.1 Q808.138 1454.1 806.309 1457.66 Q804.503 1461.2 804.503 1468.33 Q804.503 1475.44 806.309 1479.01 Q808.138 1482.55 811.749 1482.55 Q815.383 1482.55 817.188 1479.01 Q819.017 1475.44 819.017 1468.33 Q819.017 1461.2 817.188 1457.66 Q815.383 1454.1 811.749 1454.1 M811.749 1450.39 Q817.559 1450.39 820.614 1455 Q823.693 1459.58 823.693 1468.33 Q823.693 1477.06 820.614 1481.67 Q817.559 1486.25 811.749 1486.25 Q805.939 1486.25 802.86 1481.67 Q799.804 1477.06 799.804 1468.33 Q799.804 1459.58 802.86 1455 Q805.939 1450.39 811.749 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1244.92 1451.02 L1263.28 1451.02 L1263.28 1454.96 L1249.2 1454.96 L1249.2 1463.43 Q1250.22 1463.08 1251.24 1462.92 Q1252.26 1462.73 1253.28 1462.73 Q1259.07 1462.73 1262.45 1465.9 Q1265.82 1469.08 1265.82 1474.49 Q1265.82 1480.07 1262.35 1483.17 Q1258.88 1486.25 1252.56 1486.25 Q1250.39 1486.25 1248.12 1485.88 Q1245.87 1485.51 1243.46 1484.77 L1243.46 1480.07 Q1245.55 1481.2 1247.77 1481.76 Q1249.99 1482.32 1252.47 1482.32 Q1256.47 1482.32 1258.81 1480.21 Q1261.15 1478.1 1261.15 1474.49 Q1261.15 1470.88 1258.81 1468.77 Q1256.47 1466.67 1252.47 1466.67 Q1250.59 1466.67 1248.72 1467.08 Q1246.87 1467.5 1244.92 1468.38 L1244.92 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1285.04 1454.1 Q1281.43 1454.1 1279.6 1457.66 Q1277.79 1461.2 1277.79 1468.33 Q1277.79 1475.44 1279.6 1479.01 Q1281.43 1482.55 1285.04 1482.55 Q1288.67 1482.55 1290.48 1479.01 Q1292.31 1475.44 1292.31 1468.33 Q1292.31 1461.2 1290.48 1457.66 Q1288.67 1454.1 1285.04 1454.1 M1285.04 1450.39 Q1290.85 1450.39 1293.9 1455 Q1296.98 1459.58 1296.98 1468.33 Q1296.98 1477.06 1293.9 1481.67 Q1290.85 1486.25 1285.04 1486.25 Q1279.23 1486.25 1276.15 1481.67 Q1273.09 1477.06 1273.09 1468.33 Q1273.09 1459.58 1276.15 1455 Q1279.23 1450.39 1285.04 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1315.2 1454.1 Q1311.59 1454.1 1309.76 1457.66 Q1307.95 1461.2 1307.95 1468.33 Q1307.95 1475.44 1309.76 1479.01 Q1311.59 1482.55 1315.2 1482.55 Q1318.83 1482.55 1320.64 1479.01 Q1322.47 1475.44 1322.47 1468.33 Q1322.47 1461.2 1320.64 1457.66 Q1318.83 1454.1 1315.2 1454.1 M1315.2 1450.39 Q1321.01 1450.39 1324.07 1455 Q1327.14 1459.58 1327.14 1468.33 Q1327.14 1477.06 1324.07 1481.67 Q1321.01 1486.25 1315.2 1486.25 Q1309.39 1486.25 1306.31 1481.67 Q1303.26 1477.06 1303.26 1468.33 Q1303.26 1459.58 1306.31 1455 Q1309.39 1450.39 1315.2 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1747.12 1451.02 L1769.35 1451.02 L1769.35 1453.01 L1756.8 1485.58 L1751.91 1485.58 L1763.72 1454.96 L1747.12 1454.96 L1747.12 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1778.51 1451.02 L1796.87 1451.02 L1796.87 1454.96 L1782.79 1454.96 L1782.79 1463.43 Q1783.81 1463.08 1784.83 1462.92 Q1785.85 1462.73 1786.87 1462.73 Q1792.66 1462.73 1796.04 1465.9 Q1799.41 1469.08 1799.41 1474.49 Q1799.41 1480.07 1795.94 1483.17 Q1792.47 1486.25 1786.15 1486.25 Q1783.97 1486.25 1781.71 1485.88 Q1779.46 1485.51 1777.05 1484.77 L1777.05 1480.07 Q1779.14 1481.2 1781.36 1481.76 Q1783.58 1482.32 1786.06 1482.32 Q1790.06 1482.32 1792.4 1480.21 Q1794.74 1478.1 1794.74 1474.49 Q1794.74 1470.88 1792.4 1468.77 Q1790.06 1466.67 1786.06 1466.67 Q1784.18 1466.67 1782.31 1467.08 Q1780.46 1467.5 1778.51 1468.38 L1778.51 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1818.63 1454.1 Q1815.02 1454.1 1813.19 1457.66 Q1811.38 1461.2 1811.38 1468.33 Q1811.38 1475.44 1813.19 1479.01 Q1815.02 1482.55 1818.63 1482.55 Q1822.26 1482.55 1824.07 1479.01 Q1825.9 1475.44 1825.9 1468.33 Q1825.9 1461.2 1824.07 1457.66 Q1822.26 1454.1 1818.63 1454.1 M1818.63 1450.39 Q1824.44 1450.39 1827.49 1455 Q1830.57 1459.58 1830.57 1468.33 Q1830.57 1477.06 1827.49 1481.67 Q1824.44 1486.25 1818.63 1486.25 Q1812.82 1486.25 1809.74 1481.67 Q1806.68 1477.06 1806.68 1468.33 Q1806.68 1459.58 1809.74 1455 Q1812.82 1450.39 1818.63 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2236.92 1481.64 L2244.56 1481.64 L2244.56 1455.28 L2236.25 1456.95 L2236.25 1452.69 L2244.51 1451.02 L2249.19 1451.02 L2249.19 1481.64 L2256.82 1481.64 L2256.82 1485.58 L2236.92 1485.58 L2236.92 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2276.27 1454.1 Q2272.66 1454.1 2270.83 1457.66 Q2269.02 1461.2 2269.02 1468.33 Q2269.02 1475.44 2270.83 1479.01 Q2272.66 1482.55 2276.27 1482.55 Q2279.9 1482.55 2281.71 1479.01 Q2283.54 1475.44 2283.54 1468.33 Q2283.54 1461.2 2281.71 1457.66 Q2279.9 1454.1 2276.27 1454.1 M2276.27 1450.39 Q2282.08 1450.39 2285.13 1455 Q2288.21 1459.58 2288.21 1468.33 Q2288.21 1477.06 2285.13 1481.67 Q2282.08 1486.25 2276.27 1486.25 Q2270.46 1486.25 2267.38 1481.67 Q2264.32 1477.06 2264.32 1468.33 Q2264.32 1459.58 2267.38 1455 Q2270.46 1450.39 2276.27 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2306.43 1454.1 Q2302.82 1454.1 2300.99 1457.66 Q2299.19 1461.2 2299.19 1468.33 Q2299.19 1475.44 2300.99 1479.01 Q2302.82 1482.55 2306.43 1482.55 Q2310.06 1482.55 2311.87 1479.01 Q2313.7 1475.44 2313.7 1468.33 Q2313.7 1461.2 2311.87 1457.66 Q2310.06 1454.1 2306.43 1454.1 M2306.43 1450.39 Q2312.24 1450.39 2315.3 1455 Q2318.37 1459.58 2318.37 1468.33 Q2318.37 1477.06 2315.3 1481.67 Q2312.24 1486.25 2306.43 1486.25 Q2300.62 1486.25 2297.54 1481.67 Q2294.49 1477.06 2294.49 1468.33 Q2294.49 1459.58 2297.54 1455 Q2300.62 1450.39 2306.43 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2336.59 1454.1 Q2332.98 1454.1 2331.15 1457.66 Q2329.35 1461.2 2329.35 1468.33 Q2329.35 1475.44 2331.15 1479.01 Q2332.98 1482.55 2336.59 1482.55 Q2340.23 1482.55 2342.03 1479.01 Q2343.86 1475.44 2343.86 1468.33 Q2343.86 1461.2 2342.03 1457.66 Q2340.23 1454.1 2336.59 1454.1 M2336.59 1450.39 Q2342.4 1450.39 2345.46 1455 Q2348.54 1459.58 2348.54 1468.33 Q2348.54 1477.06 2345.46 1481.67 Q2342.4 1486.25 2336.59 1486.25 Q2330.78 1486.25 2327.7 1481.67 Q2324.65 1477.06 2324.65 1468.33 Q2324.65 1459.58 2327.7 1455 Q2330.78 1450.39 2336.59 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1043.3 1546.53 L1043.3 1568.04 L1037.44 1568.04 L1037.44 1546.72 Q1037.44 1541.66 1035.47 1539.14 Q1033.5 1536.63 1029.55 1536.63 Q1024.81 1536.63 1022.07 1539.65 Q1019.33 1542.68 1019.33 1547.9 L1019.33 1568.04 L1013.45 1568.04 L1013.45 1532.4 L1019.33 1532.4 L1019.33 1537.93 Q1021.43 1534.72 1024.27 1533.13 Q1027.13 1531.54 1030.86 1531.54 Q1037 1531.54 1040.15 1535.36 Q1043.3 1539.14 1043.3 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1054.38 1553.98 L1054.38 1532.4 L1060.23 1532.4 L1060.23 1553.75 Q1060.23 1558.81 1062.21 1561.36 Q1064.18 1563.87 1068.13 1563.87 Q1072.87 1563.87 1075.61 1560.85 Q1078.38 1557.83 1078.38 1552.61 L1078.38 1532.4 L1084.23 1532.4 L1084.23 1568.04 L1078.38 1568.04 L1078.38 1562.57 Q1076.24 1565.82 1073.41 1567.41 Q1070.61 1568.97 1066.89 1568.97 Q1060.74 1568.97 1057.56 1565.15 Q1054.38 1561.33 1054.38 1553.98 M1069.11 1531.54 L1069.11 1531.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1124.05 1539.24 Q1126.25 1535.29 1129.3 1533.41 Q1132.36 1531.54 1136.49 1531.54 Q1142.06 1531.54 1145.09 1535.45 Q1148.11 1539.33 1148.11 1546.53 L1148.11 1568.04 L1142.22 1568.04 L1142.22 1546.72 Q1142.22 1541.59 1140.41 1539.11 Q1138.59 1536.63 1134.87 1536.63 Q1130.32 1536.63 1127.68 1539.65 Q1125.04 1542.68 1125.04 1547.9 L1125.04 1568.04 L1119.15 1568.04 L1119.15 1546.72 Q1119.15 1541.56 1117.33 1539.11 Q1115.52 1536.63 1111.73 1536.63 Q1107.24 1536.63 1104.6 1539.68 Q1101.96 1542.71 1101.96 1547.9 L1101.96 1568.04 L1096.07 1568.04 L1096.07 1532.4 L1101.96 1532.4 L1101.96 1537.93 Q1103.97 1534.66 1106.77 1533.1 Q1109.57 1531.54 1113.42 1531.54 Q1117.3 1531.54 1120.01 1533.51 Q1122.74 1535.48 1124.05 1539.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1185.38 1550.25 Q1185.38 1543.79 1182.71 1540.13 Q1180.07 1536.44 1175.42 1536.44 Q1170.77 1536.44 1168.1 1540.13 Q1165.46 1543.79 1165.46 1550.25 Q1165.46 1556.71 1168.1 1560.4 Q1170.77 1564.07 1175.42 1564.07 Q1180.07 1564.07 1182.71 1560.4 Q1185.38 1556.71 1185.38 1550.25 M1165.46 1537.81 Q1167.3 1534.62 1170.11 1533.1 Q1172.94 1531.54 1176.85 1531.54 Q1183.35 1531.54 1187.39 1536.69 Q1191.46 1541.85 1191.46 1550.25 Q1191.46 1558.65 1187.39 1563.81 Q1183.35 1568.97 1176.85 1568.97 Q1172.94 1568.97 1170.11 1567.44 Q1167.3 1565.88 1165.46 1562.7 L1165.46 1568.04 L1159.57 1568.04 L1159.57 1518.52 L1165.46 1518.52 L1165.46 1537.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1231.66 1548.76 L1231.66 1551.62 L1204.73 1551.62 Q1205.12 1557.67 1208.36 1560.85 Q1211.64 1564 1217.47 1564 Q1220.84 1564 1223.99 1563.17 Q1227.17 1562.35 1230.29 1560.69 L1230.29 1566.23 Q1227.14 1567.57 1223.83 1568.27 Q1220.52 1568.97 1217.12 1568.97 Q1208.59 1568.97 1203.59 1564 Q1198.62 1559.04 1198.62 1550.57 Q1198.62 1541.82 1203.33 1536.69 Q1208.08 1531.54 1216.1 1531.54 Q1223.29 1531.54 1227.46 1536.18 Q1231.66 1540.8 1231.66 1548.76 M1225.81 1547.04 Q1225.74 1542.23 1223.1 1539.37 Q1220.49 1536.5 1216.16 1536.5 Q1211.26 1536.5 1208.3 1539.27 Q1205.37 1542.04 1204.93 1547.07 L1225.81 1547.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1261.93 1537.87 Q1260.94 1537.3 1259.77 1537.04 Q1258.62 1536.76 1257.22 1536.76 Q1252.25 1536.76 1249.58 1540 Q1246.94 1543.22 1246.94 1549.27 L1246.94 1568.04 L1241.05 1568.04 L1241.05 1532.4 L1246.94 1532.4 L1246.94 1537.93 Q1248.79 1534.69 1251.75 1533.13 Q1254.71 1531.54 1258.94 1531.54 Q1259.54 1531.54 1260.28 1531.63 Q1261.01 1531.7 1261.9 1531.85 L1261.93 1537.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1302.61 1536.5 Q1297.9 1536.5 1295.16 1540.19 Q1292.42 1543.85 1292.42 1550.25 Q1292.42 1556.65 1295.13 1560.34 Q1297.86 1564 1302.61 1564 Q1307.29 1564 1310.02 1560.31 Q1312.76 1556.62 1312.76 1550.25 Q1312.76 1543.92 1310.02 1540.23 Q1307.29 1536.5 1302.61 1536.5 M1302.61 1531.54 Q1310.25 1531.54 1314.61 1536.5 Q1318.97 1541.47 1318.97 1550.25 Q1318.97 1559 1314.61 1564 Q1310.25 1568.97 1302.61 1568.97 Q1294.94 1568.97 1290.58 1564 Q1286.25 1559 1286.25 1550.25 Q1286.25 1541.47 1290.58 1536.5 Q1294.94 1531.54 1302.61 1531.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1346.72 1518.52 L1346.72 1523.39 L1341.12 1523.39 Q1337.97 1523.39 1336.73 1524.66 Q1335.52 1525.93 1335.52 1529.24 L1335.52 1532.4 L1345.16 1532.4 L1345.16 1536.95 L1335.52 1536.95 L1335.52 1568.04 L1329.63 1568.04 L1329.63 1536.95 L1324.03 1536.95 L1324.03 1532.4 L1329.63 1532.4 L1329.63 1529.91 Q1329.63 1523.96 1332.4 1521.26 Q1335.17 1518.52 1341.18 1518.52 L1346.72 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1402.84 1548.76 L1402.84 1551.62 L1375.91 1551.62 Q1376.29 1557.67 1379.54 1560.85 Q1382.82 1564 1388.64 1564 Q1392.01 1564 1395.16 1563.17 Q1398.35 1562.35 1401.47 1560.69 L1401.47 1566.23 Q1398.32 1567.57 1395.01 1568.27 Q1391.7 1568.97 1388.29 1568.97 Q1379.76 1568.97 1374.76 1564 Q1369.8 1559.04 1369.8 1550.57 Q1369.8 1541.82 1374.51 1536.69 Q1379.25 1531.54 1387.27 1531.54 Q1394.46 1531.54 1398.63 1536.18 Q1402.84 1540.8 1402.84 1548.76 M1396.98 1547.04 Q1396.92 1542.23 1394.27 1539.37 Q1391.66 1536.5 1387.33 1536.5 Q1382.43 1536.5 1379.47 1539.27 Q1376.54 1542.04 1376.1 1547.07 L1396.98 1547.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1418.11 1562.7 L1418.11 1581.6 L1412.22 1581.6 L1412.22 1532.4 L1418.11 1532.4 L1418.11 1537.81 Q1419.96 1534.62 1422.76 1533.1 Q1425.59 1531.54 1429.51 1531.54 Q1436 1531.54 1440.04 1536.69 Q1444.12 1541.85 1444.12 1550.25 Q1444.12 1558.65 1440.04 1563.81 Q1436 1568.97 1429.51 1568.97 Q1425.59 1568.97 1422.76 1567.44 Q1419.96 1565.88 1418.11 1562.7 M1438.04 1550.25 Q1438.04 1543.79 1435.36 1540.13 Q1432.72 1536.44 1428.08 1536.44 Q1423.43 1536.44 1420.75 1540.13 Q1418.11 1543.79 1418.11 1550.25 Q1418.11 1556.71 1420.75 1560.4 Q1423.43 1564.07 1428.08 1564.07 Q1432.72 1564.07 1435.36 1560.4 Q1438.04 1556.71 1438.04 1550.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1467.64 1536.5 Q1462.93 1536.5 1460.19 1540.19 Q1457.45 1543.85 1457.45 1550.25 Q1457.45 1556.65 1460.16 1560.34 Q1462.9 1564 1467.64 1564 Q1472.32 1564 1475.05 1560.31 Q1477.79 1556.62 1477.79 1550.25 Q1477.79 1543.92 1475.05 1540.23 Q1472.32 1536.5 1467.64 1536.5 M1467.64 1531.54 Q1475.28 1531.54 1479.64 1536.5 Q1484 1541.47 1484 1550.25 Q1484 1559 1479.64 1564 Q1475.28 1568.97 1467.64 1568.97 Q1459.97 1568.97 1455.61 1564 Q1451.28 1559 1451.28 1550.25 Q1451.28 1541.47 1455.61 1536.5 Q1459.97 1531.54 1467.64 1531.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1519.36 1533.76 L1519.36 1539.24 Q1516.88 1537.87 1514.36 1537.2 Q1511.88 1536.5 1509.33 1536.5 Q1503.64 1536.5 1500.49 1540.13 Q1497.33 1543.73 1497.33 1550.25 Q1497.33 1556.78 1500.49 1560.4 Q1503.64 1564 1509.33 1564 Q1511.88 1564 1514.36 1563.33 Q1516.88 1562.63 1519.36 1561.26 L1519.36 1566.68 Q1516.91 1567.82 1514.27 1568.39 Q1511.66 1568.97 1508.7 1568.97 Q1500.64 1568.97 1495.9 1563.91 Q1491.16 1558.85 1491.16 1550.25 Q1491.16 1541.53 1495.93 1536.53 Q1500.74 1531.54 1509.08 1531.54 Q1511.78 1531.54 1514.36 1532.11 Q1516.94 1532.65 1519.36 1533.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1559.18 1546.53 L1559.18 1568.04 L1553.32 1568.04 L1553.32 1546.72 Q1553.32 1541.66 1551.35 1539.14 Q1549.37 1536.63 1545.43 1536.63 Q1540.68 1536.63 1537.95 1539.65 Q1535.21 1542.68 1535.21 1547.9 L1535.21 1568.04 L1529.32 1568.04 L1529.32 1518.52 L1535.21 1518.52 L1535.21 1537.93 Q1537.31 1534.72 1540.14 1533.13 Q1543.01 1531.54 1546.73 1531.54 Q1552.88 1531.54 1556.03 1535.36 Q1559.18 1539.14 1559.18 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,1417.27 2352.76,1417.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,1101.09 2352.76,1101.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,784.914 2352.76,784.914 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,468.735 2352.76,468.735 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  219.866,152.556 2352.76,152.556 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1423.18 219.866,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1417.27 238.764,1417.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,1101.09 238.764,1101.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,784.914 238.764,784.914 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,468.735 238.764,468.735 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  219.866,152.556 238.764,152.556 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip550)\" d=\"M126.691 1403.07 Q123.08 1403.07 121.251 1406.64 Q119.445 1410.18 119.445 1417.31 Q119.445 1424.41 121.251 1427.98 Q123.08 1431.52 126.691 1431.52 Q130.325 1431.52 132.13 1427.98 Q133.959 1424.41 133.959 1417.31 Q133.959 1410.18 132.13 1406.64 Q130.325 1403.07 126.691 1403.07 M126.691 1399.37 Q132.501 1399.37 135.556 1403.97 Q138.635 1408.56 138.635 1417.31 Q138.635 1426.03 135.556 1430.64 Q132.501 1435.22 126.691 1435.22 Q120.88 1435.22 117.802 1430.64 Q114.746 1426.03 114.746 1417.31 Q114.746 1408.56 117.802 1403.97 Q120.88 1399.37 126.691 1399.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M146.853 1428.67 L151.737 1428.67 L151.737 1434.55 L146.853 1434.55 L146.853 1428.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M171.922 1403.07 Q168.311 1403.07 166.482 1406.64 Q164.677 1410.18 164.677 1417.31 Q164.677 1424.41 166.482 1427.98 Q168.311 1431.52 171.922 1431.52 Q175.556 1431.52 177.362 1427.98 Q179.19 1424.41 179.19 1417.31 Q179.19 1410.18 177.362 1406.64 Q175.556 1403.07 171.922 1403.07 M171.922 1399.37 Q177.732 1399.37 180.788 1403.97 Q183.866 1408.56 183.866 1417.31 Q183.866 1426.03 180.788 1430.64 Q177.732 1435.22 171.922 1435.22 Q166.112 1435.22 163.033 1430.64 Q159.978 1426.03 159.978 1417.31 Q159.978 1408.56 163.033 1403.97 Q166.112 1399.37 171.922 1399.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M128.288 1086.89 Q124.677 1086.89 122.848 1090.46 Q121.043 1094 121.043 1101.13 Q121.043 1108.24 122.848 1111.8 Q124.677 1115.34 128.288 1115.34 Q131.922 1115.34 133.728 1111.8 Q135.556 1108.24 135.556 1101.13 Q135.556 1094 133.728 1090.46 Q131.922 1086.89 128.288 1086.89 M128.288 1083.19 Q134.098 1083.19 137.154 1087.8 Q140.232 1092.38 140.232 1101.13 Q140.232 1109.86 137.154 1114.46 Q134.098 1119.05 128.288 1119.05 Q122.478 1119.05 119.399 1114.46 Q116.343 1109.86 116.343 1101.13 Q116.343 1092.38 119.399 1087.8 Q122.478 1083.19 128.288 1083.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M148.45 1112.49 L153.334 1112.49 L153.334 1118.37 L148.45 1118.37 L148.45 1112.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M167.547 1114.44 L183.866 1114.44 L183.866 1118.37 L161.922 1118.37 L161.922 1114.44 Q164.584 1111.68 169.167 1107.05 Q173.774 1102.4 174.954 1101.06 Q177.2 1098.54 178.079 1096.8 Q178.982 1095.04 178.982 1093.35 Q178.982 1090.6 177.038 1088.86 Q175.116 1087.12 172.014 1087.12 Q169.815 1087.12 167.362 1087.89 Q164.931 1088.65 162.153 1090.2 L162.153 1085.48 Q164.977 1084.35 167.431 1083.77 Q169.885 1083.19 171.922 1083.19 Q177.292 1083.19 180.487 1085.87 Q183.681 1088.56 183.681 1093.05 Q183.681 1095.18 182.871 1097.1 Q182.084 1099 179.977 1101.59 Q179.399 1102.26 176.297 1105.48 Q173.195 1108.67 167.547 1114.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M126.205 770.713 Q122.593 770.713 120.765 774.278 Q118.959 777.82 118.959 784.949 Q118.959 792.056 120.765 795.62 Q122.593 799.162 126.205 799.162 Q129.839 799.162 131.644 795.62 Q133.473 792.056 133.473 784.949 Q133.473 777.82 131.644 774.278 Q129.839 770.713 126.205 770.713 M126.205 767.009 Q132.015 767.009 135.07 771.616 Q138.149 776.199 138.149 784.949 Q138.149 793.676 135.07 798.282 Q132.015 802.866 126.205 802.866 Q120.394 802.866 117.316 798.282 Q114.26 793.676 114.26 784.949 Q114.26 776.199 117.316 771.616 Q120.394 767.009 126.205 767.009 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M146.366 796.315 L151.251 796.315 L151.251 802.194 L146.366 802.194 L146.366 796.315 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M174.283 771.709 L162.477 790.157 L174.283 790.157 L174.283 771.709 M173.056 767.634 L178.936 767.634 L178.936 790.157 L183.866 790.157 L183.866 794.046 L178.936 794.046 L178.936 802.194 L174.283 802.194 L174.283 794.046 L158.681 794.046 L158.681 789.532 L173.056 767.634 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M126.529 454.534 Q122.918 454.534 121.089 458.099 Q119.283 461.64 119.283 468.77 Q119.283 475.876 121.089 479.441 Q122.918 482.983 126.529 482.983 Q130.163 482.983 131.968 479.441 Q133.797 475.876 133.797 468.77 Q133.797 461.64 131.968 458.099 Q130.163 454.534 126.529 454.534 M126.529 450.83 Q132.339 450.83 135.394 455.437 Q138.473 460.02 138.473 468.77 Q138.473 477.497 135.394 482.103 Q132.339 486.686 126.529 486.686 Q120.718 486.686 117.64 482.103 Q114.584 477.497 114.584 468.77 Q114.584 460.02 117.64 455.437 Q120.718 450.83 126.529 450.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M146.691 480.135 L151.575 480.135 L151.575 486.015 L146.691 486.015 L146.691 480.135 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M172.339 466.872 Q169.19 466.872 167.339 469.024 Q165.51 471.177 165.51 474.927 Q165.51 478.654 167.339 480.83 Q169.19 482.983 172.339 482.983 Q175.487 482.983 177.315 480.83 Q179.167 478.654 179.167 474.927 Q179.167 471.177 177.315 469.024 Q175.487 466.872 172.339 466.872 M181.621 452.219 L181.621 456.478 Q179.862 455.645 178.056 455.205 Q176.274 454.765 174.514 454.765 Q169.885 454.765 167.431 457.89 Q165.001 461.015 164.653 467.335 Q166.019 465.321 168.079 464.256 Q170.139 463.168 172.616 463.168 Q177.825 463.168 180.834 466.339 Q183.866 469.487 183.866 474.927 Q183.866 480.251 180.718 483.469 Q177.57 486.686 172.339 486.686 Q166.343 486.686 163.172 482.103 Q160.001 477.497 160.001 468.77 Q160.001 460.575 163.89 455.714 Q167.778 450.83 174.329 450.83 Q176.089 450.83 177.871 451.177 Q179.676 451.525 181.621 452.219 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M126.783 138.354 Q123.172 138.354 121.343 141.919 Q119.538 145.461 119.538 152.59 Q119.538 159.697 121.343 163.262 Q123.172 166.803 126.783 166.803 Q130.417 166.803 132.223 163.262 Q134.052 159.697 134.052 152.59 Q134.052 145.461 132.223 141.919 Q130.417 138.354 126.783 138.354 M126.783 134.651 Q132.593 134.651 135.649 139.257 Q138.728 143.84 138.728 152.59 Q138.728 161.317 135.649 165.924 Q132.593 170.507 126.783 170.507 Q120.973 170.507 117.894 165.924 Q114.839 161.317 114.839 152.59 Q114.839 143.84 117.894 139.257 Q120.973 134.651 126.783 134.651 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M146.945 163.956 L151.829 163.956 L151.829 169.836 L146.945 169.836 L146.945 163.956 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M172.014 153.424 Q168.681 153.424 166.76 155.206 Q164.862 156.989 164.862 160.113 Q164.862 163.238 166.76 165.021 Q168.681 166.803 172.014 166.803 Q175.348 166.803 177.269 165.021 Q179.19 163.215 179.19 160.113 Q179.19 156.989 177.269 155.206 Q175.371 153.424 172.014 153.424 M167.339 151.433 Q164.329 150.692 162.64 148.632 Q160.973 146.572 160.973 143.609 Q160.973 139.465 163.913 137.058 Q166.876 134.651 172.014 134.651 Q177.176 134.651 180.116 137.058 Q183.056 139.465 183.056 143.609 Q183.056 146.572 181.366 148.632 Q179.7 150.692 176.714 151.433 Q180.093 152.22 181.968 154.512 Q183.866 156.803 183.866 160.113 Q183.866 165.137 180.788 167.822 Q177.732 170.507 172.014 170.507 Q166.297 170.507 163.218 167.822 Q160.163 165.137 160.163 160.113 Q160.163 156.803 162.061 154.512 Q163.959 152.22 167.339 151.433 M165.626 144.049 Q165.626 146.734 167.292 148.239 Q168.982 149.743 172.014 149.743 Q175.024 149.743 176.714 148.239 Q178.426 146.734 178.426 144.049 Q178.426 141.364 176.714 139.859 Q175.024 138.354 172.014 138.354 Q168.982 138.354 167.292 139.859 Q165.626 141.364 165.626 144.049 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M14.479 1058.84 L14.479 1052.99 L64.0042 1052.99 L64.0042 1058.84 L14.479 1058.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M32.4621 1026.92 Q32.4621 1031.63 36.1542 1034.37 Q39.8145 1037.1 46.212 1037.1 Q52.6095 1037.1 56.3017 1034.4 Q59.9619 1031.66 59.9619 1026.92 Q59.9619 1022.24 56.2698 1019.5 Q52.5777 1016.77 46.212 1016.77 Q39.8781 1016.77 36.186 1019.5 Q32.4621 1022.24 32.4621 1026.92 M27.4968 1026.92 Q27.4968 1019.28 32.4621 1014.92 Q37.4273 1010.56 46.212 1010.56 Q54.9649 1010.56 59.9619 1014.92 Q64.9272 1019.28 64.9272 1026.92 Q64.9272 1034.59 59.9619 1038.95 Q54.9649 1043.28 46.212 1043.28 Q37.4273 1043.28 32.4621 1038.95 Q27.4968 1034.59 27.4968 1026.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M45.7664 977.395 Q39.4007 977.395 35.8996 980.036 Q32.3984 982.646 32.3984 987.389 Q32.3984 992.099 35.8996 994.741 Q39.4007 997.351 45.7664 997.351 Q52.1003 997.351 55.6014 994.741 Q59.1026 992.099 59.1026 987.389 Q59.1026 982.646 55.6014 980.036 Q52.1003 977.395 45.7664 977.395 M59.58 971.538 Q68.683 971.538 73.1071 975.58 Q77.5631 979.623 77.5631 987.962 Q77.5631 991.049 77.0857 993.786 Q76.6401 996.524 75.6852 999.102 L69.9879 999.102 Q71.3884 996.524 72.0568 994.009 Q72.7252 991.495 72.7252 988.885 Q72.7252 983.124 69.7015 980.259 Q66.7096 977.395 60.6303 977.395 L57.7339 977.395 Q60.885 979.209 62.4446 982.042 Q64.0042 984.874 64.0042 988.821 Q64.0042 995.378 59.0071 999.388 Q54.01 1003.4 45.7664 1003.4 Q37.491 1003.4 32.4939 999.388 Q27.4968 995.378 27.4968 988.821 Q27.4968 984.874 29.0564 982.042 Q30.616 979.209 33.7671 977.395 L28.3562 977.395 L28.3562 971.538 L59.58 971.538 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M58.5933 957.534 L58.5933 947.03 L22.3406 947.03 L24.6323 958.457 L18.7758 958.457 L16.4842 947.094 L16.4842 940.664 L58.5933 940.664 L58.5933 930.161 L64.0042 930.161 L64.0042 957.534 L58.5933 957.534 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M20.7174 903.425 Q20.7174 908.39 25.6189 910.905 Q30.4887 913.387 40.2919 913.387 Q50.0633 913.387 54.9649 910.905 Q59.8346 908.39 59.8346 903.425 Q59.8346 898.428 54.9649 895.945 Q50.0633 893.431 40.2919 893.431 Q30.4887 893.431 25.6189 895.945 Q20.7174 898.428 20.7174 903.425 M15.6248 903.425 Q15.6248 895.436 21.9587 891.235 Q28.2607 887.002 40.2919 887.002 Q52.2913 887.002 58.6251 891.235 Q64.9272 895.436 64.9272 903.425 Q64.9272 911.414 58.6251 915.647 Q52.2913 919.849 40.2919 919.849 Q28.2607 919.849 21.9587 915.647 Q15.6248 911.414 15.6248 903.425 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M14.5426 862.462 Q21.8632 866.727 29.0246 868.796 Q36.186 870.865 43.5384 870.865 Q50.8908 870.865 58.1159 868.796 Q65.3091 866.695 72.5979 862.462 L72.5979 867.554 Q65.1182 872.329 57.8931 874.716 Q50.668 877.071 43.5384 877.071 Q36.4406 877.071 29.2474 874.716 Q22.0542 872.36 14.5426 867.554 L14.5426 862.462 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M35.1993 823.345 Q31.2526 821.148 29.3747 818.093 Q27.4968 815.037 27.4968 810.9 Q27.4968 805.33 31.4117 802.306 Q35.2948 799.282 42.4881 799.282 L64.0042 799.282 L64.0042 805.17 L42.679 805.17 Q37.5546 805.17 35.072 806.985 Q32.5894 808.799 32.5894 812.523 Q32.5894 817.074 35.6131 819.716 Q38.6368 822.358 43.8567 822.358 L64.0042 822.358 L64.0042 828.246 L42.679 828.246 Q37.5228 828.246 35.072 830.06 Q32.5894 831.875 32.5894 835.662 Q32.5894 840.15 35.6449 842.792 Q38.6686 845.434 43.8567 845.434 L64.0042 845.434 L64.0042 851.322 L28.3562 851.322 L28.3562 845.434 L33.8944 845.434 Q30.616 843.428 29.0564 840.627 Q27.4968 837.827 27.4968 833.975 Q27.4968 830.092 29.4702 827.387 Q31.4436 824.649 35.1993 823.345 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M32.4621 773.787 Q32.4621 778.498 36.1542 781.235 Q39.8145 783.973 46.212 783.973 Q52.6095 783.973 56.3017 781.267 Q59.9619 778.53 59.9619 773.787 Q59.9619 769.109 56.2698 766.371 Q52.5777 763.634 46.212 763.634 Q39.8781 763.634 36.186 766.371 Q32.4621 769.109 32.4621 773.787 M27.4968 773.787 Q27.4968 766.149 32.4621 761.788 Q37.4273 757.428 46.212 757.428 Q54.9649 757.428 59.9619 761.788 Q64.9272 766.149 64.9272 773.787 Q64.9272 781.458 59.9619 785.819 Q54.9649 790.147 46.212 790.147 Q37.4273 790.147 32.4621 785.819 Q27.4968 781.458 27.4968 773.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M33.7671 724.262 L14.479 724.262 L14.479 718.406 L64.0042 718.406 L64.0042 724.262 L58.657 724.262 Q61.8398 726.108 63.3994 728.941 Q64.9272 731.742 64.9272 735.689 Q64.9272 742.15 59.771 746.224 Q54.6147 750.266 46.212 750.266 Q37.8093 750.266 32.6531 746.224 Q27.4968 742.15 27.4968 735.689 Q27.4968 731.742 29.0564 728.941 Q30.5842 726.108 33.7671 724.262 M46.212 744.219 Q52.6732 744.219 56.3653 741.577 Q60.0256 738.903 60.0256 734.256 Q60.0256 729.609 56.3653 726.936 Q52.6732 724.262 46.212 724.262 Q39.7508 724.262 36.0905 726.936 Q32.3984 729.609 32.3984 734.256 Q32.3984 738.903 36.0905 741.577 Q39.7508 744.219 46.212 744.219 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M44.7161 675.851 L47.5806 675.851 L47.5806 702.778 Q53.6281 702.396 56.8109 699.15 Q59.9619 695.871 59.9619 690.047 Q59.9619 686.673 59.1344 683.522 Q58.3069 680.339 56.6518 677.22 L62.1899 677.22 Q63.5267 680.371 64.227 683.681 Q64.9272 686.991 64.9272 690.397 Q64.9272 698.927 59.9619 703.924 Q54.9967 708.889 46.5303 708.889 Q37.7774 708.889 32.6531 704.178 Q27.4968 699.436 27.4968 691.415 Q27.4968 684.222 32.1438 680.052 Q36.7589 675.851 44.7161 675.851 M42.9973 681.708 Q38.1912 681.771 35.3266 684.413 Q32.4621 687.023 32.4621 691.352 Q32.4621 696.253 35.2312 699.213 Q38.0002 702.141 43.0292 702.587 L42.9973 681.708 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M14.479 666.239 L14.479 660.382 L64.0042 660.382 L64.0042 666.239 L14.479 666.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M14.479 627.408 L14.479 621.552 L64.0042 621.552 L64.0042 627.408 L14.479 627.408 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M32.4621 595.484 Q32.4621 600.195 36.1542 602.932 Q39.8145 605.669 46.212 605.669 Q52.6095 605.669 56.3017 602.964 Q59.9619 600.227 59.9619 595.484 Q59.9619 590.805 56.2698 588.068 Q52.5777 585.331 46.212 585.331 Q39.8781 585.331 36.186 588.068 Q32.4621 590.805 32.4621 595.484 M27.4968 595.484 Q27.4968 587.845 32.4621 583.485 Q37.4273 579.124 46.212 579.124 Q54.9649 579.124 59.9619 583.485 Q64.9272 587.845 64.9272 595.484 Q64.9272 603.155 59.9619 607.515 Q54.9649 611.844 46.212 611.844 Q37.4273 611.844 32.4621 607.515 Q27.4968 603.155 27.4968 595.484 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M29.4065 546.691 L34.9447 546.691 Q33.6716 549.174 33.035 551.847 Q32.3984 554.521 32.3984 557.385 Q32.3984 561.746 33.7352 563.942 Q35.072 566.106 37.7456 566.106 Q39.7826 566.106 40.9603 564.547 Q42.1061 562.987 43.1565 558.276 L43.6021 556.271 Q44.9389 550.033 47.3897 547.423 Q49.8086 544.781 54.1691 544.781 Q59.1344 544.781 62.0308 548.728 Q64.9272 552.643 64.9272 559.518 Q64.9272 562.382 64.3543 565.502 Q63.8132 568.589 62.6992 572.026 L56.6518 572.026 Q58.3387 568.78 59.198 565.629 Q60.0256 562.478 60.0256 559.39 Q60.0256 555.253 58.6251 553.025 Q57.1929 550.797 54.6147 550.797 Q52.2276 550.797 50.9545 552.42 Q49.6813 554.011 48.5037 559.454 L48.0262 561.491 Q46.8804 566.934 44.5251 569.353 Q42.138 571.772 38.0002 571.772 Q32.9713 571.772 30.2341 568.207 Q27.4968 564.642 27.4968 558.086 Q27.4968 554.839 27.9743 551.974 Q28.4517 549.11 29.4065 546.691 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M29.4065 512.73 L34.9447 512.73 Q33.6716 515.212 33.035 517.886 Q32.3984 520.56 32.3984 523.424 Q32.3984 527.785 33.7352 529.981 Q35.072 532.145 37.7456 532.145 Q39.7826 532.145 40.9603 530.586 Q42.1061 529.026 43.1565 524.315 L43.6021 522.31 Q44.9389 516.072 47.3897 513.462 Q49.8086 510.82 54.1691 510.82 Q59.1344 510.82 62.0308 514.767 Q64.9272 518.682 64.9272 525.557 Q64.9272 528.421 64.3543 531.541 Q63.8132 534.628 62.6992 538.065 L56.6518 538.065 Q58.3387 534.819 59.198 531.668 Q60.0256 528.517 60.0256 525.429 Q60.0256 521.292 58.6251 519.064 Q57.1929 516.836 54.6147 516.836 Q52.2276 516.836 50.9545 518.459 Q49.6813 520.05 48.5037 525.493 L48.0262 527.53 Q46.8804 532.973 44.5251 535.392 Q42.138 537.811 38.0002 537.811 Q32.9713 537.811 30.2341 534.246 Q27.4968 530.681 27.4968 524.124 Q27.4968 520.878 27.9743 518.013 Q28.4517 515.149 29.4065 512.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M14.5426 502.417 L14.5426 497.325 Q22.0542 492.551 29.2474 490.195 Q36.4406 487.808 43.5384 487.808 Q50.668 487.808 57.8931 490.195 Q65.1182 492.551 72.5979 497.325 L72.5979 502.417 Q65.3091 498.184 58.1159 496.115 Q50.8908 494.015 43.5384 494.015 Q36.186 494.015 29.0246 496.115 Q21.8632 498.184 14.5426 502.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1201.42 12.096 L1209.61 12.096 L1209.61 65.6895 L1239.06 65.6895 L1239.06 72.576 L1201.42 72.576 L1201.42 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1263.44 32.4315 Q1257.45 32.4315 1253.96 37.1306 Q1250.48 41.7891 1250.48 49.9314 Q1250.48 58.0738 1253.92 62.7728 Q1257.41 67.4314 1263.44 67.4314 Q1269.4 67.4314 1272.88 62.7323 Q1276.37 58.0333 1276.37 49.9314 Q1276.37 41.8701 1272.88 37.1711 Q1269.4 32.4315 1263.44 32.4315 M1263.44 26.1121 Q1273.17 26.1121 1278.72 32.4315 Q1284.27 38.7509 1284.27 49.9314 Q1284.27 61.0714 1278.72 67.4314 Q1273.17 73.7508 1263.44 73.7508 Q1253.68 73.7508 1248.13 67.4314 Q1242.62 61.0714 1242.62 49.9314 Q1242.62 38.7509 1248.13 32.4315 Q1253.68 26.1121 1263.44 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1325.54 28.5427 L1325.54 35.5912 Q1322.38 33.9709 1318.98 33.1607 Q1315.58 32.3505 1311.93 32.3505 Q1306.38 32.3505 1303.59 34.0519 Q1300.83 35.7533 1300.83 39.156 Q1300.83 41.7486 1302.82 43.2475 Q1304.8 44.7058 1310.8 46.0426 L1313.35 46.6097 Q1321.29 48.3111 1324.61 51.4303 Q1327.97 54.509 1327.97 60.0587 Q1327.97 66.3781 1322.95 70.0644 Q1317.97 73.7508 1309.22 73.7508 Q1305.57 73.7508 1301.6 73.0216 Q1297.67 72.3329 1293.3 70.9151 L1293.3 63.2184 Q1297.43 65.3654 1301.44 66.4591 Q1305.45 67.5124 1309.38 67.5124 Q1314.65 67.5124 1317.48 65.73 Q1320.32 63.9071 1320.32 60.6258 Q1320.32 57.5877 1318.25 55.9673 Q1316.23 54.3469 1309.3 52.8481 L1306.71 52.2405 Q1299.78 50.7821 1296.7 47.7845 Q1293.62 44.7463 1293.62 39.4801 Q1293.62 33.0797 1298.16 29.5959 Q1302.7 26.1121 1311.04 26.1121 Q1315.17 26.1121 1318.82 26.7198 Q1322.47 27.3274 1325.54 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1368.77 28.5427 L1368.77 35.5912 Q1365.61 33.9709 1362.2 33.1607 Q1358.8 32.3505 1355.16 32.3505 Q1349.61 32.3505 1346.81 34.0519 Q1344.06 35.7533 1344.06 39.156 Q1344.06 41.7486 1346.04 43.2475 Q1348.03 44.7058 1354.02 46.0426 L1356.57 46.6097 Q1364.51 48.3111 1367.84 51.4303 Q1371.2 54.509 1371.2 60.0587 Q1371.2 66.3781 1366.17 70.0644 Q1361.19 73.7508 1352.44 73.7508 Q1348.8 73.7508 1344.83 73.0216 Q1340.9 72.3329 1336.52 70.9151 L1336.52 63.2184 Q1340.65 65.3654 1344.66 66.4591 Q1348.67 67.5124 1352.6 67.5124 Q1357.87 67.5124 1360.71 65.73 Q1363.54 63.9071 1363.54 60.6258 Q1363.54 57.5877 1361.48 55.9673 Q1359.45 54.3469 1352.52 52.8481 L1349.93 52.2405 Q1343 50.7821 1339.92 47.7845 Q1336.85 44.7463 1336.85 39.4801 Q1336.85 33.0797 1341.38 29.5959 Q1345.92 26.1121 1354.27 26.1121 Q1358.4 26.1121 1362.04 26.7198 Q1365.69 27.3274 1368.77 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip552)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,187.692 282.245,196.266 284.259,204.69 286.274,212.934 288.288,221.11 290.302,229.092 292.316,236.995 294.33,244.817 296.344,252.401 298.359,259.892 \n",
       "  300.373,267.388 302.387,274.746 304.401,282 306.415,289.185 308.43,296.312 310.444,303.297 312.458,310.258 314.472,317.166 316.486,323.962 318.5,330.671 \n",
       "  320.515,337.347 322.529,343.987 324.543,350.547 326.557,357.038 328.571,363.444 330.585,369.865 332.6,376.196 334.614,382.464 336.628,388.682 338.642,394.818 \n",
       "  340.656,400.915 342.67,406.91 344.685,412.862 346.699,418.705 348.713,424.502 350.727,430.331 352.741,436.03 354.756,441.627 356.77,447.237 358.784,452.775 \n",
       "  360.798,458.29 362.812,463.764 364.826,469.092 366.841,474.376 368.855,479.664 370.869,484.941 372.883,490.099 374.897,495.224 376.911,500.316 378.926,505.245 \n",
       "  380.94,510.192 382.954,515.064 384.968,519.915 386.982,524.669 388.996,529.419 391.011,534.107 393.025,538.751 395.039,543.408 397.053,547.961 399.067,552.505 \n",
       "  401.082,556.981 403.096,561.487 405.11,565.923 407.124,570.328 409.138,574.701 411.152,579.062 413.167,583.349 415.181,587.601 417.195,591.776 419.209,595.998 \n",
       "  421.223,600.183 423.237,604.288 425.252,608.376 427.266,612.468 429.28,616.498 431.294,620.531 433.308,624.544 435.322,628.537 437.337,632.488 439.351,636.418 \n",
       "  441.365,640.282 443.379,644.145 445.393,647.986 447.408,651.803 449.422,655.573 451.436,659.364 453.45,663.131 455.464,666.849 457.478,670.588 459.493,674.3 \n",
       "  461.507,677.963 463.521,681.621 465.535,685.229 467.549,688.855 469.563,692.453 471.578,696.046 473.592,699.585 475.606,703.094 477.62,706.621 479.634,710.117 \n",
       "  481.648,713.558 483.663,717.015 485.677,720.44 487.691,723.858 489.705,727.267 491.719,730.669 493.734,734.036 495.748,737.394 497.762,740.744 499.776,744.058 \n",
       "  501.79,747.362 503.804,750.657 505.819,753.967 507.833,757.215 509.847,760.425 511.861,763.65 513.875,766.864 515.889,770.04 517.904,773.203 519.918,776.354 \n",
       "  521.932,779.493 523.946,782.619 525.96,785.677 527.974,788.748 529.989,791.778 532.003,794.794 534.017,797.795 536.031,800.782 538.045,803.753 540.06,806.681 \n",
       "  542.074,809.593 544.088,812.518 546.102,815.398 548.116,818.261 550.13,821.108 552.145,823.937 554.159,826.72 556.173,829.544 558.187,832.35 560.201,835.108 \n",
       "  562.215,837.849 564.23,840.6 566.244,843.303 568.258,846.016 570.272,848.68 572.286,851.355 574.301,854.01 576.315,856.645 578.329,859.229 580.343,861.822 \n",
       "  582.357,864.395 584.371,866.978 586.386,869.539 588.4,872.048 590.414,874.598 592.428,877.094 594.442,879.599 596.456,882.082 598.471,884.543 600.485,886.981 \n",
       "  602.499,889.395 604.513,891.818 606.527,894.25 608.541,896.593 610.556,898.945 612.57,901.305 614.584,903.641 616.598,905.984 618.612,908.303 620.627,910.564 \n",
       "  622.641,912.898 624.655,915.174 626.669,917.425 628.683,919.65 630.697,921.882 632.712,924.088 634.726,926.301 636.74,928.487 638.754,930.68 640.768,932.847 \n",
       "  642.782,935.02 644.797,937.132 646.811,939.251 648.825,941.376 650.839,943.473 652.853,945.542 654.867,947.618 656.882,949.699 658.896,951.752 660.91,953.777 \n",
       "  662.924,955.807 664.938,957.844 666.953,959.815 668.967,961.829 670.981,963.776 672.995,965.765 675.009,967.725 677.023,969.654 679.038,971.588 681.052,973.456 \n",
       "  683.066,975.329 685.08,977.243 687.094,979.09 689.108,980.943 691.123,982.8 693.137,984.625 695.151,986.456 697.165,988.291 699.179,990.058 701.193,991.866 \n",
       "  703.208,993.642 705.222,995.385 707.236,997.133 709.25,998.885 711.264,1000.6 713.279,1002.37 715.293,1004.06 717.307,1005.79 719.321,1007.49 721.335,1009.19 \n",
       "  723.349,1010.9 725.364,1012.57 727.378,1014.25 729.392,1015.89 731.406,1017.54 733.42,1019.19 735.434,1020.85 737.449,1022.47 739.463,1024.09 741.477,1025.68 \n",
       "  743.491,1027.27 745.505,1028.87 747.519,1030.43 749.534,1032.04 751.548,1033.6 753.562,1035.18 755.576,1036.71 757.59,1038.29 759.605,1039.8 761.619,1041.34 \n",
       "  763.633,1042.85 765.647,1044.37 767.661,1045.89 769.675,1047.37 771.69,1048.85 773.704,1050.34 775.718,1051.83 777.732,1053.28 779.746,1054.74 781.76,1056.2 \n",
       "  783.775,1057.62 785.789,1059.04 787.803,1060.47 789.817,1061.9 791.831,1063.29 793.845,1064.69 795.86,1066.09 797.874,1067.49 799.888,1068.85 801.902,1070.22 \n",
       "  803.916,1071.63 805.931,1072.95 807.945,1074.33 809.959,1075.66 811.973,1077 813.987,1078.34 816.001,1079.64 818.016,1080.94 820.03,1082.25 822.044,1083.56 \n",
       "  824.058,1084.82 826.072,1086.14 828.086,1087.41 830.101,1088.64 832.115,1089.92 834.129,1091.16 836.143,1092.44 838.157,1093.68 840.171,1094.88 842.186,1096.13 \n",
       "  844.2,1097.33 846.214,1098.54 848.228,1099.75 850.242,1100.96 852.257,1102.13 854.271,1103.3 856.285,1104.52 858.299,1105.65 860.313,1106.83 862.327,1108.01 \n",
       "  864.342,1109.2 866.356,1110.34 868.37,1111.48 870.384,1112.62 872.398,1113.81 874.412,1114.92 876.427,1116.07 878.441,1117.18 880.455,1118.33 882.469,1119.44 \n",
       "  884.483,1120.55 886.498,1121.67 888.512,1122.74 890.526,1123.82 892.54,1124.94 894.554,1126.01 896.568,1127.09 898.583,1128.13 900.597,1129.21 902.611,1130.25 \n",
       "  904.625,1131.29 906.639,1132.33 908.653,1133.37 910.668,1134.42 912.682,1135.42 914.696,1136.47 916.71,1137.47 918.724,1138.53 920.738,1139.53 922.753,1140.54 \n",
       "  924.767,1141.51 926.781,1142.47 928.795,1143.48 930.809,1144.45 932.824,1145.47 934.838,1146.44 936.852,1147.41 938.866,1148.39 940.88,1149.31 942.894,1150.29 \n",
       "  944.909,1151.22 946.923,1152.2 948.937,1153.14 950.951,1154.07 952.965,1155.01 954.979,1155.95 956.994,1156.88 959.008,1157.78 961.022,1158.72 963.036,1159.61 \n",
       "  965.05,1160.56 967.064,1161.46 969.079,1162.36 971.093,1163.26 973.107,1164.16 975.121,1165.06 977.135,1165.92 979.15,1166.82 981.164,1167.68 983.178,1168.59 \n",
       "  985.192,1169.45 987.206,1170.31 989.22,1171.18 991.235,1172.04 993.249,1172.9 995.263,1173.77 997.277,1174.59 999.291,1175.46 1001.31,1176.28 1003.32,1177.15 \n",
       "  1005.33,1177.97 1007.35,1178.8 1009.36,1179.62 1011.38,1180.45 1013.39,1181.28 1015.4,1182.1 1017.42,1182.93 1019.43,1183.71 1021.45,1184.55 1023.46,1185.33 \n",
       "  1025.48,1186.16 1027.49,1186.95 1029.5,1187.73 1031.52,1188.52 1033.53,1189.31 1035.55,1190.1 1037.56,1190.89 1039.57,1191.68 1041.59,1192.42 1043.6,1193.21 \n",
       "  1045.62,1194 1047.63,1194.75 1049.65,1195.49 1051.66,1196.29 1053.67,1197.03 1055.69,1197.78 1057.7,1198.53 1059.72,1199.28 1061.73,1200.03 1063.74,1200.78 \n",
       "  1065.76,1201.53 1067.77,1202.29 1069.79,1202.99 1071.8,1203.74 1073.82,1204.45 1075.83,1205.2 1077.84,1205.91 1079.86,1206.62 1081.87,1207.37 1083.89,1208.08 \n",
       "  1085.9,1208.79 1087.91,1209.5 1089.93,1210.21 1091.94,1210.92 1093.96,1211.64 1095.97,1212.3 1097.99,1213.01 1100,1213.73 1102.01,1214.39 1104.03,1215.11 \n",
       "  1106.04,1215.77 1108.06,1216.49 1110.07,1217.15 1112.09,1217.82 1114.1,1218.49 1116.11,1219.16 1118.13,1219.88 1120.14,1220.55 1122.16,1221.22 1124.17,1221.89 \n",
       "  1126.18,1222.56 1128.2,1223.23 1130.21,1223.86 1132.23,1224.53 1134.24,1225.2 1136.26,1225.83 1138.27,1226.5 1140.28,1227.13 1142.3,1227.8 1144.31,1228.43 \n",
       "  1146.33,1229.06 1148.34,1229.68 1150.35,1230.36 1152.37,1230.99 1154.38,1231.62 1156.4,1232.25 1158.41,1232.88 1160.43,1233.51 1162.44,1234.14 1164.45,1234.72 \n",
       "  1166.47,1235.35 1168.48,1235.98 1170.5,1236.61 1172.51,1237.19 1174.52,1237.83 1176.54,1238.41 1178.55,1238.99 1180.57,1239.63 1182.58,1240.21 1184.6,1240.85 \n",
       "  1186.61,1241.43 1188.62,1242.02 1190.64,1242.66 1192.65,1243.24 1194.67,1243.83 1196.68,1244.41 1198.69,1245 1200.71,1245.54 1202.72,1246.12 1204.74,1246.71 \n",
       "  1206.75,1247.3 1208.77,1247.89 1210.78,1248.43 1212.79,1249.02 1214.81,1249.56 1216.82,1250.1 1218.84,1250.69 1220.85,1251.23 1222.86,1251.82 1224.88,1252.36 \n",
       "  1226.89,1252.9 1228.91,1253.44 1230.92,1253.98 1232.94,1254.58 1234.95,1255.12 1236.96,1255.66 1238.98,1256.2 1240.99,1256.75 1243.01,1257.29 1245.02,1257.84 \n",
       "  1247.03,1258.38 1249.05,1258.87 1251.06,1259.42 1253.08,1259.96 1255.09,1260.51 1257.11,1261 1259.12,1261.55 1261.13,1262.09 1263.15,1262.59 1265.16,1263.08 \n",
       "  1267.18,1263.63 1269.19,1264.12 1271.2,1264.67 1273.22,1265.17 1275.23,1265.66 1277.25,1266.16 1279.26,1266.65 1281.28,1267.21 1283.29,1267.7 1285.3,1268.2 \n",
       "  1287.32,1268.7 1289.33,1269.2 1291.35,1269.69 1293.36,1270.19 1295.37,1270.69 1297.39,1271.19 1299.4,1271.69 1301.42,1272.19 1303.43,1272.64 1305.45,1273.14 \n",
       "  1307.46,1273.64 1309.47,1274.08 1311.49,1274.58 1313.5,1275.09 1315.52,1275.53 1317.53,1276.04 1319.54,1276.48 1321.56,1276.93 1323.57,1277.43 1325.59,1277.88 \n",
       "  1327.6,1278.39 1329.62,1278.84 1331.63,1279.29 1333.64,1279.74 1335.66,1280.24 1337.67,1280.69 1339.69,1281.14 1341.7,1281.59 1343.72,1282.04 1345.73,1282.49 \n",
       "  1347.74,1282.95 1349.76,1283.4 1351.77,1283.85 1353.79,1284.3 1355.8,1284.75 1357.81,1285.21 1359.83,1285.6 1361.84,1286.06 1363.86,1286.51 1365.87,1286.97 \n",
       "  1367.89,1287.36 1369.9,1287.82 1371.91,1288.27 1373.93,1288.67 1375.94,1289.13 1377.96,1289.53 1379.97,1289.93 1381.98,1290.38 1384,1290.78 1386.01,1291.18 \n",
       "  1388.03,1291.64 1390.04,1292.04 1392.06,1292.44 1394.07,1292.84 1396.08,1293.24 1398.1,1293.64 1400.11,1294.1 1402.13,1294.5 1404.14,1294.91 1406.15,1295.31 \n",
       "  1408.17,1295.65 1410.18,1296.06 1412.2,1296.46 1414.21,1296.86 1416.23,1297.26 1418.24,1297.67 1420.25,1298.01 1422.27,1298.42 1424.28,1298.82 1426.3,1299.17 \n",
       "  1428.31,1299.57 1430.32,1299.98 1432.34,1300.33 1434.35,1300.73 1436.37,1301.14 1438.38,1301.49 1440.4,1301.89 1442.41,1302.24 1444.42,1302.65 1446.44,1303 \n",
       "  1448.45,1303.4 1450.47,1303.75 1452.48,1304.16 1454.49,1304.51 1456.51,1304.86 1458.52,1305.27 1460.54,1305.62 1462.55,1306.02 1464.57,1306.38 1466.58,1306.73 \n",
       "  1468.59,1307.08 1470.61,1307.43 1472.62,1307.78 1474.64,1308.19 1476.65,1308.48 1478.66,1308.89 1480.68,1309.24 1482.69,1309.54 1484.71,1309.89 1486.72,1310.24 \n",
       "  1488.74,1310.59 1490.75,1310.95 1492.76,1311.3 1494.78,1311.65 1496.79,1311.95 1498.81,1312.3 1500.82,1312.65 1502.83,1313.01 1504.85,1313.3 1506.86,1313.66 \n",
       "  1508.88,1314.01 1510.89,1314.31 1512.91,1314.66 1514.92,1314.96 1516.93,1315.31 1518.95,1315.67 1520.96,1315.96 1522.98,1316.32 1524.99,1316.62 1527,1316.97 \n",
       "  1529.02,1317.27 1531.03,1317.62 1533.05,1317.92 1535.06,1318.22 1537.08,1318.58 1539.09,1318.87 1541.1,1319.23 1543.12,1319.53 1545.13,1319.83 1547.15,1320.18 \n",
       "  1549.16,1320.48 1551.17,1320.78 1553.19,1321.08 1555.2,1321.44 1557.22,1321.73 1559.23,1322.03 1561.25,1322.33 1563.26,1322.63 1565.27,1322.99 1567.29,1323.29 \n",
       "  1569.3,1323.59 1571.32,1323.89 1573.33,1324.19 1575.35,1324.49 1577.36,1324.79 1579.37,1325.09 1581.39,1325.39 1583.4,1325.69 1585.42,1325.93 1587.43,1326.23 \n",
       "  1589.44,1326.53 1591.46,1326.83 1593.47,1327.13 1595.49,1327.37 1597.5,1327.67 1599.52,1327.98 1601.53,1328.22 1603.54,1328.52 1605.56,1328.82 1607.57,1329.06 \n",
       "  1609.59,1329.36 1611.6,1329.67 1613.61,1329.91 1615.63,1330.21 1617.64,1330.45 1619.66,1330.75 1621.67,1331 1623.69,1331.3 1625.7,1331.54 1627.71,1331.84 \n",
       "  1629.73,1332.09 1631.74,1332.39 1633.76,1332.63 1635.77,1332.94 1637.78,1333.18 1639.8,1333.42 1641.81,1333.73 1643.83,1333.97 1645.84,1334.21 1647.86,1334.52 \n",
       "  1649.87,1334.76 1651.88,1335 1653.9,1335.25 1655.91,1335.55 1657.93,1335.8 1659.94,1336.04 1661.95,1336.28 1663.97,1336.53 1665.98,1336.77 1668,1337.08 \n",
       "  1670.01,1337.32 1672.03,1337.57 1674.04,1337.81 1676.05,1338.06 1678.07,1338.3 1680.08,1338.55 1682.1,1338.79 1684.11,1339.04 1686.12,1339.28 1688.14,1339.53 \n",
       "  1690.15,1339.77 1692.17,1340.02 1694.18,1340.26 1696.2,1340.51 1698.21,1340.69 1700.22,1340.94 1702.24,1341.18 1704.25,1341.43 1706.27,1341.68 1708.28,1341.92 \n",
       "  1710.29,1342.11 1712.31,1342.35 1714.32,1342.6 1716.34,1342.84 1718.35,1343.09 1720.37,1343.28 1722.38,1343.52 1724.39,1343.77 1726.41,1344.02 1728.42,1344.2 \n",
       "  1730.44,1344.45 1732.45,1344.7 1734.46,1344.88 1736.48,1345.13 1738.49,1345.37 1740.51,1345.56 1742.52,1345.81 1744.54,1345.99 1746.55,1346.24 1748.56,1346.49 \n",
       "  1750.58,1346.67 1752.59,1346.92 1754.61,1347.11 1756.62,1347.36 1758.63,1347.54 1760.65,1347.79 1762.66,1347.98 1764.68,1348.23 1766.69,1348.41 1768.71,1348.66 \n",
       "  1770.72,1348.85 1772.73,1349.03 1774.75,1349.28 1776.76,1349.47 1778.78,1349.72 1780.79,1349.9 1782.81,1350.09 1784.82,1350.34 1786.83,1350.53 1788.85,1350.71 \n",
       "  1790.86,1350.96 1792.88,1351.15 1794.89,1351.34 1796.9,1351.52 1798.92,1351.77 1800.93,1351.96 1802.95,1352.15 1804.96,1352.34 1806.98,1352.52 1808.99,1352.77 \n",
       "  1811,1352.96 1813.02,1353.15 1815.03,1353.34 1817.05,1353.52 1819.06,1353.71 1821.07,1353.9 1823.09,1354.09 1825.1,1354.34 1827.12,1354.52 1829.13,1354.71 \n",
       "  1831.15,1354.9 1833.16,1355.09 1835.17,1355.28 1837.19,1355.47 1839.2,1355.65 1841.22,1355.84 1843.23,1356.03 1845.24,1356.22 1847.26,1356.41 1849.27,1356.6 \n",
       "  1851.29,1356.78 1853.3,1356.97 1855.32,1357.16 1857.33,1357.35 1859.34,1357.54 1861.36,1357.73 1863.37,1357.85 1865.39,1358.04 1867.4,1358.23 1869.41,1358.42 \n",
       "  1871.43,1358.61 1873.44,1358.8 1875.46,1358.99 1877.47,1359.11 1879.49,1359.3 1881.5,1359.49 1883.51,1359.68 1885.53,1359.87 1887.54,1360 1889.56,1360.19 \n",
       "  1891.57,1360.38 1893.58,1360.57 1895.6,1360.69 1897.61,1360.88 1899.63,1361.07 1901.64,1361.26 1903.66,1361.39 1905.67,1361.58 1907.68,1361.77 1909.7,1361.9 \n",
       "  1911.71,1362.09 1913.73,1362.28 1915.74,1362.4 1917.75,1362.59 1919.77,1362.78 1921.78,1362.91 1923.8,1363.1 1925.81,1363.29 1927.83,1363.42 1929.84,1363.61 \n",
       "  1931.85,1363.73 1933.87,1363.93 1935.88,1364.05 1937.9,1364.24 1939.91,1364.37 1941.92,1364.56 1943.94,1364.75 1945.95,1364.88 1947.97,1365.07 1949.98,1365.2 \n",
       "  1952,1365.39 1954.01,1365.52 1956.02,1365.71 1958.04,1365.83 1960.05,1365.96 1962.07,1366.15 1964.08,1366.28 1966.09,1366.47 1968.11,1366.6 1970.12,1366.79 \n",
       "  1972.14,1366.92 1974.15,1367.05 1976.17,1367.24 1978.18,1367.36 1980.19,1367.56 1982.21,1367.68 1984.22,1367.81 1986.24,1368 1988.25,1368.13 1990.26,1368.26 \n",
       "  1992.28,1368.45 1994.29,1368.58 1996.31,1368.77 1998.32,1368.9 2000.34,1369.03 2002.35,1369.22 2004.36,1369.35 2006.38,1369.47 2008.39,1369.6 2010.41,1369.79 \n",
       "  2012.42,1369.92 2014.44,1370.05 2016.45,1370.24 2018.46,1370.37 2020.48,1370.5 2022.49,1370.63 2024.51,1370.82 2026.52,1370.95 2028.53,1371.08 2030.55,1371.21 \n",
       "  2032.56,1371.33 2034.58,1371.53 2036.59,1371.66 2038.61,1371.78 2040.62,1371.91 2042.63,1372.04 2044.65,1372.17 2046.66,1372.36 2048.68,1372.49 2050.69,1372.62 \n",
       "  2052.7,1372.75 2054.72,1372.88 2056.73,1373.01 2058.75,1373.13 2060.76,1373.33 2062.78,1373.46 2064.79,1373.59 2066.8,1373.71 2068.82,1373.84 2070.83,1373.97 \n",
       "  2072.85,1374.1 2074.86,1374.23 2076.87,1374.36 2078.89,1374.49 2080.9,1374.62 2082.92,1374.75 2084.93,1374.87 2086.95,1375 2088.96,1375.13 2090.97,1375.26 \n",
       "  2092.99,1375.39 2095,1375.52 2097.02,1375.65 2099.03,1375.78 2101.04,1375.91 2103.06,1376.04 2105.07,1376.17 2107.09,1376.3 2109.1,1376.43 2111.12,1376.56 \n",
       "  2113.13,1376.68 2115.14,1376.81 2117.16,1376.94 2119.17,1377.07 2121.19,1377.2 2123.2,1377.33 2125.21,1377.46 2127.23,1377.53 2129.24,1377.66 2131.26,1377.79 \n",
       "  2133.27,1377.92 2135.29,1378.04 2137.3,1378.17 2139.31,1378.3 2141.33,1378.37 2143.34,1378.5 2145.36,1378.63 2147.37,1378.76 2149.38,1378.89 2151.4,1379.02 \n",
       "  2153.41,1379.08 2155.43,1379.21 2157.44,1379.34 2159.46,1379.47 2161.47,1379.6 2163.48,1379.67 2165.5,1379.8 2167.51,1379.93 2169.53,1380.06 2171.54,1380.12 \n",
       "  2173.55,1380.25 2175.57,1380.38 2177.58,1380.51 2179.6,1380.58 2181.61,1380.71 2183.63,1380.84 2185.64,1380.97 2187.65,1381.03 2189.67,1381.16 2191.68,1381.29 \n",
       "  2193.7,1381.36 2195.71,1381.49 2197.72,1381.62 2199.74,1381.69 2201.75,1381.82 2203.77,1381.95 2205.78,1382.01 2207.8,1382.14 2209.81,1382.27 2211.82,1382.34 \n",
       "  2213.84,1382.47 2215.85,1382.6 2217.87,1382.66 2219.88,1382.8 2221.9,1382.86 2223.91,1382.99 2225.92,1383.12 2227.94,1383.19 2229.95,1383.32 2231.97,1383.45 \n",
       "  2233.98,1383.51 2235.99,1383.64 2238.01,1383.71 2240.02,1383.84 2242.04,1383.91 2244.05,1384.04 2246.07,1384.17 2248.08,1384.23 2250.09,1384.36 2252.11,1384.43 \n",
       "  2254.12,1384.56 2256.14,1384.63 2258.15,1384.76 2260.16,1384.82 2262.18,1384.95 2264.19,1385.02 2266.21,1385.15 2268.22,1385.22 2270.24,1385.35 2272.25,1385.41 \n",
       "  2274.26,1385.54 2276.28,1385.61 2278.29,1385.74 2280.31,1385.81 2282.32,1385.94 2284.33,1386 2286.35,1386.13 2288.36,1386.2 2290.38,1386.26 2292.39,1386.4 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,160.256 282.245,168.86 284.259,177.235 286.274,185.462 288.288,193.581 290.302,201.493 292.316,209.178 294.33,216.806 296.344,224.122 298.359,231.395 \n",
       "  300.373,238.585 302.387,245.564 304.401,252.351 306.415,259.066 308.43,265.667 310.444,272.073 312.458,278.462 314.472,284.686 316.486,290.9 318.5,296.916 \n",
       "  320.515,302.796 322.529,308.631 324.543,314.35 326.557,319.951 328.571,325.5 330.585,330.953 332.6,336.521 334.614,341.79 336.628,346.999 338.642,352.073 \n",
       "  340.656,357.112 342.67,362.084 344.685,367.048 346.699,371.749 348.713,376.437 350.727,381.096 352.741,385.696 354.756,390.111 356.77,394.617 358.784,398.84 \n",
       "  360.798,403.153 362.812,407.351 364.826,411.321 366.841,415.41 368.855,419.363 370.869,423.274 372.883,427.256 374.897,430.968 376.911,434.781 378.926,438.402 \n",
       "  380.94,441.959 382.954,445.601 384.968,449.162 386.982,452.489 388.996,455.916 391.011,459.31 393.025,462.618 395.039,465.873 397.053,469.11 399.067,472.327 \n",
       "  401.082,475.576 403.096,478.736 405.11,481.858 407.124,484.994 409.138,488.127 411.152,491.096 413.167,494.096 415.181,497.055 417.195,500.009 419.209,502.849 \n",
       "  421.223,505.864 423.237,508.673 425.252,511.402 427.266,514.124 429.28,516.874 431.294,519.673 433.308,522.446 435.322,525.062 437.337,527.707 439.351,530.437 \n",
       "  441.365,533.122 443.379,535.893 445.393,538.389 447.408,541.009 449.422,543.408 451.436,546.124 453.45,548.542 455.464,551.201 457.478,553.714 459.493,556.178 \n",
       "  461.507,558.591 463.521,560.974 465.535,563.385 467.549,565.705 469.563,568.052 471.578,570.348 473.592,572.712 475.606,575.104 477.62,577.403 479.634,579.751 \n",
       "  481.648,581.984 483.663,584.144 485.677,586.392 487.691,588.668 489.705,590.848 491.719,593.036 493.734,595.209 495.748,597.473 497.762,599.577 499.776,601.604 \n",
       "  501.79,603.553 503.804,605.634 505.819,607.827 507.833,609.773 509.847,611.894 511.861,613.873 513.875,615.964 515.889,617.826 517.904,619.95 519.918,621.974 \n",
       "  521.932,623.787 523.946,625.887 525.96,627.667 527.974,629.692 529.989,631.613 532.003,633.408 534.017,635.23 536.031,636.924 538.045,638.955 540.06,640.902 \n",
       "  542.074,642.677 544.088,644.301 546.102,646.13 548.116,648.008 550.13,649.892 552.145,651.555 554.159,653.268 556.173,654.894 558.187,656.797 560.201,658.454 \n",
       "  562.215,660.002 564.23,661.713 566.244,663.52 568.258,665.401 570.272,666.803 572.286,668.462 574.301,670.033 576.315,671.769 578.329,673.533 580.343,675.045 \n",
       "  582.357,676.654 584.371,678.454 586.386,680.048 588.4,681.598 590.414,683.152 592.428,684.756 594.442,686.341 596.456,687.834 598.471,689.378 600.485,690.926 \n",
       "  602.499,692.501 604.513,694.127 606.527,695.662 608.541,696.983 610.556,698.548 612.57,699.899 614.584,701.543 616.598,702.876 618.612,704.333 620.627,705.768 \n",
       "  622.641,707.329 624.655,708.722 626.669,710.142 628.683,711.688 630.697,713.04 632.712,714.297 634.726,715.581 636.74,716.99 638.754,718.378 640.768,719.943 \n",
       "  642.782,721.387 644.797,722.634 646.811,723.833 648.825,725.21 650.839,726.539 652.853,727.846 654.867,729.256 656.882,730.467 658.896,731.756 660.91,733.199 \n",
       "  662.924,734.366 664.938,735.586 666.953,737.012 668.967,738.16 670.981,739.361 672.995,740.539 675.009,741.898 677.023,743.183 679.038,744.522 681.052,745.579 \n",
       "  683.066,746.716 685.08,748.036 687.094,749.254 689.108,750.475 691.123,751.515 693.137,752.845 695.151,753.967 697.165,755.196 699.179,756.48 701.193,757.661 \n",
       "  703.208,758.818 705.222,760.161 707.236,761.19 709.25,762.194 711.264,763.438 713.279,764.552 715.293,765.721 717.307,766.918 719.321,767.903 721.335,768.837 \n",
       "  723.349,769.986 725.364,771.191 727.378,772.103 729.392,773.257 731.406,774.306 733.42,775.383 735.434,776.408 737.449,777.408 739.463,778.274 741.477,779.439 \n",
       "  743.491,780.334 745.505,781.503 747.519,782.673 749.534,783.791 751.548,784.938 753.562,786.06 755.576,786.991 757.59,788.061 759.605,788.995 761.619,790.151 \n",
       "  763.633,791.143 765.647,792.054 767.661,793.022 769.675,794.101 771.69,795.071 773.704,796.099 775.718,796.821 777.732,797.823 779.746,798.604 781.76,799.636 \n",
       "  783.775,800.558 785.789,801.65 787.803,802.434 789.817,803.781 791.831,804.794 793.845,805.779 795.86,806.794 797.874,807.726 799.888,808.829 801.902,809.452 \n",
       "  803.916,810.302 805.931,811.324 807.945,812.205 809.959,813.287 811.973,814.113 813.987,814.941 816.001,815.884 818.016,816.828 820.03,817.688 822.044,818.405 \n",
       "  824.058,819.123 826.072,820.014 828.086,820.993 830.101,822.002 832.115,822.723 834.129,823.619 836.143,824.487 838.157,825.501 840.171,826.662 842.186,827.505 \n",
       "  844.2,828.407 846.214,829.223 848.228,829.748 850.242,830.828 852.257,831.559 854.271,832.496 856.285,833.434 858.299,834.197 860.313,834.903 862.327,835.903 \n",
       "  864.342,836.698 866.356,837.406 868.37,838.439 870.384,839.238 872.398,839.83 874.412,840.689 876.427,841.698 878.441,842.559 880.455,843.332 882.469,844.107 \n",
       "  884.483,844.792 886.498,845.598 888.512,846.494 890.526,847.332 892.54,848.141 894.554,848.8 896.568,849.911 898.583,850.843 900.597,851.265 902.611,852.199 \n",
       "  904.625,852.983 906.639,853.647 908.653,854.282 910.668,855.159 912.682,855.795 914.696,856.584 916.71,857.404 918.724,858.194 920.738,859.046 922.753,859.899 \n",
       "  924.767,860.692 926.781,861.425 928.795,862.189 930.809,862.679 932.824,863.291 934.838,864.273 936.852,864.825 938.866,865.409 940.88,866.27 942.894,867.132 \n",
       "  944.909,867.964 946.923,868.643 948.937,869.138 950.951,869.787 952.965,870.406 954.979,871.273 956.994,871.831 959.008,872.483 961.022,873.104 963.036,874.037 \n",
       "  965.05,874.504 967.064,874.972 969.079,875.907 971.093,876.813 973.107,877.407 975.121,878.064 977.135,878.69 979.15,879.254 981.164,880.039 983.178,880.667 \n",
       "  985.192,881.39 987.206,882.24 989.22,882.933 991.235,883.627 993.249,884.195 995.263,884.701 997.277,885.207 999.291,885.967 1001.31,886.79 1003.32,887.076 \n",
       "  1005.33,887.837 1007.35,888.473 1009.36,889.172 1011.38,889.841 1013.39,890.51 1015.4,891.148 1017.42,892.074 1019.43,892.713 1021.45,893.353 1023.46,894.314 \n",
       "  1025.48,894.667 1027.49,895.148 1029.5,895.758 1031.52,896.593 1033.53,897.14 1035.55,897.688 1037.56,898.526 1039.57,899.139 1041.59,899.656 1043.6,899.882 \n",
       "  1045.62,900.431 1047.63,901.046 1049.65,901.921 1051.66,902.569 1053.67,903.413 1055.69,904.063 1057.7,904.356 1059.72,904.681 1061.73,905.332 1063.74,906.212 \n",
       "  1065.76,906.702 1067.77,907.355 1069.79,907.91 1071.8,908.499 1073.82,909.252 1075.83,909.58 1077.84,909.908 1079.86,910.925 1081.87,911.188 1083.89,911.714 \n",
       "  1085.9,912.404 1087.91,912.931 1089.93,913.722 1091.94,914.118 1093.96,914.712 1095.97,915.241 1097.99,915.902 1100,916.398 1102.01,917.226 1104.03,917.889 \n",
       "  1106.04,918.288 1108.06,918.686 1110.07,919.45 1112.09,919.75 1114.1,920.382 1116.11,920.915 1118.13,921.381 1120.14,922.115 1122.16,922.951 1124.17,923.318 \n",
       "  1126.18,923.92 1128.2,924.758 1130.21,925.026 1132.23,925.696 1134.24,925.931 1136.26,926.334 1138.27,927.04 1140.28,927.881 1142.3,928.554 1144.31,929.06 \n",
       "  1146.33,929.768 1148.34,930.174 1150.35,930.545 1152.37,931.187 1154.38,931.932 1156.4,932.271 1158.41,932.677 1160.43,933.288 1162.44,933.661 1164.45,934.442 \n",
       "  1166.47,934.714 1168.48,935.326 1170.5,935.667 1172.51,936.348 1174.52,936.757 1176.54,937.371 1178.55,937.712 1180.57,938.43 1182.58,939.182 1184.6,939.422 \n",
       "  1186.61,940.141 1188.62,940.484 1190.64,940.964 1192.65,941.754 1194.67,942.475 1196.68,942.957 1198.69,943.576 1200.71,944.093 1202.72,944.645 1204.74,945.024 \n",
       "  1206.75,945.715 1208.77,946.199 1210.78,946.718 1212.79,947.167 1214.81,947.652 1216.82,948.137 1218.84,948.901 1220.85,949.421 1222.86,950.082 1224.88,950.534 \n",
       "  1226.89,950.951 1228.91,951.474 1230.92,951.718 1232.94,952.206 1234.95,952.904 1236.96,953.392 1238.98,953.602 1240.99,954.301 1243.01,954.826 1245.02,955.387 \n",
       "  1247.03,955.737 1249.05,956.228 1251.06,956.86 1253.08,957.457 1255.09,958.019 1257.11,958.336 1259.12,959.075 1261.13,959.428 1263.15,959.639 1265.16,960.345 \n",
       "  1267.18,960.556 1269.19,961.086 1271.2,961.44 1273.22,962.218 1275.23,962.395 1277.25,963.315 1279.26,963.812 1281.28,964.309 1283.29,964.593 1285.3,965.268 \n",
       "  1287.32,965.588 1289.33,965.801 1291.35,966.264 1293.36,966.584 1295.37,967.439 1297.39,967.832 1299.4,968.474 1301.42,969.01 1303.43,969.51 1305.45,969.797 \n",
       "  1307.46,970.262 1309.47,970.656 1311.49,971.158 1313.5,971.731 1315.52,972.198 1317.53,972.701 1319.54,972.952 1321.56,973.096 1323.57,973.672 1325.59,974.212 \n",
       "  1327.6,974.464 1329.62,974.896 1331.63,975.185 1333.64,975.978 1335.66,976.593 1337.67,976.809 1339.69,977.279 1341.7,977.895 1343.72,978.474 1345.73,979.018 \n",
       "  1347.74,979.489 1349.76,979.707 1351.77,980.325 1353.79,980.724 1355.8,980.979 1357.81,981.088 1359.83,981.707 1361.84,982.435 1363.86,982.836 1365.87,983.42 \n",
       "  1367.89,984.004 1369.9,984.26 1371.91,984.625 1373.93,984.881 1375.94,985.393 1377.96,985.943 1379.97,986.346 1381.98,987.189 1384,987.41 1386.01,987.703 \n",
       "  1388.03,988.034 1390.04,988.144 1392.06,988.696 1394.07,989.137 1396.08,989.837 1398.1,990.279 1400.11,990.353 1402.13,990.832 1404.14,991.201 1406.15,991.459 \n",
       "  1408.17,992.235 1410.18,992.679 1412.2,992.901 1414.21,993.308 1416.23,993.456 1418.24,994.049 1420.25,994.457 1422.27,994.42 1424.28,994.754 1426.3,995.496 \n",
       "  1428.31,995.719 1430.32,996.202 1432.34,996.649 1434.35,996.835 1436.37,997.207 1438.38,997.356 1440.4,997.803 1442.41,998.213 1444.42,998.922 1446.44,999.183 \n",
       "  1448.45,999.52 1450.47,999.819 1452.48,1000.42 1454.49,1001.02 1456.51,1001.09 1458.52,1001.95 1460.54,1002.52 1462.55,1002.67 1464.57,1003.3 1466.58,1003.23 \n",
       "  1468.59,1003.68 1470.61,1004.17 1472.62,1004.43 1474.64,1004.81 1476.65,1005.03 1478.66,1005.56 1480.68,1005.94 1482.69,1006.09 1484.71,1006.54 1486.72,1007.11 \n",
       "  1488.74,1007.3 1490.75,1007.6 1492.76,1007.98 1494.78,1008.28 1496.79,1008.55 1498.81,1008.74 1500.82,1009 1502.83,1009.83 1504.85,1009.87 1506.86,1010.48 \n",
       "  1508.88,1010.97 1510.89,1011.05 1512.91,1011.39 1514.92,1011.89 1516.93,1012.04 1518.95,1012.42 1520.96,1012.57 1522.98,1013.14 1524.99,1013.45 1527,1014.02 \n",
       "  1529.02,1014.55 1531.03,1014.55 1533.05,1014.94 1535.06,1014.94 1537.08,1015.32 1539.09,1015.74 1541.1,1016.01 1543.12,1016.43 1545.13,1017.16 1547.15,1017.35 \n",
       "  1549.16,1017.69 1551.17,1017.73 1553.19,1018.38 1555.2,1018.81 1557.22,1018.96 1559.23,1019.38 1561.25,1019.77 1563.26,1019.92 1565.27,1020.46 1567.29,1020.65 \n",
       "  1569.3,1020.58 1571.32,1021.23 1573.33,1022.04 1575.35,1022.23 1577.36,1022.35 1579.37,1022.5 1581.39,1023.08 1583.4,1023.55 1585.42,1023.74 1587.43,1024.01 \n",
       "  1589.44,1024.13 1591.46,1024.79 1593.47,1024.9 1595.49,1025.18 1597.5,1025.68 1599.52,1026.15 1601.53,1026.61 1603.54,1026.57 1605.56,1026.81 1607.57,1027.39 \n",
       "  1609.59,1027.55 1611.6,1027.58 1613.61,1028.17 1615.63,1028.64 1617.64,1029.06 1619.66,1029.26 1621.67,1029.46 1623.69,1029.77 1625.7,1030.12 1627.71,1030.35 \n",
       "  1629.73,1030.59 1631.74,1030.9 1633.76,1031.21 1635.77,1031.64 1637.78,1031.76 1639.8,1032.31 1641.81,1032.51 1643.83,1032.39 1645.84,1032.7 1647.86,1033.37 \n",
       "  1649.87,1033.72 1651.88,1034.04 1653.9,1034.19 1655.91,1034.19 1657.93,1034.43 1659.94,1034.82 1661.95,1035.45 1663.97,1035.18 1665.98,1035.53 1668,1035.96 \n",
       "  1670.01,1036.28 1672.03,1036.52 1674.04,1036.83 1676.05,1037.11 1678.07,1037.38 1680.08,1037.54 1682.1,1037.62 1684.11,1037.9 1686.12,1038.33 1688.14,1038.45 \n",
       "  1690.15,1038.85 1692.17,1039.16 1694.18,1039.44 1696.2,1039.52 1698.21,1039.52 1700.22,1039.92 1702.24,1040.35 1704.25,1040.35 1706.27,1040.79 1708.28,1041.03 \n",
       "  1710.29,1041.26 1712.31,1041.34 1714.32,1041.7 1716.34,1041.7 1718.35,1041.98 1720.37,1042.42 1722.38,1042.7 1724.39,1043.17 1726.41,1043.57 1728.42,1043.57 \n",
       "  1730.44,1043.73 1732.45,1044.09 1734.46,1044.29 1736.48,1044.49 1738.49,1044.77 1740.51,1044.85 1742.52,1045.53 1744.54,1045.57 1746.55,1045.81 1748.56,1046.01 \n",
       "  1750.58,1046.05 1752.59,1046.17 1754.61,1046.21 1756.62,1046.49 1758.63,1046.37 1760.65,1046.61 1762.66,1046.93 1764.68,1047.17 1766.69,1047.45 1768.71,1047.89 \n",
       "  1770.72,1047.65 1772.73,1047.85 1774.75,1048.53 1776.76,1048.73 1778.78,1049.09 1780.79,1048.93 1782.81,1049.13 1784.82,1049.81 1786.83,1050.18 1788.85,1050.02 \n",
       "  1790.86,1050.42 1792.88,1050.66 1794.89,1050.98 1796.9,1050.94 1798.92,1051.26 1800.93,1051.34 1802.95,1051.79 1804.96,1052.03 1806.98,1052.15 1808.99,1052.27 \n",
       "  1811,1052.31 1813.02,1052.55 1815.03,1052.67 1817.05,1053.32 1819.06,1053.6 1821.07,1054.09 1823.09,1053.89 1825.1,1054.21 1827.12,1054.7 1829.13,1054.78 \n",
       "  1831.15,1055.02 1833.16,1055.55 1835.17,1055.22 1837.19,1055.3 1839.2,1055.59 1841.22,1055.91 1843.23,1056.24 1845.24,1056.36 1847.26,1056.64 1849.27,1056.85 \n",
       "  1851.29,1056.85 1853.3,1057.01 1855.32,1057.21 1857.33,1057.7 1859.34,1057.86 1861.36,1058.15 1863.37,1058.39 1865.39,1058.59 1867.4,1058.63 1869.41,1058.92 \n",
       "  1871.43,1059.12 1873.44,1059.33 1875.46,1059.49 1877.47,1059.74 1879.49,1060.22 1881.5,1060.1 1883.51,1060.1 1885.53,1060.76 1887.54,1060.47 1889.56,1061 \n",
       "  1891.57,1061.25 1893.58,1061.29 1895.6,1061.61 1897.61,1061.98 1899.63,1061.82 1901.64,1062.15 1903.66,1062.06 1905.67,1062.56 1907.68,1063.54 1909.7,1063.21 \n",
       "  1911.71,1063.5 1913.73,1063.99 1915.74,1064.03 1917.75,1064.11 1919.77,1064.52 1921.78,1064.65 1923.8,1064.93 1925.81,1064.85 1927.83,1064.85 1929.84,1065.14 \n",
       "  1931.85,1065.02 1933.87,1065.43 1935.88,1065.59 1937.9,1065.96 1939.91,1066.33 1941.92,1066.29 1943.94,1066.54 1945.95,1066.83 1947.97,1066.99 1949.98,1067.2 \n",
       "  1952,1067.2 1954.01,1067.24 1956.02,1067.41 1958.04,1067.74 1960.05,1067.98 1962.07,1068.11 1964.08,1068.07 1966.09,1068.31 1968.11,1068.4 1970.12,1068.35 \n",
       "  1972.14,1068.85 1974.15,1068.93 1976.17,1068.77 1978.18,1068.97 1980.19,1069.22 1982.21,1069.43 1984.22,1069.72 1986.24,1070.01 1988.25,1070.09 1990.26,1070.59 \n",
       "  1992.28,1071.04 1994.29,1071.42 1996.31,1071.46 1998.32,1071.5 2000.34,1071.67 2002.35,1071.71 2004.36,1071.54 2006.38,1071.83 2008.39,1071.79 2010.41,1072 \n",
       "  2012.42,1072.16 2014.44,1072.33 2016.45,1072.46 2018.46,1072.5 2020.48,1072.7 2022.49,1072.83 2024.51,1073.25 2026.52,1073.37 2028.53,1073.54 2030.55,1073.33 \n",
       "  2032.56,1073.58 2034.58,1073.66 2036.59,1074.04 2038.61,1074.24 2040.62,1074.12 2042.63,1074.37 2044.65,1074.7 2046.66,1075.16 2048.68,1075.29 2050.69,1074.99 \n",
       "  2052.7,1075.12 2054.72,1075.62 2056.73,1075.66 2058.75,1076.12 2060.76,1076.29 2062.78,1076.37 2064.79,1076.46 2066.8,1076.67 2068.82,1077 2070.83,1077.29 \n",
       "  2072.85,1077.29 2074.86,1077.38 2076.87,1077.38 2078.89,1077.79 2080.9,1078.05 2082.92,1078.17 2084.93,1078.26 2086.95,1078.51 2088.96,1078.51 2090.97,1078.8 \n",
       "  2092.99,1078.76 2095,1078.76 2097.02,1079.14 2099.03,1079.26 2101.04,1078.93 2103.06,1079.18 2105.07,1079.72 2107.09,1079.98 2109.1,1080.48 2111.12,1080.14 \n",
       "  2113.13,1080.56 2115.14,1080.73 2117.16,1080.52 2119.17,1080.82 2121.19,1081.24 2123.2,1081.28 2125.21,1081.57 2127.23,1081.74 2129.24,1081.74 2131.26,1081.7 \n",
       "  2133.27,1081.87 2135.29,1081.99 2137.3,1082.25 2139.31,1082.88 2141.33,1082.92 2143.34,1082.92 2145.36,1083.34 2147.37,1083.34 2149.38,1083.51 2151.4,1083.68 \n",
       "  2153.41,1083.64 2155.43,1084.15 2157.44,1084.36 2159.46,1084.61 2161.47,1084.49 2163.48,1084.78 2165.5,1084.7 2167.51,1084.87 2169.53,1084.95 2171.54,1085.2 \n",
       "  2173.55,1085.25 2175.57,1085.37 2177.58,1085.29 2179.6,1085.2 2181.61,1085.88 2183.63,1086.18 2185.64,1086.18 2187.65,1086.31 2189.67,1086.43 2191.68,1086.48 \n",
       "  2193.7,1086.35 2195.71,1086.56 2197.72,1086.65 2199.74,1086.98 2201.75,1087.03 2203.77,1087.03 2205.78,1087.03 2207.8,1087.28 2209.81,1087.41 2211.82,1087.41 \n",
       "  2213.84,1087.62 2215.85,1087.71 2217.87,1088.17 2219.88,1088.3 2221.9,1088.39 2223.91,1088.51 2225.92,1088.47 2227.94,1088.85 2229.95,1088.9 2231.97,1088.94 \n",
       "  2233.98,1088.94 2235.99,1089.11 2238.01,1089.32 2240.02,1089.58 2242.04,1089.71 2244.05,1089.71 2246.07,1090.3 2248.08,1090.09 2250.09,1090.39 2252.11,1090.43 \n",
       "  2254.12,1090.52 2256.14,1090.64 2258.15,1090.9 2260.16,1091.07 2262.18,1091.37 2264.19,1091.54 2266.21,1091.41 2268.22,1091.84 2270.24,1091.71 2272.25,1091.97 \n",
       "  2274.26,1092.14 2276.28,1092.05 2278.29,1092.14 2280.31,1092.31 2282.32,1092.14 2284.33,1092.05 2286.35,1092.4 2288.36,1092.74 2290.38,1092.95 2292.39,1092.99 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip552)\" style=\"stroke:#3da44d; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  280.231,1092.99 282.245,1092.99 284.259,1092.99 286.274,1092.99 288.288,1092.99 290.302,1092.99 292.316,1092.99 294.33,1092.99 296.344,1092.99 298.359,1092.99 \n",
       "  300.373,1092.99 302.387,1092.99 304.401,1092.99 306.415,1092.99 308.43,1092.99 310.444,1092.99 312.458,1092.99 314.472,1092.99 316.486,1092.99 318.5,1092.99 \n",
       "  320.515,1092.99 322.529,1092.99 324.543,1092.99 326.557,1092.99 328.571,1092.99 330.585,1092.99 332.6,1092.99 334.614,1092.99 336.628,1092.99 338.642,1092.99 \n",
       "  340.656,1092.99 342.67,1092.99 344.685,1092.99 346.699,1092.99 348.713,1092.99 350.727,1092.99 352.741,1092.99 354.756,1092.99 356.77,1092.99 358.784,1092.99 \n",
       "  360.798,1092.99 362.812,1092.99 364.826,1092.99 366.841,1092.99 368.855,1092.99 370.869,1092.99 372.883,1092.99 374.897,1092.99 376.911,1092.99 378.926,1092.99 \n",
       "  380.94,1092.99 382.954,1092.99 384.968,1092.99 386.982,1092.99 388.996,1092.99 391.011,1092.99 393.025,1092.99 395.039,1092.99 397.053,1092.99 399.067,1092.99 \n",
       "  401.082,1092.99 403.096,1092.99 405.11,1092.99 407.124,1092.99 409.138,1092.99 411.152,1092.99 413.167,1092.99 415.181,1092.99 417.195,1092.99 419.209,1092.99 \n",
       "  421.223,1092.99 423.237,1092.99 425.252,1092.99 427.266,1092.99 429.28,1092.99 431.294,1092.99 433.308,1092.99 435.322,1092.99 437.337,1092.99 439.351,1092.99 \n",
       "  441.365,1092.99 443.379,1092.99 445.393,1092.99 447.408,1092.99 449.422,1092.99 451.436,1092.99 453.45,1092.99 455.464,1092.99 457.478,1092.99 459.493,1092.99 \n",
       "  461.507,1092.99 463.521,1092.99 465.535,1092.99 467.549,1092.99 469.563,1092.99 471.578,1092.99 473.592,1092.99 475.606,1092.99 477.62,1092.99 479.634,1092.99 \n",
       "  481.648,1092.99 483.663,1092.99 485.677,1092.99 487.691,1092.99 489.705,1092.99 491.719,1092.99 493.734,1092.99 495.748,1092.99 497.762,1092.99 499.776,1092.99 \n",
       "  501.79,1092.99 503.804,1092.99 505.819,1092.99 507.833,1092.99 509.847,1092.99 511.861,1092.99 513.875,1092.99 515.889,1092.99 517.904,1092.99 519.918,1092.99 \n",
       "  521.932,1092.99 523.946,1092.99 525.96,1092.99 527.974,1092.99 529.989,1092.99 532.003,1092.99 534.017,1092.99 536.031,1092.99 538.045,1092.99 540.06,1092.99 \n",
       "  542.074,1092.99 544.088,1092.99 546.102,1092.99 548.116,1092.99 550.13,1092.99 552.145,1092.99 554.159,1092.99 556.173,1092.99 558.187,1092.99 560.201,1092.99 \n",
       "  562.215,1092.99 564.23,1092.99 566.244,1092.99 568.258,1092.99 570.272,1092.99 572.286,1092.99 574.301,1092.99 576.315,1092.99 578.329,1092.99 580.343,1092.99 \n",
       "  582.357,1092.99 584.371,1092.99 586.386,1092.99 588.4,1092.99 590.414,1092.99 592.428,1092.99 594.442,1092.99 596.456,1092.99 598.471,1092.99 600.485,1092.99 \n",
       "  602.499,1092.99 604.513,1092.99 606.527,1092.99 608.541,1092.99 610.556,1092.99 612.57,1092.99 614.584,1092.99 616.598,1092.99 618.612,1092.99 620.627,1092.99 \n",
       "  622.641,1092.99 624.655,1092.99 626.669,1092.99 628.683,1092.99 630.697,1092.99 632.712,1092.99 634.726,1092.99 636.74,1092.99 638.754,1092.99 640.768,1092.99 \n",
       "  642.782,1092.99 644.797,1092.99 646.811,1092.99 648.825,1092.99 650.839,1092.99 652.853,1092.99 654.867,1092.99 656.882,1092.99 658.896,1092.99 660.91,1092.99 \n",
       "  662.924,1092.99 664.938,1092.99 666.953,1092.99 668.967,1092.99 670.981,1092.99 672.995,1092.99 675.009,1092.99 677.023,1092.99 679.038,1092.99 681.052,1092.99 \n",
       "  683.066,1092.99 685.08,1092.99 687.094,1092.99 689.108,1092.99 691.123,1092.99 693.137,1092.99 695.151,1092.99 697.165,1092.99 699.179,1092.99 701.193,1092.99 \n",
       "  703.208,1092.99 705.222,1092.99 707.236,1092.99 709.25,1092.99 711.264,1092.99 713.279,1092.99 715.293,1092.99 717.307,1092.99 719.321,1092.99 721.335,1092.99 \n",
       "  723.349,1092.99 725.364,1092.99 727.378,1092.99 729.392,1092.99 731.406,1092.99 733.42,1092.99 735.434,1092.99 737.449,1092.99 739.463,1092.99 741.477,1092.99 \n",
       "  743.491,1092.99 745.505,1092.99 747.519,1092.99 749.534,1092.99 751.548,1092.99 753.562,1092.99 755.576,1092.99 757.59,1092.99 759.605,1092.99 761.619,1092.99 \n",
       "  763.633,1092.99 765.647,1092.99 767.661,1092.99 769.675,1092.99 771.69,1092.99 773.704,1092.99 775.718,1092.99 777.732,1092.99 779.746,1092.99 781.76,1092.99 \n",
       "  783.775,1092.99 785.789,1092.99 787.803,1092.99 789.817,1092.99 791.831,1092.99 793.845,1092.99 795.86,1092.99 797.874,1092.99 799.888,1092.99 801.902,1092.99 \n",
       "  803.916,1092.99 805.931,1092.99 807.945,1092.99 809.959,1092.99 811.973,1092.99 813.987,1092.99 816.001,1092.99 818.016,1092.99 820.03,1092.99 822.044,1092.99 \n",
       "  824.058,1092.99 826.072,1092.99 828.086,1092.99 830.101,1092.99 832.115,1092.99 834.129,1092.99 836.143,1092.99 838.157,1092.99 840.171,1092.99 842.186,1092.99 \n",
       "  844.2,1092.99 846.214,1092.99 848.228,1092.99 850.242,1092.99 852.257,1092.99 854.271,1092.99 856.285,1092.99 858.299,1092.99 860.313,1092.99 862.327,1092.99 \n",
       "  864.342,1092.99 866.356,1092.99 868.37,1092.99 870.384,1092.99 872.398,1092.99 874.412,1092.99 876.427,1092.99 878.441,1092.99 880.455,1092.99 882.469,1092.99 \n",
       "  884.483,1092.99 886.498,1092.99 888.512,1092.99 890.526,1092.99 892.54,1092.99 894.554,1092.99 896.568,1092.99 898.583,1092.99 900.597,1092.99 902.611,1092.99 \n",
       "  904.625,1092.99 906.639,1092.99 908.653,1092.99 910.668,1092.99 912.682,1092.99 914.696,1092.99 916.71,1092.99 918.724,1092.99 920.738,1092.99 922.753,1092.99 \n",
       "  924.767,1092.99 926.781,1092.99 928.795,1092.99 930.809,1092.99 932.824,1092.99 934.838,1092.99 936.852,1092.99 938.866,1092.99 940.88,1092.99 942.894,1092.99 \n",
       "  944.909,1092.99 946.923,1092.99 948.937,1092.99 950.951,1092.99 952.965,1092.99 954.979,1092.99 956.994,1092.99 959.008,1092.99 961.022,1092.99 963.036,1092.99 \n",
       "  965.05,1092.99 967.064,1092.99 969.079,1092.99 971.093,1092.99 973.107,1092.99 975.121,1092.99 977.135,1092.99 979.15,1092.99 981.164,1092.99 983.178,1092.99 \n",
       "  985.192,1092.99 987.206,1092.99 989.22,1092.99 991.235,1092.99 993.249,1092.99 995.263,1092.99 997.277,1092.99 999.291,1092.99 1001.31,1092.99 1003.32,1092.99 \n",
       "  1005.33,1092.99 1007.35,1092.99 1009.36,1092.99 1011.38,1092.99 1013.39,1092.99 1015.4,1092.99 1017.42,1092.99 1019.43,1092.99 1021.45,1092.99 1023.46,1092.99 \n",
       "  1025.48,1092.99 1027.49,1092.99 1029.5,1092.99 1031.52,1092.99 1033.53,1092.99 1035.55,1092.99 1037.56,1092.99 1039.57,1092.99 1041.59,1092.99 1043.6,1092.99 \n",
       "  1045.62,1092.99 1047.63,1092.99 1049.65,1092.99 1051.66,1092.99 1053.67,1092.99 1055.69,1092.99 1057.7,1092.99 1059.72,1092.99 1061.73,1092.99 1063.74,1092.99 \n",
       "  1065.76,1092.99 1067.77,1092.99 1069.79,1092.99 1071.8,1092.99 1073.82,1092.99 1075.83,1092.99 1077.84,1092.99 1079.86,1092.99 1081.87,1092.99 1083.89,1092.99 \n",
       "  1085.9,1092.99 1087.91,1092.99 1089.93,1092.99 1091.94,1092.99 1093.96,1092.99 1095.97,1092.99 1097.99,1092.99 1100,1092.99 1102.01,1092.99 1104.03,1092.99 \n",
       "  1106.04,1092.99 1108.06,1092.99 1110.07,1092.99 1112.09,1092.99 1114.1,1092.99 1116.11,1092.99 1118.13,1092.99 1120.14,1092.99 1122.16,1092.99 1124.17,1092.99 \n",
       "  1126.18,1092.99 1128.2,1092.99 1130.21,1092.99 1132.23,1092.99 1134.24,1092.99 1136.26,1092.99 1138.27,1092.99 1140.28,1092.99 1142.3,1092.99 1144.31,1092.99 \n",
       "  1146.33,1092.99 1148.34,1092.99 1150.35,1092.99 1152.37,1092.99 1154.38,1092.99 1156.4,1092.99 1158.41,1092.99 1160.43,1092.99 1162.44,1092.99 1164.45,1092.99 \n",
       "  1166.47,1092.99 1168.48,1092.99 1170.5,1092.99 1172.51,1092.99 1174.52,1092.99 1176.54,1092.99 1178.55,1092.99 1180.57,1092.99 1182.58,1092.99 1184.6,1092.99 \n",
       "  1186.61,1092.99 1188.62,1092.99 1190.64,1092.99 1192.65,1092.99 1194.67,1092.99 1196.68,1092.99 1198.69,1092.99 1200.71,1092.99 1202.72,1092.99 1204.74,1092.99 \n",
       "  1206.75,1092.99 1208.77,1092.99 1210.78,1092.99 1212.79,1092.99 1214.81,1092.99 1216.82,1092.99 1218.84,1092.99 1220.85,1092.99 1222.86,1092.99 1224.88,1092.99 \n",
       "  1226.89,1092.99 1228.91,1092.99 1230.92,1092.99 1232.94,1092.99 1234.95,1092.99 1236.96,1092.99 1238.98,1092.99 1240.99,1092.99 1243.01,1092.99 1245.02,1092.99 \n",
       "  1247.03,1092.99 1249.05,1092.99 1251.06,1092.99 1253.08,1092.99 1255.09,1092.99 1257.11,1092.99 1259.12,1092.99 1261.13,1092.99 1263.15,1092.99 1265.16,1092.99 \n",
       "  1267.18,1092.99 1269.19,1092.99 1271.2,1092.99 1273.22,1092.99 1275.23,1092.99 1277.25,1092.99 1279.26,1092.99 1281.28,1092.99 1283.29,1092.99 1285.3,1092.99 \n",
       "  1287.32,1092.99 1289.33,1092.99 1291.35,1092.99 1293.36,1092.99 1295.37,1092.99 1297.39,1092.99 1299.4,1092.99 1301.42,1092.99 1303.43,1092.99 1305.45,1092.99 \n",
       "  1307.46,1092.99 1309.47,1092.99 1311.49,1092.99 1313.5,1092.99 1315.52,1092.99 1317.53,1092.99 1319.54,1092.99 1321.56,1092.99 1323.57,1092.99 1325.59,1092.99 \n",
       "  1327.6,1092.99 1329.62,1092.99 1331.63,1092.99 1333.64,1092.99 1335.66,1092.99 1337.67,1092.99 1339.69,1092.99 1341.7,1092.99 1343.72,1092.99 1345.73,1092.99 \n",
       "  1347.74,1092.99 1349.76,1092.99 1351.77,1092.99 1353.79,1092.99 1355.8,1092.99 1357.81,1092.99 1359.83,1092.99 1361.84,1092.99 1363.86,1092.99 1365.87,1092.99 \n",
       "  1367.89,1092.99 1369.9,1092.99 1371.91,1092.99 1373.93,1092.99 1375.94,1092.99 1377.96,1092.99 1379.97,1092.99 1381.98,1092.99 1384,1092.99 1386.01,1092.99 \n",
       "  1388.03,1092.99 1390.04,1092.99 1392.06,1092.99 1394.07,1092.99 1396.08,1092.99 1398.1,1092.99 1400.11,1092.99 1402.13,1092.99 1404.14,1092.99 1406.15,1092.99 \n",
       "  1408.17,1092.99 1410.18,1092.99 1412.2,1092.99 1414.21,1092.99 1416.23,1092.99 1418.24,1092.99 1420.25,1092.99 1422.27,1092.99 1424.28,1092.99 1426.3,1092.99 \n",
       "  1428.31,1092.99 1430.32,1092.99 1432.34,1092.99 1434.35,1092.99 1436.37,1092.99 1438.38,1092.99 1440.4,1092.99 1442.41,1092.99 1444.42,1092.99 1446.44,1092.99 \n",
       "  1448.45,1092.99 1450.47,1092.99 1452.48,1092.99 1454.49,1092.99 1456.51,1092.99 1458.52,1092.99 1460.54,1092.99 1462.55,1092.99 1464.57,1092.99 1466.58,1092.99 \n",
       "  1468.59,1092.99 1470.61,1092.99 1472.62,1092.99 1474.64,1092.99 1476.65,1092.99 1478.66,1092.99 1480.68,1092.99 1482.69,1092.99 1484.71,1092.99 1486.72,1092.99 \n",
       "  1488.74,1092.99 1490.75,1092.99 1492.76,1092.99 1494.78,1092.99 1496.79,1092.99 1498.81,1092.99 1500.82,1092.99 1502.83,1092.99 1504.85,1092.99 1506.86,1092.99 \n",
       "  1508.88,1092.99 1510.89,1092.99 1512.91,1092.99 1514.92,1092.99 1516.93,1092.99 1518.95,1092.99 1520.96,1092.99 1522.98,1092.99 1524.99,1092.99 1527,1092.99 \n",
       "  1529.02,1092.99 1531.03,1092.99 1533.05,1092.99 1535.06,1092.99 1537.08,1092.99 1539.09,1092.99 1541.1,1092.99 1543.12,1092.99 1545.13,1092.99 1547.15,1092.99 \n",
       "  1549.16,1092.99 1551.17,1092.99 1553.19,1092.99 1555.2,1092.99 1557.22,1092.99 1559.23,1092.99 1561.25,1092.99 1563.26,1092.99 1565.27,1092.99 1567.29,1092.99 \n",
       "  1569.3,1092.99 1571.32,1092.99 1573.33,1092.99 1575.35,1092.99 1577.36,1092.99 1579.37,1092.99 1581.39,1092.99 1583.4,1092.99 1585.42,1092.99 1587.43,1092.99 \n",
       "  1589.44,1092.99 1591.46,1092.99 1593.47,1092.99 1595.49,1092.99 1597.5,1092.99 1599.52,1092.99 1601.53,1092.99 1603.54,1092.99 1605.56,1092.99 1607.57,1092.99 \n",
       "  1609.59,1092.99 1611.6,1092.99 1613.61,1092.99 1615.63,1092.99 1617.64,1092.99 1619.66,1092.99 1621.67,1092.99 1623.69,1092.99 1625.7,1092.99 1627.71,1092.99 \n",
       "  1629.73,1092.99 1631.74,1092.99 1633.76,1092.99 1635.77,1092.99 1637.78,1092.99 1639.8,1092.99 1641.81,1092.99 1643.83,1092.99 1645.84,1092.99 1647.86,1092.99 \n",
       "  1649.87,1092.99 1651.88,1092.99 1653.9,1092.99 1655.91,1092.99 1657.93,1092.99 1659.94,1092.99 1661.95,1092.99 1663.97,1092.99 1665.98,1092.99 1668,1092.99 \n",
       "  1670.01,1092.99 1672.03,1092.99 1674.04,1092.99 1676.05,1092.99 1678.07,1092.99 1680.08,1092.99 1682.1,1092.99 1684.11,1092.99 1686.12,1092.99 1688.14,1092.99 \n",
       "  1690.15,1092.99 1692.17,1092.99 1694.18,1092.99 1696.2,1092.99 1698.21,1092.99 1700.22,1092.99 1702.24,1092.99 1704.25,1092.99 1706.27,1092.99 1708.28,1092.99 \n",
       "  1710.29,1092.99 1712.31,1092.99 1714.32,1092.99 1716.34,1092.99 1718.35,1092.99 1720.37,1092.99 1722.38,1092.99 1724.39,1092.99 1726.41,1092.99 1728.42,1092.99 \n",
       "  1730.44,1092.99 1732.45,1092.99 1734.46,1092.99 1736.48,1092.99 1738.49,1092.99 1740.51,1092.99 1742.52,1092.99 1744.54,1092.99 1746.55,1092.99 1748.56,1092.99 \n",
       "  1750.58,1092.99 1752.59,1092.99 1754.61,1092.99 1756.62,1092.99 1758.63,1092.99 1760.65,1092.99 1762.66,1092.99 1764.68,1092.99 1766.69,1092.99 1768.71,1092.99 \n",
       "  1770.72,1092.99 1772.73,1092.99 1774.75,1092.99 1776.76,1092.99 1778.78,1092.99 1780.79,1092.99 1782.81,1092.99 1784.82,1092.99 1786.83,1092.99 1788.85,1092.99 \n",
       "  1790.86,1092.99 1792.88,1092.99 1794.89,1092.99 1796.9,1092.99 1798.92,1092.99 1800.93,1092.99 1802.95,1092.99 1804.96,1092.99 1806.98,1092.99 1808.99,1092.99 \n",
       "  1811,1092.99 1813.02,1092.99 1815.03,1092.99 1817.05,1092.99 1819.06,1092.99 1821.07,1092.99 1823.09,1092.99 1825.1,1092.99 1827.12,1092.99 1829.13,1092.99 \n",
       "  1831.15,1092.99 1833.16,1092.99 1835.17,1092.99 1837.19,1092.99 1839.2,1092.99 1841.22,1092.99 1843.23,1092.99 1845.24,1092.99 1847.26,1092.99 1849.27,1092.99 \n",
       "  1851.29,1092.99 1853.3,1092.99 1855.32,1092.99 1857.33,1092.99 1859.34,1092.99 1861.36,1092.99 1863.37,1092.99 1865.39,1092.99 1867.4,1092.99 1869.41,1092.99 \n",
       "  1871.43,1092.99 1873.44,1092.99 1875.46,1092.99 1877.47,1092.99 1879.49,1092.99 1881.5,1092.99 1883.51,1092.99 1885.53,1092.99 1887.54,1092.99 1889.56,1092.99 \n",
       "  1891.57,1092.99 1893.58,1092.99 1895.6,1092.99 1897.61,1092.99 1899.63,1092.99 1901.64,1092.99 1903.66,1092.99 1905.67,1092.99 1907.68,1092.99 1909.7,1092.99 \n",
       "  1911.71,1092.99 1913.73,1092.99 1915.74,1092.99 1917.75,1092.99 1919.77,1092.99 1921.78,1092.99 1923.8,1092.99 1925.81,1092.99 1927.83,1092.99 1929.84,1092.99 \n",
       "  1931.85,1092.99 1933.87,1092.99 1935.88,1092.99 1937.9,1092.99 1939.91,1092.99 1941.92,1092.99 1943.94,1092.99 1945.95,1092.99 1947.97,1092.99 1949.98,1092.99 \n",
       "  1952,1092.99 1954.01,1092.99 1956.02,1092.99 1958.04,1092.99 1960.05,1092.99 1962.07,1092.99 1964.08,1092.99 1966.09,1092.99 1968.11,1092.99 1970.12,1092.99 \n",
       "  1972.14,1092.99 1974.15,1092.99 1976.17,1092.99 1978.18,1092.99 1980.19,1092.99 1982.21,1092.99 1984.22,1092.99 1986.24,1092.99 1988.25,1092.99 1990.26,1092.99 \n",
       "  1992.28,1092.99 1994.29,1092.99 1996.31,1092.99 1998.32,1092.99 2000.34,1092.99 2002.35,1092.99 2004.36,1092.99 2006.38,1092.99 2008.39,1092.99 2010.41,1092.99 \n",
       "  2012.42,1092.99 2014.44,1092.99 2016.45,1092.99 2018.46,1092.99 2020.48,1092.99 2022.49,1092.99 2024.51,1092.99 2026.52,1092.99 2028.53,1092.99 2030.55,1092.99 \n",
       "  2032.56,1092.99 2034.58,1092.99 2036.59,1092.99 2038.61,1092.99 2040.62,1092.99 2042.63,1092.99 2044.65,1092.99 2046.66,1092.99 2048.68,1092.99 2050.69,1092.99 \n",
       "  2052.7,1092.99 2054.72,1092.99 2056.73,1092.99 2058.75,1092.99 2060.76,1092.99 2062.78,1092.99 2064.79,1092.99 2066.8,1092.99 2068.82,1092.99 2070.83,1092.99 \n",
       "  2072.85,1092.99 2074.86,1092.99 2076.87,1092.99 2078.89,1092.99 2080.9,1092.99 2082.92,1092.99 2084.93,1092.99 2086.95,1092.99 2088.96,1092.99 2090.97,1092.99 \n",
       "  2092.99,1092.99 2095,1092.99 2097.02,1092.99 2099.03,1092.99 2101.04,1092.99 2103.06,1092.99 2105.07,1092.99 2107.09,1092.99 2109.1,1092.99 2111.12,1092.99 \n",
       "  2113.13,1092.99 2115.14,1092.99 2117.16,1092.99 2119.17,1092.99 2121.19,1092.99 2123.2,1092.99 2125.21,1092.99 2127.23,1092.99 2129.24,1092.99 2131.26,1092.99 \n",
       "  2133.27,1092.99 2135.29,1092.99 2137.3,1092.99 2139.31,1092.99 2141.33,1092.99 2143.34,1092.99 2145.36,1092.99 2147.37,1092.99 2149.38,1092.99 2151.4,1092.99 \n",
       "  2153.41,1092.99 2155.43,1092.99 2157.44,1092.99 2159.46,1092.99 2161.47,1092.99 2163.48,1092.99 2165.5,1092.99 2167.51,1092.99 2169.53,1092.99 2171.54,1092.99 \n",
       "  2173.55,1092.99 2175.57,1092.99 2177.58,1092.99 2179.6,1092.99 2181.61,1092.99 2183.63,1092.99 2185.64,1092.99 2187.65,1092.99 2189.67,1092.99 2191.68,1092.99 \n",
       "  2193.7,1092.99 2195.71,1092.99 2197.72,1092.99 2199.74,1092.99 2201.75,1092.99 2203.77,1092.99 2205.78,1092.99 2207.8,1092.99 2209.81,1092.99 2211.82,1092.99 \n",
       "  2213.84,1092.99 2215.85,1092.99 2217.87,1092.99 2219.88,1092.99 2221.9,1092.99 2223.91,1092.99 2225.92,1092.99 2227.94,1092.99 2229.95,1092.99 2231.97,1092.99 \n",
       "  2233.98,1092.99 2235.99,1092.99 2238.01,1092.99 2240.02,1092.99 2242.04,1092.99 2244.05,1092.99 2246.07,1092.99 2248.08,1092.99 2250.09,1092.99 2252.11,1092.99 \n",
       "  2254.12,1092.99 2256.14,1092.99 2258.15,1092.99 2260.16,1092.99 2262.18,1092.99 2264.19,1092.99 2266.21,1092.99 2268.22,1092.99 2270.24,1092.99 2272.25,1092.99 \n",
       "  2274.26,1092.99 2276.28,1092.99 2278.29,1092.99 2280.31,1092.99 2282.32,1092.99 2284.33,1092.99 2286.35,1092.99 2288.36,1092.99 2290.38,1092.99 2292.39,1092.99 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip550)\" d=\"\n",
       "M1745.07 374.156 L2281.66 374.156 L2281.66 166.796 L1745.07 166.796  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1745.07,374.156 2281.66,374.156 2281.66,166.796 1745.07,166.796 1745.07,374.156 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip550)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1768.77,218.636 1910.96,218.636 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip550)\" d=\"M1942.06 202.629 L1942.06 209.99 L1950.84 209.99 L1950.84 213.3 L1942.06 213.3 L1942.06 227.374 Q1942.06 230.545 1942.92 231.448 Q1943.8 232.351 1946.46 232.351 L1950.84 232.351 L1950.84 235.916 L1946.46 235.916 Q1941.53 235.916 1939.66 234.087 Q1937.78 232.235 1937.78 227.374 L1937.78 213.3 L1934.66 213.3 L1934.66 209.99 L1937.78 209.99 L1937.78 202.629 L1942.06 202.629 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1971.46 213.971 Q1970.74 213.555 1969.89 213.37 Q1969.06 213.161 1968.04 213.161 Q1964.43 213.161 1962.48 215.522 Q1960.56 217.86 1960.56 222.258 L1960.56 235.916 L1956.28 235.916 L1956.28 209.99 L1960.56 209.99 L1960.56 214.018 Q1961.9 211.657 1964.06 210.522 Q1966.21 209.365 1969.29 209.365 Q1969.73 209.365 1970.26 209.434 Q1970.79 209.481 1971.44 209.596 L1971.46 213.971 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1987.71 222.883 Q1982.55 222.883 1980.56 224.064 Q1978.57 225.244 1978.57 228.092 Q1978.57 230.36 1980.05 231.703 Q1981.56 233.022 1984.12 233.022 Q1987.67 233.022 1989.8 230.522 Q1991.95 227.999 1991.95 223.832 L1991.95 222.883 L1987.71 222.883 M1996.21 221.124 L1996.21 235.916 L1991.95 235.916 L1991.95 231.981 Q1990.49 234.342 1988.31 235.476 Q1986.14 236.587 1982.99 236.587 Q1979.01 236.587 1976.65 234.365 Q1974.31 232.119 1974.31 228.369 Q1974.31 223.994 1977.23 221.772 Q1980.17 219.55 1985.98 219.55 L1991.95 219.55 L1991.95 219.133 Q1991.95 216.194 1990 214.596 Q1988.08 212.976 1984.59 212.976 Q1982.37 212.976 1980.26 213.508 Q1978.15 214.041 1976.21 215.106 L1976.21 211.17 Q1978.55 210.268 1980.74 209.828 Q1982.94 209.365 1985.03 209.365 Q1990.65 209.365 1993.43 212.282 Q1996.21 215.198 1996.21 221.124 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2004.98 209.99 L2009.24 209.99 L2009.24 235.916 L2004.98 235.916 L2004.98 209.99 M2004.98 199.897 L2009.24 199.897 L2009.24 205.291 L2004.98 205.291 L2004.98 199.897 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2039.7 220.268 L2039.7 235.916 L2035.44 235.916 L2035.44 220.407 Q2035.44 216.726 2034.01 214.897 Q2032.57 213.069 2029.7 213.069 Q2026.25 213.069 2024.26 215.268 Q2022.27 217.467 2022.27 221.263 L2022.27 235.916 L2017.99 235.916 L2017.99 209.99 L2022.27 209.99 L2022.27 214.018 Q2023.8 211.68 2025.86 210.522 Q2027.94 209.365 2030.65 209.365 Q2035.12 209.365 2037.41 212.143 Q2039.7 214.897 2039.7 220.268 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2046.05 221.032 L2058.52 221.032 L2058.52 224.828 L2046.05 224.828 L2046.05 221.032 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2065.3 199.897 L2069.56 199.897 L2069.56 235.916 L2065.3 235.916 L2065.3 199.897 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2088.52 212.976 Q2085.1 212.976 2083.11 215.661 Q2081.11 218.323 2081.11 222.976 Q2081.11 227.629 2083.08 230.314 Q2085.07 232.976 2088.52 232.976 Q2091.92 232.976 2093.92 230.291 Q2095.91 227.606 2095.91 222.976 Q2095.91 218.37 2093.92 215.684 Q2091.92 212.976 2088.52 212.976 M2088.52 209.365 Q2094.08 209.365 2097.25 212.976 Q2100.42 216.587 2100.42 222.976 Q2100.42 229.342 2097.25 232.976 Q2094.08 236.587 2088.52 236.587 Q2082.94 236.587 2079.77 232.976 Q2076.62 229.342 2076.62 222.976 Q2076.62 216.587 2079.77 212.976 Q2082.94 209.365 2088.52 209.365 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2124.01 210.754 L2124.01 214.782 Q2122.2 213.856 2120.26 213.393 Q2118.31 212.93 2116.23 212.93 Q2113.06 212.93 2111.46 213.902 Q2109.89 214.874 2109.89 216.819 Q2109.89 218.3 2111.02 219.157 Q2112.16 219.99 2115.58 220.754 L2117.04 221.078 Q2121.58 222.05 2123.48 223.832 Q2125.4 225.592 2125.4 228.763 Q2125.4 232.374 2122.53 234.481 Q2119.68 236.587 2114.68 236.587 Q2112.6 236.587 2110.33 236.17 Q2108.08 235.777 2105.58 234.967 L2105.58 230.569 Q2107.94 231.795 2110.23 232.42 Q2112.53 233.022 2114.77 233.022 Q2117.78 233.022 2119.4 232.004 Q2121.02 230.962 2121.02 229.087 Q2121.02 227.351 2119.84 226.425 Q2118.68 225.499 2114.73 224.643 L2113.24 224.295 Q2109.29 223.462 2107.53 221.749 Q2105.77 220.013 2105.77 217.004 Q2105.77 213.346 2108.36 211.356 Q2110.95 209.365 2115.72 209.365 Q2118.08 209.365 2120.17 209.712 Q2122.25 210.059 2124.01 210.754 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2148.71 210.754 L2148.71 214.782 Q2146.9 213.856 2144.96 213.393 Q2143.01 212.93 2140.93 212.93 Q2137.76 212.93 2136.16 213.902 Q2134.59 214.874 2134.59 216.819 Q2134.59 218.3 2135.72 219.157 Q2136.86 219.99 2140.28 220.754 L2141.74 221.078 Q2146.28 222.05 2148.17 223.832 Q2150.1 225.592 2150.1 228.763 Q2150.1 232.374 2147.23 234.481 Q2144.38 236.587 2139.38 236.587 Q2137.29 236.587 2135.03 236.17 Q2132.78 235.777 2130.28 234.967 L2130.28 230.569 Q2132.64 231.795 2134.93 232.42 Q2137.23 233.022 2139.47 233.022 Q2142.48 233.022 2144.1 232.004 Q2145.72 230.962 2145.72 229.087 Q2145.72 227.351 2144.54 226.425 Q2143.38 225.499 2139.42 224.643 L2137.94 224.295 Q2133.98 223.462 2132.23 221.749 Q2130.47 220.013 2130.47 217.004 Q2130.47 213.346 2133.06 211.356 Q2135.65 209.365 2140.42 209.365 Q2142.78 209.365 2144.86 209.712 Q2146.95 210.059 2148.71 210.754 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip550)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1768.77,270.476 1910.96,270.476 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip550)\" d=\"M1942.06 254.469 L1942.06 261.83 L1950.84 261.83 L1950.84 265.14 L1942.06 265.14 L1942.06 279.214 Q1942.06 282.385 1942.92 283.288 Q1943.8 284.191 1946.46 284.191 L1950.84 284.191 L1950.84 287.756 L1946.46 287.756 Q1941.53 287.756 1939.66 285.927 Q1937.78 284.075 1937.78 279.214 L1937.78 265.14 L1934.66 265.14 L1934.66 261.83 L1937.78 261.83 L1937.78 254.469 L1942.06 254.469 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1978.62 273.728 L1978.62 275.811 L1959.03 275.811 Q1959.31 280.209 1961.67 282.524 Q1964.06 284.816 1968.29 284.816 Q1970.74 284.816 1973.04 284.214 Q1975.35 283.612 1977.62 282.409 L1977.62 286.436 Q1975.33 287.408 1972.92 287.918 Q1970.51 288.427 1968.04 288.427 Q1961.83 288.427 1958.2 284.816 Q1954.59 281.205 1954.59 275.047 Q1954.59 268.682 1958.01 264.955 Q1961.46 261.205 1967.3 261.205 Q1972.53 261.205 1975.56 264.585 Q1978.62 267.941 1978.62 273.728 M1974.36 272.478 Q1974.31 268.983 1972.39 266.899 Q1970.49 264.816 1967.34 264.816 Q1963.78 264.816 1961.62 266.83 Q1959.49 268.844 1959.17 272.501 L1974.36 272.478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2002.13 262.594 L2002.13 266.622 Q2000.33 265.696 1998.38 265.233 Q1996.44 264.77 1994.36 264.77 Q1991.18 264.77 1989.59 265.742 Q1988.01 266.714 1988.01 268.659 Q1988.01 270.14 1989.15 270.997 Q1990.28 271.83 1993.71 272.594 L1995.17 272.918 Q1999.7 273.89 2001.6 275.672 Q2003.52 277.432 2003.52 280.603 Q2003.52 284.214 2000.65 286.321 Q1997.8 288.427 1992.8 288.427 Q1990.72 288.427 1988.45 288.01 Q1986.21 287.617 1983.71 286.807 L1983.71 282.409 Q1986.07 283.635 1988.36 284.26 Q1990.65 284.862 1992.9 284.862 Q1995.91 284.862 1997.53 283.844 Q1999.15 282.802 1999.15 280.927 Q1999.15 279.191 1997.97 278.265 Q1996.81 277.339 1992.85 276.483 L1991.37 276.135 Q1987.41 275.302 1985.65 273.589 Q1983.89 271.853 1983.89 268.844 Q1983.89 265.186 1986.49 263.196 Q1989.08 261.205 1993.85 261.205 Q1996.21 261.205 1998.29 261.552 Q2000.37 261.899 2002.13 262.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2014.52 254.469 L2014.52 261.83 L2023.29 261.83 L2023.29 265.14 L2014.52 265.14 L2014.52 279.214 Q2014.52 282.385 2015.37 283.288 Q2016.25 284.191 2018.92 284.191 L2023.29 284.191 L2023.29 287.756 L2018.92 287.756 Q2013.99 287.756 2012.11 285.927 Q2010.24 284.075 2010.24 279.214 L2010.24 265.14 L2007.11 265.14 L2007.11 261.83 L2010.24 261.83 L2010.24 254.469 L2014.52 254.469 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2026.74 272.872 L2039.22 272.872 L2039.22 276.668 L2026.74 276.668 L2026.74 272.872 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2046 251.737 L2050.26 251.737 L2050.26 287.756 L2046 287.756 L2046 251.737 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2069.22 264.816 Q2065.79 264.816 2063.8 267.501 Q2061.81 270.163 2061.81 274.816 Q2061.81 279.469 2063.78 282.154 Q2065.77 284.816 2069.22 284.816 Q2072.62 284.816 2074.61 282.131 Q2076.6 279.446 2076.6 274.816 Q2076.6 270.21 2074.61 267.524 Q2072.62 264.816 2069.22 264.816 M2069.22 261.205 Q2074.77 261.205 2077.94 264.816 Q2081.11 268.427 2081.11 274.816 Q2081.11 281.182 2077.94 284.816 Q2074.77 288.427 2069.22 288.427 Q2063.64 288.427 2060.47 284.816 Q2057.32 281.182 2057.32 274.816 Q2057.32 268.427 2060.47 264.816 Q2063.64 261.205 2069.22 261.205 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2104.7 262.594 L2104.7 266.622 Q2102.9 265.696 2100.95 265.233 Q2099.01 264.77 2096.92 264.77 Q2093.75 264.77 2092.16 265.742 Q2090.58 266.714 2090.58 268.659 Q2090.58 270.14 2091.72 270.997 Q2092.85 271.83 2096.28 272.594 L2097.73 272.918 Q2102.27 273.89 2104.17 275.672 Q2106.09 277.432 2106.09 280.603 Q2106.09 284.214 2103.22 286.321 Q2100.37 288.427 2095.37 288.427 Q2093.29 288.427 2091.02 288.01 Q2088.78 287.617 2086.28 286.807 L2086.28 282.409 Q2088.64 283.635 2090.93 284.26 Q2093.22 284.862 2095.47 284.862 Q2098.48 284.862 2100.1 283.844 Q2101.72 282.802 2101.72 280.927 Q2101.72 279.191 2100.54 278.265 Q2099.38 277.339 2095.42 276.483 L2093.94 276.135 Q2089.98 275.302 2088.22 273.589 Q2086.46 271.853 2086.46 268.844 Q2086.46 265.186 2089.05 263.196 Q2091.65 261.205 2096.42 261.205 Q2098.78 261.205 2100.86 261.552 Q2102.94 261.899 2104.7 262.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2129.4 262.594 L2129.4 266.622 Q2127.6 265.696 2125.65 265.233 Q2123.71 264.77 2121.62 264.77 Q2118.45 264.77 2116.86 265.742 Q2115.28 266.714 2115.28 268.659 Q2115.28 270.14 2116.42 270.997 Q2117.55 271.83 2120.98 272.594 L2122.43 272.918 Q2126.97 273.89 2128.87 275.672 Q2130.79 277.432 2130.79 280.603 Q2130.79 284.214 2127.92 286.321 Q2125.07 288.427 2120.07 288.427 Q2117.99 288.427 2115.72 288.01 Q2113.48 287.617 2110.98 286.807 L2110.98 282.409 Q2113.34 283.635 2115.63 284.26 Q2117.92 284.862 2120.17 284.862 Q2123.17 284.862 2124.8 283.844 Q2126.42 282.802 2126.42 280.927 Q2126.42 279.191 2125.23 278.265 Q2124.08 277.339 2120.12 276.483 L2118.64 276.135 Q2114.68 275.302 2112.92 273.589 Q2111.16 271.853 2111.16 268.844 Q2111.16 265.186 2113.75 263.196 Q2116.35 261.205 2121.11 261.205 Q2123.48 261.205 2125.56 261.552 Q2127.64 261.899 2129.4 262.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip550)\" style=\"stroke:#3da44d; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1768.77,322.316 1910.96,322.316 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip550)\" d=\"M1955 318.647 Q1956.6 315.776 1958.82 314.411 Q1961.05 313.045 1964.06 313.045 Q1968.11 313.045 1970.31 315.892 Q1972.5 318.716 1972.5 323.948 L1972.5 339.596 L1968.22 339.596 L1968.22 324.087 Q1968.22 320.36 1966.9 318.554 Q1965.58 316.749 1962.87 316.749 Q1959.56 316.749 1957.64 318.948 Q1955.72 321.147 1955.72 324.943 L1955.72 339.596 L1951.44 339.596 L1951.44 324.087 Q1951.44 320.337 1950.12 318.554 Q1948.8 316.749 1946.05 316.749 Q1942.78 316.749 1940.86 318.971 Q1938.94 321.17 1938.94 324.943 L1938.94 339.596 L1934.66 339.596 L1934.66 313.67 L1938.94 313.67 L1938.94 317.698 Q1940.4 315.313 1942.43 314.179 Q1944.47 313.045 1947.27 313.045 Q1950.1 313.045 1952.06 314.48 Q1954.06 315.915 1955 318.647 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M1981 313.67 L1985.26 313.67 L1985.26 339.596 L1981 339.596 L1981 313.67 M1981 303.577 L1985.26 303.577 L1985.26 308.971 L1981 308.971 L1981 303.577 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2015.72 323.948 L2015.72 339.596 L2011.46 339.596 L2011.46 324.087 Q2011.46 320.406 2010.03 318.577 Q2008.59 316.749 2005.72 316.749 Q2002.27 316.749 2000.28 318.948 Q1998.29 321.147 1998.29 324.943 L1998.29 339.596 L1994.01 339.596 L1994.01 313.67 L1998.29 313.67 L1998.29 317.698 Q1999.82 315.36 2001.88 314.202 Q2003.96 313.045 2006.67 313.045 Q2011.14 313.045 2013.43 315.823 Q2015.72 318.577 2015.72 323.948 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2022.06 324.712 L2034.54 324.712 L2034.54 328.508 L2022.06 328.508 L2022.06 324.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2045.54 306.309 L2045.54 313.67 L2054.31 313.67 L2054.31 316.98 L2045.54 316.98 L2045.54 331.054 Q2045.54 334.225 2046.39 335.128 Q2047.27 336.031 2049.93 336.031 L2054.31 336.031 L2054.31 339.596 L2049.93 339.596 Q2045 339.596 2043.13 337.767 Q2041.25 335.915 2041.25 331.054 L2041.25 316.98 L2038.13 316.98 L2038.13 313.67 L2041.25 313.67 L2041.25 306.309 L2045.54 306.309 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2082.09 325.568 L2082.09 327.651 L2062.5 327.651 Q2062.78 332.049 2065.14 334.364 Q2067.53 336.656 2071.76 336.656 Q2074.22 336.656 2076.51 336.054 Q2078.82 335.452 2081.09 334.249 L2081.09 338.276 Q2078.8 339.248 2076.39 339.758 Q2073.99 340.267 2071.51 340.267 Q2065.3 340.267 2061.67 336.656 Q2058.06 333.045 2058.06 326.887 Q2058.06 320.522 2061.49 316.795 Q2064.93 313.045 2070.77 313.045 Q2076 313.045 2079.03 316.425 Q2082.09 319.781 2082.09 325.568 M2077.83 324.318 Q2077.78 320.823 2075.86 318.739 Q2073.96 316.656 2070.81 316.656 Q2067.25 316.656 2065.1 318.67 Q2062.97 320.684 2062.64 324.341 L2077.83 324.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2105.61 314.434 L2105.61 318.462 Q2103.8 317.536 2101.86 317.073 Q2099.91 316.61 2097.83 316.61 Q2094.66 316.61 2093.06 317.582 Q2091.49 318.554 2091.49 320.499 Q2091.49 321.98 2092.62 322.837 Q2093.75 323.67 2097.18 324.434 L2098.64 324.758 Q2103.17 325.73 2105.07 327.512 Q2106.99 329.272 2106.99 332.443 Q2106.99 336.054 2104.12 338.161 Q2101.28 340.267 2096.28 340.267 Q2094.19 340.267 2091.92 339.85 Q2089.68 339.457 2087.18 338.647 L2087.18 334.249 Q2089.54 335.475 2091.83 336.1 Q2094.12 336.702 2096.37 336.702 Q2099.38 336.702 2101 335.684 Q2102.62 334.642 2102.62 332.767 Q2102.62 331.031 2101.44 330.105 Q2100.28 329.179 2096.32 328.323 L2094.84 327.975 Q2090.88 327.142 2089.12 325.429 Q2087.36 323.693 2087.36 320.684 Q2087.36 317.026 2089.96 315.036 Q2092.55 313.045 2097.32 313.045 Q2099.68 313.045 2101.76 313.392 Q2103.85 313.739 2105.61 314.434 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2117.99 306.309 L2117.99 313.67 L2126.76 313.67 L2126.76 316.98 L2117.99 316.98 L2117.99 331.054 Q2117.99 334.225 2118.85 335.128 Q2119.73 336.031 2122.39 336.031 L2126.76 336.031 L2126.76 339.596 L2122.39 339.596 Q2117.46 339.596 2115.58 337.767 Q2113.71 335.915 2113.71 331.054 L2113.71 316.98 L2110.58 316.98 L2110.58 313.67 L2113.71 313.67 L2113.71 306.309 L2117.99 306.309 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2130.21 324.712 L2142.69 324.712 L2142.69 328.508 L2130.21 328.508 L2130.21 324.712 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2149.47 303.577 L2153.73 303.577 L2153.73 339.596 L2149.47 339.596 L2149.47 303.577 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2172.69 316.656 Q2169.26 316.656 2167.27 319.341 Q2165.28 322.003 2165.28 326.656 Q2165.28 331.309 2167.25 333.994 Q2169.24 336.656 2172.69 336.656 Q2176.09 336.656 2178.08 333.971 Q2180.07 331.286 2180.07 326.656 Q2180.07 322.05 2178.08 319.364 Q2176.09 316.656 2172.69 316.656 M2172.69 313.045 Q2178.24 313.045 2181.42 316.656 Q2184.59 320.267 2184.59 326.656 Q2184.59 333.022 2181.42 336.656 Q2178.24 340.267 2172.69 340.267 Q2167.11 340.267 2163.94 336.656 Q2160.79 333.022 2160.79 326.656 Q2160.79 320.267 2163.94 316.656 Q2167.11 313.045 2172.69 313.045 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2208.17 314.434 L2208.17 318.462 Q2206.37 317.536 2204.42 317.073 Q2202.48 316.61 2200.4 316.61 Q2197.23 316.61 2195.63 317.582 Q2194.05 318.554 2194.05 320.499 Q2194.05 321.98 2195.19 322.837 Q2196.32 323.67 2199.75 324.434 L2201.21 324.758 Q2205.74 325.73 2207.64 327.512 Q2209.56 329.272 2209.56 332.443 Q2209.56 336.054 2206.69 338.161 Q2203.85 340.267 2198.85 340.267 Q2196.76 340.267 2194.49 339.85 Q2192.25 339.457 2189.75 338.647 L2189.75 334.249 Q2192.11 335.475 2194.4 336.1 Q2196.69 336.702 2198.94 336.702 Q2201.95 336.702 2203.57 335.684 Q2205.19 334.642 2205.19 332.767 Q2205.19 331.031 2204.01 330.105 Q2202.85 329.179 2198.89 328.323 L2197.41 327.975 Q2193.45 327.142 2191.69 325.429 Q2189.93 323.693 2189.93 320.684 Q2189.93 317.026 2192.53 315.036 Q2195.12 313.045 2199.89 313.045 Q2202.25 313.045 2204.33 313.392 Q2206.41 313.739 2208.17 314.434 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip550)\" d=\"M2232.87 314.434 L2232.87 318.462 Q2231.07 317.536 2229.12 317.073 Q2227.18 316.61 2225.1 316.61 Q2221.92 316.61 2220.33 317.582 Q2218.75 318.554 2218.75 320.499 Q2218.75 321.98 2219.89 322.837 Q2221.02 323.67 2224.45 324.434 L2225.91 324.758 Q2230.44 325.73 2232.34 327.512 Q2234.26 329.272 2234.26 332.443 Q2234.26 336.054 2231.39 338.161 Q2228.54 340.267 2223.54 340.267 Q2221.46 340.267 2219.19 339.85 Q2216.95 339.457 2214.45 338.647 L2214.45 334.249 Q2216.81 335.475 2219.1 336.1 Q2221.39 336.702 2223.64 336.702 Q2226.65 336.702 2228.27 335.684 Q2229.89 334.642 2229.89 332.767 Q2229.89 331.031 2228.71 330.105 Q2227.55 329.179 2223.59 328.323 L2222.11 327.975 Q2218.15 327.142 2216.39 325.429 Q2214.63 323.693 2214.63 320.684 Q2214.63 317.026 2217.23 315.036 Q2219.82 313.045 2224.59 313.045 Q2226.95 313.045 2229.03 313.392 Q2231.11 313.739 2232.87 314.434 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepoch = 1000\n",
    "plot(collect(1:nepoch), \n",
    "    hcat(log.(10, trainloss .+ 1), \n",
    "    log.(10, testloss .+1 ), \n",
    "    fill(minimum(log.(10, testloss .+1 )), nepoch)), \n",
    "    title = \"Loss\", \n",
    "    label=[\"train-loss\" \"test-loss\" \"min-test-loss\"], \n",
    "    xlabel=\"number of epoch\", ylabel=\"log10(model loss)\", legend=:topright)\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do inference and see how our model performs on testdata ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×2 Matrix{String}:\n",
       " \"L5a\"   \"L6a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5a\"   \"L6a\"\n",
       " \"L5a\"   \"L6a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5b\"   \"L6a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5b\"   \"L5a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5a\"   \"L5a\"\n",
       " \"L5b\"   \"L5b\"\n",
       " ⋮       \n",
       " \"L4\"    \"L4\"\n",
       " \"L6b\"   \"L6a\"\n",
       " \"L2/3\"  \"L2/3\"\n",
       " \"L6a\"   \"L6a\"\n",
       " \"L2\"    \"L2\"\n",
       " \"L6a\"   \"L6a\"\n",
       " \"L2/3\"  \"L6a\"\n",
       " \"L2/3\"  \"L2/3\"\n",
       " \"L2/3\"  \"L6a\"\n",
       " \"L2/3\"  \"L6a\"\n",
       " \"L5b\"   \"L2\"\n",
       " \"L6a\"   \"L6a\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "Flux.onecold(softmax(model(first(test_loader)[1])), unique(adata.celltypes)) Flux.onecold(first(test_loader)[2], unique(adata.celltypes))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a trained model, we are going to apply IG method, but first we will gain some in depth knowledge \n",
    "about IG ... \n",
    "\n",
    "First, you will generate a linear interpolation between the **baseline** and a **target**, that we want to get the its ingegrated gradient. Interpolation can be thought as small steps in the feature space between your baseline and input/target, represented by in the original equation $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0f0:0.02f0:1.0f0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interpolation steps \n",
    "msteps = 50\n",
    "alphas = range(0.0f0, 1.0f0, length=msteps+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interpolate (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define an interpolation function \n",
    "interpolate(baseline, target, alphas) = hcat(map(alpha -> baseline .+ alpha .* (target .- baseline), \n",
    "                                        alphas)...) # 180×51 genes x msteps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate the gradients. **Gradients** measure the relationship between changes to a feature and changes in the model's predictions. <br>\n",
    "In our case, the **gradient tells us which gene have the strongest effect on the models predicted class probabilities**.\n",
    "\n",
    "### Compute Gradients\n",
    "\n",
    "In Julia, we use `ForwardDiff`. ForwardDiff implements methods to take **derivatives**, **gradients**, **Jacobians**, **Hessians**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_gradients (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_gradients(model, x::AbstractVector, label_index)\n",
    "    gradients = ForwardDiff.gradient(x -> softmax(model(x))[label_index], x)\n",
    "    return gradients\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integral Approximation \n",
    "\n",
    "Here we compute a numerical approximation of an integral for IG with using Riemann sums(it is the summation in the formula). In this implementation Trapezoidal rule has been used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "integral_approx (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function integral_approx(pathgrads::AbstractMatrix) # assumes features are rows and path values are columns\n",
    "    # Riemann trapezoidal \n",
    "    int_grads = (pathgrads[:,1:(end-2)] + pathgrads[:,3:end])./2\n",
    "    return vec(mean(int_grads,dims=2))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Integrated Gradients\n",
    "\n",
    "Now we put things together and compute integrated gradients ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_integrated_gradients (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_integrated_gradients(model, baseline, target, label_index; msteps::Int=50)\n",
    "\n",
    "    alphas = range(0.0f0, 1.0f0, length=msteps+1)\n",
    "    pathgrads = hcat(\n",
    "                map(alpha -> compute_gradients(model, vec(interpolate(baseline, target, alpha)), label_index)\n",
    "                , alphas)...) # #genes=180× #step = 51\n",
    "    \n",
    "    igs = integral_approx(pathgrads) # #genes = 180 \n",
    "    scalefac = (target .- baseline) # #genes = 180\n",
    "    return scalefac .* igs   # #genes = 180 \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give the method a shot and see the accumlated gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L5a\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = first(test_loader)[1][:,1] # first cell in the testset as a baseline \n",
    "target = first(test_loader)[1][:,2] # second cell in the testset as a target \n",
    "label_index = 7\n",
    "celltype = unique(adata.celltypes)[label_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Choosing a baseline:** \n",
    ">\n",
    "> If your data represented by images on a gray scale, a black image (e.g. pixels values are zeros) can be a natural choice. \n",
    "> **Discussion:** what if your data represented by cells and genes are the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180-element Vector{Float32}:\n",
       " -8.757472f-5\n",
       "  0.0\n",
       " -9.843328f-5\n",
       "  0.0\n",
       "  0.0\n",
       " -0.00074680796\n",
       " -0.0\n",
       " -0.00031007238\n",
       "  0.00074985076\n",
       "  0.0\n",
       " -0.0\n",
       " -2.1628027f-6\n",
       "  0.0\n",
       "  ⋮\n",
       "  0.0\n",
       "  0.0\n",
       " -0.0\n",
       " -0.0\n",
       "  0.0\n",
       " -0.0\n",
       "  0.0\n",
       " -0.0\n",
       " -0.0\n",
       "  0.0\n",
       " -0.0\n",
       "  0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated_gradient = compute_integrated_gradients(model, baseline, target, label_index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pack the integrated gradients in a dictionary and invistigate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Vector} with 2 entries:\n",
       "  \"accumulated_gradients\" => Float32[-8.75747f-5, 0.0, -9.84333f-5, 0.0, 0.0, -…\n",
       "  \"genes\"                 => Any[\"Adora1\", \"Adra1a\", \"Adra1b\", \"Adra1d\", \"Adra2…"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated_gradient_results = Dict(\"genes\" => adata.vars[\"gene_names\"],\n",
    "                \"accumulated_gradients\" => integrated_gradient) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Pair{String, Vector}}:\n",
       " \"accumulated_gradients\" => Float32[-8.757472f-5, 0.0, -9.843328f-5, 0.0, 0.0, -0.00074680796, -0.0, -0.00031007238, 0.00074985076, 0.0  …  -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0]\n",
       "                 \"genes\" => Any[\"Adora1\", \"Adra1a\", \"Adra1b\", \"Adra1d\", \"Adra2a\", \"Adrb1\", \"Adrb2\", \"Aqp4\", \"Arf5\", \"Batf3\"  …  \"Th\", \"Tnfaip8l3\", \"Tnmd\", \"Tpbg\", \"Tph2\", \"Ucma\", \"Vip\", \"Vtn\", \"Wt1\", \"Xdh\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_results = sort(collect(integrated_gradient_results), by = x->x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using integrated gradients, we can see which gene has affected the model in classifiying a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180×2 Matrix{Any}:\n",
       " -8.75747f-5   \"Adora1\"\n",
       "  0.0          \"Adra1a\"\n",
       " -9.84333f-5   \"Adra1b\"\n",
       "  0.0          \"Adra1d\"\n",
       "  0.0          \"Adra2a\"\n",
       " -0.000746808  \"Adrb1\"\n",
       " -0.0          \"Adrb2\"\n",
       " -0.000310072  \"Aqp4\"\n",
       "  0.000749851  \"Arf5\"\n",
       "  0.0          \"Batf3\"\n",
       " -0.0          \"Bcl6\"\n",
       " -2.1628f-6    \"Bgn\"\n",
       "  0.0          \"Brca1\"\n",
       "  ⋮            \n",
       "  0.0          \"Tcerg1l\"\n",
       "  0.0          \"Tcf7l2\"\n",
       " -0.0          \"Th\"\n",
       " -0.0          \"Tnfaip8l3\"\n",
       "  0.0          \"Tnmd\"\n",
       " -0.0          \"Tpbg\"\n",
       "  0.0          \"Tph2\"\n",
       " -0.0          \"Ucma\"\n",
       " -0.0          \"Vip\"\n",
       "  0.0          \"Vtn\"\n",
       " -0.0          \"Wt1\"\n",
       "  0.0          \"Xdh\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[integrated_gradient_results[\"accumulated_gradients\"] integrated_gradient_results[\"genes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adata.vars[\"gene_names\"] = Any[\"Adora1\", \"Adra1a\", \"Adra1b\", \"Adra1d\", \"Adra2a\", \"Adrb1\", \"Adrb2\", \"Aqp4\", \"Arf5\", \"Batf3\", \"Bcl6\", \"Bgn\", \"Brca1\", \"C1qb\", \"Calb2\", \"Car12\", \"Car4\", \"Cbln4\", \"Cd9\", \"Cdh13\", \"Cdk6\", \"Chat\", \"Chodl\", \"Chrm1\", \"Chrm2\", \"Chrm3\", \"Chrm4\", \"Chrna1\", \"Chrna2\", \"Chrna3\", \"Chrna4\", \"Chrna5\", \"Chrna7\", \"Chrnb1\", \"Chrnb2\", \"Chrnb3\", \"Cnr1\", \"Cnr2\", \"Col6a1\", \"Cpne5\", \"Crip1\", \"Crispld2\", \"Cspg4\", \"Ctgf\", \"Ctss\", \"Ctxn3\", \"Cx3cr1\", \"Cxcl14\", \"Ddit4l\", \"Deptor\", \"Drd1a\", \"Drd5\", \"Enpp2\", \"Enpp6\", \"Exosc7\", \"F3\", \"Flt1\", \"Foxp2\", \"Frmd7\", \"Fyn\", \"Gabbr1\", \"Gabbr2\", \"Gabra1\", \"Gabra2\", \"Gabra3\", \"Gabra4\", \"Gabra5\", \"Gabrb1\", \"Gabrb2\", \"Gabrb3\", \"Gabrd\", \"Gabrg1\", \"Gabrg2\", \"Gabrg3\", \"Gfap\", \"Gja1\", \"Glra2\", \"Glra3\", \"Glrb\", \"Gpc3\", \"Gpr34\", \"Gpx3\", \"Gria1\", \"Gria2\", \"Gria3\", \"Gria4\", \"Grid1\", \"Grik1\", \"Grik2\", \"Grik3\", \"Grik4\", \"Grik5\", \"Grin1\", \"Grin2a\", \"Grin2b\", \"Grin2d\", \"Grin3a\", \"Grina\", \"Grm1\", \"Grm2\", \"Grm3\", \"Grm4\", \"Grm5\", \"Grm7\", \"Grm8\", \"Hrh1\", \"Hrh2\", \"Hrh3\", \"Hsd11b1\", \"Htr1a\", \"Htr1b\", \"Htr1d\", \"Htr1f\", \"Htr2a\", \"Htr2b\", \"Htr2c\", \"Htr3a\", \"Htr3b\", \"Htr4\", \"Htr5a\", \"Htr5b\", \"Htr7\", \"Id1\", \"Igtp\", \"Inhba\", \"Itgam\", \"Ly6d\", \"Mab21l1\", \"Mag\", \"Mbp\", \"Mgp\", \"Mog\", \"Mybpc1\", \"Myh8\", \"Myl4\", \"Myl9\", \"Ngb\", \"Nos1\", \"Obox3\", \"Olig1\", \"Opalin\", \"Otof\", \"Parm1\", \"Pcdh15\", \"Pde1c\", \"Pdgfra\", \"Penk\", \"Prss22\", \"Ptgs2\", \"Pvalb\", \"Qrfpr\", \"Reln\", \"Rgs12\", \"Rnf122\", \"Rorb\", \"Rspo2\", \"Scnn1a\", \"Sema3c\", \"Sla\", \"Smad3\", \"Sncg\", \"Sox9\", \"Sst\", \"Stac\", \"Syt17\", \"Tacr1\", \"Tacr3\", \"Tbc1d4\", \"Tcerg1l\", \"Tcf7l2\", \"Th\", \"Tnfaip8l3\", \"Tnmd\", \"Tpbg\", \"Tph2\", \"Ucma\", \"Vip\", \"Vtn\", \"Wt1\", \"Xdh\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "180-element Vector{Any}:\n",
       " \"Adora1\"\n",
       " \"Adra1a\"\n",
       " \"Adra1b\"\n",
       " \"Adra1d\"\n",
       " \"Adra2a\"\n",
       " \"Adrb1\"\n",
       " \"Adrb2\"\n",
       " \"Aqp4\"\n",
       " \"Arf5\"\n",
       " \"Batf3\"\n",
       " \"Bcl6\"\n",
       " \"Bgn\"\n",
       " \"Brca1\"\n",
       " ⋮\n",
       " \"Tcerg1l\"\n",
       " \"Tcf7l2\"\n",
       " \"Th\"\n",
       " \"Tnfaip8l3\"\n",
       " \"Tnmd\"\n",
       " \"Tpbg\"\n",
       " \"Tph2\"\n",
       " \"Ucma\"\n",
       " \"Vip\"\n",
       " \"Vtn\"\n",
       " \"Wt1\"\n",
       " \"Xdh\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show(adata.vars[\"gene_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "[Integrated gradients in Tensorflow](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients) <br>\n",
    "[Axiomatic Attribution for Deep Networks by Mukund Sundararajan](https://arxiv.org/abs/1703.01365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@article{DBLP:journals/corr/SundararajanTY17,\n",
    "  author    = {Mukund Sundararajan and\n",
    "               Ankur Taly and\n",
    "               Qiqi Yan},\n",
    "  title     = {Axiomatic Attribution for Deep Networks},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1703.01365},\n",
    "  year      = {2017},\n",
    "  url       = {http://arxiv.org/abs/1703.01365},\n",
    "  eprinttype = {arXiv},\n",
    "  eprint    = {1703.01365},\n",
    "  timestamp = {Mon, 13 Aug 2018 16:48:32 +0200},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/SundararajanTY17.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
